<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>常青的小屋</title>
  
  <subtitle>Keep looking, do not settle.</subtitle>
  <link href="https://changqingaas.github.io/atom.xml" rel="self"/>
  
  <link href="https://changqingaas.github.io/"/>
  <updated>2021-10-06T04:57:07.546Z</updated>
  <id>https://changqingaas.github.io/</id>
  
  <author>
    <name>常青</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Policy Gradient Methods</title>
    <link href="https://changqingaas.github.io/MARL/MADRL/Policy%20Gradient%20Mwthods/"/>
    <id>https://changqingaas.github.io/MARL/MADRL/Policy%20Gradient%20Mwthods/</id>
    <published>2021-10-06T15:00:33.000Z</published>
    <updated>2021-10-06T04:57:07.546Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇博文的内容整理了我们如何去近似价值函数或者是动作价值函数的方法：</p><script type="math/tex; mode=display">V_{\theta}(s)\approx V^{\pi}(s) \\Q_{\theta}(s)\approx Q^{\pi}(s, a)</script><p>通过机器学习的方法我们一旦近似了价值函数或者是动作价值函数就可以通过一些策略进行控制，比如  ϵ-greedy。</p><p>通过机器学习的方法我们一旦近似了价值函数或者是动作价值函数就可以通过一些策略进行控制，比如 ϵ -greedy。</p><p>那么我们简单回顾下 RL 的学习目标：通过 agent 与环境进行交互，获取累计回报最大化。既然我们最终要学习如何与环境交互的策略，那么我们可以直接学习策略 ，而之前先近似价值函数，再通过贪婪策略控制的思路更像是”曲线救国”。<br>这就是本篇文章的内容，我们如何直接来学习策略，用数学的形式表达就是：</p><p>$\pi_{\theta}(s, a) = P[a | s, \theta]$</p><p>这就是被称为策略梯度（Policy Gradient，简称PG）算法。</p><p>当然，本篇内容同样的是针对 model-free 的强化学习。</p><h1 id="Value-Based-vs-Policy-Based-RL"><a href="#Value-Based-vs-Policy-Based-RL" class="headerlink" title="Value-Based vs. Policy-Based RL"></a>Value-Based vs. Policy-Based RL</h1><p>Value-Based：</p><ul><li>学习价值函数</li><li>Implicit policy，比如 ϵϵ-greedy</li></ul><p>Policy-Based：</p><ul><li>没有价值函数</li><li>直接学习策略</li></ul><p>Actor-Critic：</p><ul><li>学习价值函数</li><li>学习策略</li></ul><p>三者的关系可以形式化地表示如下：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006124701.png" alt=""></p><p>认识到 Value-Based 与 Policy-Based 区别后，我们再来讨论Policy-Based RL 的优缺点：</p><p>优点：</p><ul><li>收敛性更好</li><li>对于具有高维或者连续动作空间的问题更加有效</li><li>可以学习随机策略</li></ul><p>缺点：</p><ul><li>绝大多数情况下收敛到局部最优点，而非全局最优</li><li>评估一个策略一般情况下低效且存在较高的方差</li></ul><h1 id="Policy-Search"><a href="#Policy-Search" class="headerlink" title="Policy Search"></a>Policy Search</h1><p>我们首先定义下目标函数。</p><h2 id="Policy-Objective-Functions"><a href="#Policy-Objective-Functions" class="headerlink" title="Policy Objective Functions"></a>Policy Objective Functions</h2><p>目标：给定一个带有参数 θ 的策略 π~θ~(s,a)  ，找到最优的参数 θ 。<br>但是我们如何评估不同参数下策略 π~θ(~s,a)  的优劣呢？</p><ul><li>对于episode 任务来说，我们可以使用start value：</li></ul><script type="math/tex; mode=display">J_1(\theta)=V^{\pi_{\theta}}(s_1)=E_{\pi_{\theta}}[v_1]</script><ul><li>对于连续性任务来说，我们可以使用 average value：</li></ul><script type="math/tex; mode=display">J_{avV}(\theta)=\sum_{s}d^{\pi_{\theta}}(s)V^{\pi_{\theta}}(s)</script><p>或者每一步的平均回报：</p><script type="math/tex; mode=display">J_{avR}(\theta)=\sum_{s}d^{\pi_{\theta}}(s)\sum_{a}\pi_{\theta}(s, a)R_s^a</script><p>其中 d^πθ^(s)  是马尔卡夫链在 π~θ~  下的静态分布。</p><h1 id="Policy-Optimisation"><a href="#Policy-Optimisation" class="headerlink" title="Policy Optimisation"></a>Policy Optimisation</h1><p>在明确目标以后，我们再来看基于策略的 RL 为一个典型的优化问题：找出 θ 最大化 J(θ)<br>最优化的方法有很多，比如不依赖梯度（gradient-free）的算法：</p><ul><li>爬山算法</li><li>模拟退火</li><li>进化算法</li><li>…</li></ul><p>但是一般来说，如果我们能在问题中获得梯度的话，基于梯度的最优化方法具有比较好的效果：</p><ul><li>梯度下降</li><li>共轭梯度</li><li>拟牛顿法</li><li>…</li></ul><p>我们本篇讨论梯度下降的方法。</p><h1 id="策略梯度定理"><a href="#策略梯度定理" class="headerlink" title="策略梯度定理"></a>策略梯度定理</h1><h1 id="蒙特卡洛策略梯度算法（REINFORCE）"><a href="#蒙特卡洛策略梯度算法（REINFORCE）" class="headerlink" title="蒙特卡洛策略梯度算法（REINFORCE）"></a>蒙特卡洛策略梯度算法（REINFORCE）</h1><h1 id="Actir-Critic-策略梯度算法"><a href="#Actir-Critic-策略梯度算法" class="headerlink" title="Actir-Critic 策略梯度算法"></a>Actir-Critic 策略梯度算法</h1><p>Monte-Carlo策略梯度的方差较高，因此放弃用return来估计行动-价值函数Q，而是使用 critic 来估计Q：</p><p>$Q_w(s, a)\approx Q^{\pi_{\theta}}(s, a)$</p><p>这就是大名鼎鼎的 Actor-Critic 算法，它有两套参数：</p><ul><li>Critic：更新动作价值函数参数 w </li><li>Actor： 朝着 Critic 方向更新策略参数 θ</li></ul><p>Actor-Critic 算法是一个近似的策略梯度算法：</p><script type="math/tex; mode=display">\triangledown_\theta J(\theta)\approx E_{\pi_{\theta}}[\triangledown_{\theta}\log \pi_{\theta}(s, a)Q_w(s, a)]\\\Delta\theta = \alpha\triangledown_\theta\log\pi_{\theta}(s,a)Q_w(s,a)</script><p>Critic 本质就是在进行策略评估：How good is policy π~θ~  for current parameters θ </p><p>策略评估我们之前介绍过MC、TD、TD(λλ)，以及价值函数近似方法。如下所示，简单的 Actir-Critic 算法 Critic 为动作价值函数近似，使用最为简单的线性方程，即：$Q_w(s, a) = \phi(s, a)^T w$，具体的伪代码如下所示：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006125435.png" alt="img"></p><p>在 Actir-Critic 算法中，对策略进行了估计，这会产生误差（bias），但是当满足以下两个条件时，策略梯度是准确的：</p><ul><li>价值函数的估计值没有和策略相违背，即： $\triangledown_w Q_w(s,a) = \triangledown_\theta\log\pi_{\theta}(s,a)$</li><li>价值函数的参数w能够最小化误差，即： $\epsilon = E_{\pi_{\theta}}[(Q^{\pi_{\theta}}(s, a) - Q_w(s,a))^2]$</li></ul><p>最后总结一下策略梯度算法：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006125705.png" alt="img"></p>]]></content>
    
    
    <summary type="html">Policy Gradient Methods</summary>
    
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/categories/MARL/"/>
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/tags/MARL/"/>
    
  </entry>
  
  <entry>
    <title>Value Function Approximation</title>
    <link href="https://changqingaas.github.io/MARL/MADRL/Value%20Function%20Approximation/"/>
    <id>https://changqingaas.github.io/MARL/MADRL/Value%20Function%20Approximation/</id>
    <published>2021-10-06T15:00:33.000Z</published>
    <updated>2021-10-06T04:08:01.729Z</updated>
    
    <content type="html"><![CDATA[<h1 id="为什么需要值函数近似？"><a href="#为什么需要值函数近似？" class="headerlink" title="为什么需要值函数近似？"></a>为什么需要值函数近似？</h1><p>之前我们提到过各种计算值函数的方法，比如对于 MDP 已知的问题可以使用 Bellman 期望方程求得值函数；对于 MDP 未知的情况，可以通过 MC 以及 TD 方法来获得值函数，为什么需要再进行值函数近似呢？</p><p>其实到目前为止，我们介绍的值函数计算方法都是通过查表的方式获取的：</p><ul><li>表中每一个状态 s  均对应一个 V(s) </li><li>或者每一个状态-动作 <s,a ></li></ul><p>但是对于大型 MDP 问题，上述方法会遇到瓶颈：</p><ul><li>太多的 MDP 状态、动作需要存储</li><li>单独计算每一个状态的价值都非常的耗时</li></ul><p>因此我们需要有一种能够适用于解决大型 MDP 问题的通用方法，这就是本文介绍的值函数近似方法。即：</p><script type="math/tex; mode=display">\hat{v}(s, \mathbf{w}) \approx v_{\pi}(s) \\\text{or } \hat{q}(s, a, \mathbf{w}) \approx q_{\pi}(s, a)</script><p>那么为什么值函数近似的方法可以求解大型 MDP 问题？</p><blockquote><p>对于大型 MDP 问题而言，我们可以近似认为其所有的状态和动作都被采样和计算是不现实的，那么我们一旦获取了近似的值函数，我们就可以对于那些在历史经验或者采样中没有出现过的状态和动作进行泛化（generalize）。</p></blockquote><p>进行值函数近似的训练方法有很多，比如：</p><ul><li>线性回归</li><li>神经网络</li><li>决策树</li><li>…</li></ul><p>此外，针对 MDP 问题的特点，训练函数必须可以适用于非静态、非独立同分布（non-i.i.d）的数据。</p><h1 id="增量方法"><a href="#增量方法" class="headerlink" title="增量方法"></a>增量方法</h1><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><h3 id="通过随机梯度下降进行值函数近似"><a href="#通过随机梯度下降进行值函数近似" class="headerlink" title="通过随机梯度下降进行值函数近似"></a>通过随机梯度下降进行值函数近似</h3><p>我们优化的目标函数是找到一组参数 w  来最小化最小平方误差（MSE），即：</p><script type="math/tex; mode=display">J(\mathbf{w}) = E_{\pi}[(v_{\pi}(S) - \hat{v}(S, \mathbf{w}))^2]</script><p>通过梯度下降方法来寻优：</p><script type="math/tex; mode=display">\begin{align}\Delta\mathbf{w}&=-\frac{1}{2}\alpha\triangledown_{\mathbf{w}}J(\mathbf{w})\\&=\alpha E_{\pi}\Bigl[\Bigl(v_{\pi}(S) - \hat{v}(S, \mathbf{w})\Bigr)\triangledown_{\mathbf{w}}J(\mathbf{w})\Bigr]\end{align}</script><p>对于随机梯度下降（Stochastic Gradient Descent，SGD），对应的梯度：</p><script type="math/tex; mode=display">\Delta\mathbf{w} = \alpha\underbrace{\Bigl(v_{\pi}(S) - \hat{v}(S, \mathbf{w})\Bigr)}_{\text{error}}\underbrace{\triangledown_{\mathbf{w}}\hat{v}(S, \mathbf{w})}_{\text{gradient}}</script><h2 id="值函数近似"><a href="#值函数近似" class="headerlink" title="值函数近似"></a>值函数近似</h2><p>上述公式中需要真实的策略价值函数 vπ(S)vπ(S) 作为学习的目标（supervisor），但是在RL中没有真实的策略价值函数，只有rewards。在实际应用中，我们用v~π~(S)来代替target vπ(S)：</p><ul><li>对于MC，target 为 return G~t ~：</li></ul><script type="math/tex; mode=display">\Delta\mathbf{w}=\alpha\Bigl(G_t - \hat{v}(S_t, \mathbf{w})\Bigr)\triangledown_{\mathbf{w}}\hat{v}(S_t, \mathbf{w})</script><ul><li>对于TD(0)，target 为TD target $_{t+1}+\gamma\hat{v}(S_{t+1}, \mathbf{w})$：</li></ul><script type="math/tex; mode=display">\Delta\mathbf{w}=\alpha\Bigl(R_{t+1} + \gamma\hat{v}(S_{t+1}, \mathbf{w})- \hat{v}(S_t, \mathbf{w})\Bigr)\triangledown_{\mathbf{w}}\hat{v}(S_t, \mathbf{w})</script><ul><li>对于TD(λ)，target 为 TD λ-return  $G_t^{\lambda}$：</li></ul><script type="math/tex; mode=display">\Delta\mathbf{w}=\alpha\Bigl(G_t^{\lambda}- \hat{v}(S_t, \mathbf{w})\Bigr)\triangledown_{\mathbf{w}}\hat{v}(S_t, \mathbf{w})</script><p>在获取了值函数近似后就可以进行控制了，具体示意图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006115744.png" alt=""></p><h2 id="动作价值函数近似"><a href="#动作价值函数近似" class="headerlink" title="动作价值函数近似"></a>动作价值函数近似</h2><p>动作价值函数近似：</p><script type="math/tex; mode=display">\hat{q}(S, A, \mathbf{w})\approx q_{\pi}(S, A)</script><p>优化目标：最小化MSE</p><script type="math/tex; mode=display">J(\mathbf{w}) = E_{\pi}[(q_{\pi}(S, A) - \hat{q}(S, A, \mathbf{w}))^2]</script><p>使用SGD寻优：</p><script type="math/tex; mode=display">\begin{align} \Delta\mathbf{w} &=-\frac{1}{2}\alpha\triangledown_{\mathbf{w}}J(\mathbf{w})\\ &=\alpha\Bigl(q_{\pi}(S, A)-\hat{q}_{\pi}(S, A, \mathbf{w})\Bigr) \triangledown_{\mathbf{w}}\hat{q}_{\pi}(S, A, \mathbf{w}) \end{align}</script><h1 id="批量方法"><a href="#批量方法" class="headerlink" title="批量方法"></a>批量方法</h1><p>随机梯度下降SGD简单，但是批量的方法可以根据agent的经验来更好的拟合价值函数。</p><h2 id="值函数近似-1"><a href="#值函数近似-1" class="headerlink" title="值函数近似"></a>值函数近似</h2><p>优化目标：批量方法解决的问题同样是  $\hat{v}(s, \mathbf{w})\approx v_{\pi}(s)$</p><p>经验集合  D 包含了一系列的 <state, value> pair：</p><script type="math/tex; mode=display">D=\{<s_1, v_1^{\pi}>, <s_2, v_2^{\pi}>, ..., <s_T, v_T^{\pi}>\}</script><p>根据最小化平方误差之和来拟合 $\hat{v}(s, \mathbf{w})$和 v~π~(s) ，即：</p><script type="math/tex; mode=display">\begin{align}LS(w) &= \sum_{t=1}^{T}(v_{t}^{\pi}-\hat{v}(s_t, \mathbf{w}))^2\\&= E_{D}[(v^{\pi}-\hat{v}(s, \mathbf{w}))^2]\end{align}</script><p>经验回放（Experience Replay）：</p><blockquote><p>给定经验集合：</p><script type="math/tex; mode=display">D=\{<s_1, v_1^{\pi}>, <s_2, v_2^{\pi}>, ..., <s_T, v_T^{\pi}>\}</script><p>Repeat：</p><ol><li>从经验集合中采样状态和价值：<s,v^π^>∼D </li><li>使用SGD进行更新：$\Delta\mathbf{w}=\alpha\Bigl(v^{\pi}-\hat{v}(s, \mathbf{w})\Bigr)\triangledown_{\mathbf{w}}\hat{v}(s, \mathbf{w})$<br>通过上述经验回放，获得最小化平方误差的参数值：</li></ol><p>$\mathbf{w}^{\pi}=\arg\min_{\mathbf{w}}LS(\mathbf{w})$</p></blockquote><p>我们经常听到的 DQN 算法就使用了经验回放的手段，这个后续会在《深度强化学习》中整理。</p><p>通过上述经验回放和不断的迭代可以获取最小平方误差的参数值，然后就可以通过 greedy 的策略进行策略提升，具体如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006120213.png" alt=""></p><h2 id="动作价值函数近似-1"><a href="#动作价值函数近似-1" class="headerlink" title="动作价值函数近似"></a>动作价值函数近似</h2><p>同样的套路：</p><ul><li>优化目标： $\hat{q}(s, a, \mathbf{w})\approx q_{\pi}(s, a)$</li><li>采取包含 <state, action, value> 的经验集合 D </li><li>通过最小化平方误差来拟合</li></ul><p>对于控制环节，我们采取与Q-Learning一样的思路：</p><ul><li>利用之前策略的经验</li><li>但是考虑另一个后继动作 $A’=\pi_{\text{new}}(S_{t+1})$</li><li>朝着另一个后继动作的方向去更新 $\hat{q}(S_t, A_t, \mathbf{w})$，即</li></ul><p>$\delta = R_{t+1} + \gamma\hat{q}(S_{t+1}, \pi{S_{t+1}, \mathbf{\pi}}) - \hat{q}(S_t, A_t, \mathbf{w})$</p><ul><li>梯度：线性拟合情况，$\Delta\mathbf{w}=\alpha\delta\mathbf{x}(S_t, A_t)$</li></ul>]]></content>
    
    
    <summary type="html">Value Function Approximation</summary>
    
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/categories/MARL/"/>
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/tags/MARL/"/>
    
  </entry>
  
  <entry>
    <title>Cross-entrpy Method</title>
    <link href="https://changqingaas.github.io/MARL/MADRL/Cross-entroy%20Method/"/>
    <id>https://changqingaas.github.io/MARL/MADRL/Cross-entroy%20Method/</id>
    <published>2021-10-05T15:00:33.000Z</published>
    <updated>2021-10-05T14:26:06.057Z</updated>
    
    <content type="html"><![CDATA[<h3 id="CEM-amp-amp-RL"><a href="#CEM-amp-amp-RL" class="headerlink" title="CEM &amp;&amp; RL"></a>CEM &amp;&amp; RL</h3><p>注：以下内容引自博文《进化策略优化算法CEM(Cross Entropy Method)》[1]。</p><p>CEM也可以用来求解马尔可夫决策过程，也就是强化学习问题。我们知道，强化学习也是一种动态规划过程，在某个状态下选择某个动作就像在某个节点选择路径一样，整个过程就是一个从初始状态到末状态的路径规划问题，只不过我们希望得到一条能最大化收益的路径。在这种考虑下，就可以用CEM建模了，我们让一条完整的路径成为一个样本x=(s0,a0,s1,a1,…,sn,an)，路径获得的总收益为S(x)=∑Ni=0r(si,ai)，目标是最大化这个S(x)，那么如何采样出这些样本呢？我们可以构建一个pp矩阵：矩阵行表示状态，列表示动作，如pij表示在状态si下执行aj动作的概率，我们通过对这个pp矩阵进行多次采样就可以获得多个样本，然后选出S(x)较高的样本用来更新pp矩阵，不断迭代，最终找到最优p^矩阵。</p><p>这是一种类似于策略迭代(policy iteration)的强化学习方法：通过p矩阵找到在每一步状态下各个动作的概率来形成决策策略，但参数更新并没有用到梯度。从另外一个角度，你也可以认为这是一种值迭代(value iteration)的强化学习方法，此时p矩阵就是经典Q-learning中的Q矩阵，只不过Q矩阵中第i行第j列元素表示的是状态si下动作aj的未来收益的期望，基于贝尔曼方程(Bellman equation)来更新Q值；而p矩阵表示的是概率值，通过交叉墒来更新。</p><p>[1] <a href="https://blog.csdn.net/ppp8300885/article/details/80567682">进化策略优化算法CEM(Cross Entropy Method)</a></p>]]></content>
    
    
    <summary type="html">Cross-entrpy Method</summary>
    
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/categories/MARL/"/>
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/tags/MARL/"/>
    
  </entry>
  
  <entry>
    <title>Model-Free Prediction</title>
    <link href="https://changqingaas.github.io/MARL/MADRL/Model-Free%20Prediction/"/>
    <id>https://changqingaas.github.io/MARL/MADRL/Model-Free%20Prediction/</id>
    <published>2021-10-05T15:00:33.000Z</published>
    <updated>2021-10-05T15:15:35.589Z</updated>
    
    <content type="html"><![CDATA[<p>上篇文章介绍了 Model-based 的通用方法——动态规划，本文内容介绍 Model-Free 情况下 Prediction 问题，即 “Estimate the value function of an unknown MDP”。</p><ul><li>Model-based：MDP已知，即转移矩阵和奖赏函数均已知</li><li>Model-Free：MDP未知</li></ul><h2 id="蒙特卡洛学习"><a href="#蒙特卡洛学习" class="headerlink" title="蒙特卡洛学习"></a>蒙特卡洛学习</h2><p>蒙特卡洛方法（Monte-Carlo Methods，简称MC）也叫做蒙特卡洛模拟，是指使用随机数（或更常见的伪随机数）来解决很多计算问题的方法。其实本质就是，通过尽可能随机的行为产生后验，然后通过后验来表征目标系统。</p><p>在Model-Free的情况下，MC在强化学习中的应用就是获取价值函数，其特点如下：</p><ul><li>MC 可以从完整的 episodes 中学习（no bootstrapping）</li><li>MC 以均值来计算价值，即 value = mean(return)</li><li>MC 只能适用于 episodic MDPs（有限MDPs）</li></ul><h3 id="First-Visit-蒙特卡洛策略评估"><a href="#First-Visit-蒙特卡洛策略评估" class="headerlink" title="First-Visit 蒙特卡洛策略评估"></a>First-Visit 蒙特卡洛策略评估</h3><p>First-Visit Monte-Carlo Policy Evaluation：</p><blockquote><p>评估状态 s  在给定策略 π  下的价值函数 v~π~(s))时，在一次 episode 中，状态 s  在时刻  t <strong>第一次</strong>被访问时，计数器 N(s)←N(s)+1 ，累计价值 S(s)←S(s)+Gt<br>当整个过程结束后，状态 s 的价值  $V(s) = \frac{S(s)}{N(s)}$根据大数定理（Law of Large Numbers）：$V(s) → v_{\pi}(s) \text{ as } N(s) → \infty$</p></blockquote><h3 id="Every-Visit-蒙特卡洛策略评估"><a href="#Every-Visit-蒙特卡洛策略评估" class="headerlink" title="Every-Visit 蒙特卡洛策略评估"></a>Every-Visit 蒙特卡洛策略评估</h3><p>Every-Visit Monte-Carlo Policy Evaluation：</p><blockquote><p>评估状态 s  在给定策略 π 下的价值函数 v~π(~s)时，在一次 episode 中，状态 s 在时刻 t <strong>每次</strong>被访问时，计数器 N(s)←N(s)+1，累计价值 S(s)←S(s)+Gt<br>当整个过程结束后，状态 s 的价值 V(s)=S(s)/N(s)<br>根据大数定理（Law of Large Numbers）：V(s)→v~π~(s) as N(s)→∞</p></blockquote><h3 id="Incremental-Monte-Carlo"><a href="#Incremental-Monte-Carlo" class="headerlink" title="Incremental Monte-Carlo"></a>Incremental Monte-Carlo</h3><p>增量式求平均：<br>The mean μ1,μ2,… of a sequence x1,x2,… . can be computed incrementally：</p><script type="math/tex; mode=display">\begin{align}\mu_k &= \frac{1}{k}\sum_{j=1}^{k}x_j\\&= \frac{1}{k}\Bigl(x_k+\sum_{j=1}^{k-1}x_j \Bigr)\\&= \frac{1}{k}(x_k + (k-1)\mu_{k-1})\\&= \mu_{k-1} + \frac{1}{k}(x_k - \mu_{k-1})\end{align}</script><p>根据上式我们可以得出增量式进行MC更新的公式：<br>每次 episode 结束后，增量式更新 V(s) ，对于每个状态 St ，其对应的 return 为 Gt :</p><p> $N(S_t) ← N(S_t) + 1 \\ V(S_t) ← V(S_t) + \frac{1}{N(S_t)}(G_t - V(S_t))$</p><p>在非静态问题中，更新公式形式可以改为如下：</p><p>$V(S_t) ← V(S_t) + \alpha (G_t - V(S_t))$</p><h2 id="时序差分学习"><a href="#时序差分学习" class="headerlink" title="时序差分学习"></a>时序差分学习</h2><p>时序差分方法（Temporal-Difference Methods，简称TD）特点：</p><ul><li>TD 可以通过 bootstrapping 从非完整的 episodes 中学习</li><li>TD updates a guess towards a guess</li></ul><h3 id="TD-λ"><a href="#TD-λ" class="headerlink" title="TD(λ)"></a>TD(λ)</h3><p>下图为 TD target 在不同 n 下的示意图：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005224847.png" alt=""></p><p>从上图可以看出，当 n 达到终止时，即为一个episode，此时对应的方法为MC，因此从这个角度看，MC属于TD的特殊情况。</p><h3 id="n-step-Return"><a href="#n-step-Return" class="headerlink" title="n-step Return"></a>n-step Return</h3><p>n-step returns 可以表示如下：<br>n=1 时：$G_{t}^{(1)} = R_{t+1} + \gamma V(S_{t+1})$<br>n=2 时：$G_{t}^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 V(S_{t+2})$<br>…<br>n=∞ 时：$G_{t}^{\infty} = R_{t+1} + \gamma R_{t+2} + … + \gamma^{T-1} R_T)$<br>因此，n-step return $G_{t}^{(n)} = R_{t+1} + \gamma R_{t+2} + … + \gamma^{n}V(S_{t+n})$</p><p>n-step TD 更新公式：</p><p>$V(S_t) ← V(S_t) + \alpha (G_t^{(n)} - V(S_t))$</p><h3 id="Forward-View-of-TD-λ"><a href="#Forward-View-of-TD-λ" class="headerlink" title="Forward View of TD(λ)"></a>Forward View of TD(λ)</h3><p>我们能否把所有的 n-step return 组合起来？答案肯定是可以，组合后的return被称为是λ-return，其中λ是为了组合不同的n-step returns引入的权重因子。</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005225422.png" alt=""></p><p>λ-return:</p><p>$G_t^{\lambda} = (1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}G_t^{(n)}$</p><p>Forward-view TD(λλ)：</p><p>$V(S_t) ← V(S_t) + \alpha\Bigl(G_t^{\lambda} - V(S_t)\Bigr)$</p><p>TD(λ)对应的权重公式为 (1−λ)λ^n−1^，分布图如下所示：</p><p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181029093216246-346895182.png" alt=""></p><p>Forward-view TD(λ)的特点：</p><ul><li>Update value function towards the λ-return</li><li>Forward-view looks into the future to compute GλtGtλ</li><li>Like MC, can only be computed from complete episodes</li></ul><h3 id="Backward-View-TD-λ"><a href="#Backward-View-TD-λ" class="headerlink" title="Backward View TD(λ)"></a>Backward View TD(λ)</h3><ul><li>Forward view provides theory</li><li>Backward view provides mechanism</li><li>Update online, every step, from incomplete sequences</li></ul><p>带有资格迹的TD(λλ)：</p><script type="math/tex; mode=display">\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\\V(s) ← V(s) + \alpha \delta_t E_t(s)</script><p>其中δt为TD-error，Et(s)为资格迹。</p><p><strong>资格迹(Eligibility Traces)</strong></p><blockquote><p>资格迹本质就是对于频率高的，最近的状态赋予更高的信任（credit）/ 权重。</p></blockquote><p>下图是对资格迹的一个描述：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005230115.png" alt=""></p><p>关于TD(λ)有一个结论：</p><blockquote><p>The sum of offline updates is identical for forward-view and backward-view TD(λ).</p></blockquote><p>这一块的内容不再深入介绍了，感兴趣的可以看Sutton的书和David的教程。</p><h2 id="蒙特卡洛学习-vs-时序差分学习"><a href="#蒙特卡洛学习-vs-时序差分学习" class="headerlink" title="蒙特卡洛学习 vs. 时序差分学习"></a>蒙特卡洛学习 vs. 时序差分学习</h2><h3 id="MC与TD异同点"><a href="#MC与TD异同点" class="headerlink" title="MC与TD异同点"></a>MC与TD异同点</h3><p>相同点：都是从经验中在线的学习给定策略 π 的价值函数 v~π~</p><p>不同点：</p><ul><li>Incremental every-visit Monte-Carlo：朝着真实的 return G~t~  更新 V(S~t~) </li></ul><p>$V(S_t) ← V(S_t) + \alpha (\textcolor{Red}{G_t}   - V(S_t))$</p><ul><li><p>Simplest temporal-difference learning algorithm: TD(0)</p><ul><li>朝着已预估的 return  $\color{Red}{R_{t+1} + \gamma V(S_{t+1})}$ 更新 V(S~t~) </li></ul><script type="math/tex; mode=display">V(S_t) ← V(S_t) + \alpha (\textcolor{Red}{R_{t+1} + \gamma V(S_{t+1})} - V(S_t))</script><ul><li>$\color{Red}{R_{t+1} + \gamma V(S_{t+1})}$称为是 TD target</li><li>$\color{Red}{R_{t+1} + \gamma V(S_{t+1})}−V(S_{t})$ 称为是 TD error</li></ul></li></ul><p>下图以 Drive Home 举例说明两者的不同，MC 只能在回家后才能改变对回家时间的预判，而 TD 在每一步中不断根据实际情况来调整自己的预判。</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005231045.png" alt=""></p><h2 id="MC与TD优缺点"><a href="#MC与TD优缺点" class="headerlink" title="MC与TD优缺点"></a>MC与TD优缺点</h2><h3 id="学习方式"><a href="#学习方式" class="headerlink" title="学习方式"></a>学习方式</h3><ul><li>TD 可以在知道最后结果之前学习（如上图举例）<ul><li>TD can learn online after every step</li><li>MC must wait until end of episode before return is known</li></ul></li><li>TD 可以在不存在最后结果的情况下学习（比如无限/连续MDPs）<ul><li>TD can learn from incomplete sequences</li><li>MC can only learn from complete sequences</li><li>TD works in continuing (non-terminating) environments</li><li>MC only works for episodic (terminating) environments</li></ul></li></ul><h3 id="方差与偏差"><a href="#方差与偏差" class="headerlink" title="方差与偏差"></a>方差与偏差</h3><ul><li>MC has high variance, zero bias（高方差，零偏差）<ul><li>Good convergence properties</li><li>Not very sensitive to initial value</li><li>Very simple to understand and use</li></ul></li><li>TD has low variance, some bias（低方差，存在一定偏差）<ul><li>Usually more efficient than MC</li><li>TD(0) converges to vπ(s)vπ(s)</li><li>More sensitive to initial value</li></ul></li></ul><p>关于 MC 和 TD 中方差和偏差问题的解释：</p><blockquote><ul><li>MC 更新基于真实的 return  $G_t = R_{t+1} + \gamma R_{t+2} + … + \gamma^{T-1}R_{T}$是 v~π~(St)  的无偏估计。</li><li>真实的TD target $R_{t+1} + \gamma v_{\pi}(S_{t+1})$ 也是 vπ(St) 的无偏估计。但是实际更新时用的 TD target $R_{t+1} + \gamma V(S_{t+1})$ 是 vπ(St)  的有偏估计。</li><li>TD target 具有更低的偏差：<ul><li>Return 每次模拟依赖于许多的随机动作、转移概率以及回报</li><li>TD target 每次只依赖一次随机动作、转移概率以及回报</li></ul></li></ul></blockquote><h3 id="马尔可夫性"><a href="#马尔可夫性" class="headerlink" title="马尔可夫性"></a>马尔可夫性</h3><ul><li>TD exploits Markov property<ul><li>Usually more efficient in Markov environments</li></ul></li><li>MC does not exploit Markov property<ul><li>Usually more effective in non-Markov environments</li></ul></li></ul><h2 id="DP、MC以及TD-0"><a href="#DP、MC以及TD-0" class="headerlink" title="DP、MC以及TD(0)"></a>DP、MC以及TD(0)</h2><p>首先我们从 backup tree 上去直观地认识三者的不同。</p><ul><li>DP backup tree：Full-Width step（完整的step）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005231417.png" alt=""></p><ul><li>MC backup tree：完整的episode</li></ul><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005231438.png" alt=""></p><ul><li>TD(0) backup tree：单个step</li></ul><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005231514.png" alt=""></p><h2 id="Bootstrapping-vs-Sampling"><a href="#Bootstrapping-vs-Sampling" class="headerlink" title="Bootstrapping vs. Sampling"></a>Bootstrapping vs. Sampling</h2><p>Bootstrapping：基于已预测的值进行更新</p><ul><li>DP bootstraps</li><li>MC does not bootstrap</li><li>TD bootstraps</li></ul><p>Sampling：基于采样的期望来更新</p><ul><li>DP does not sample（model-based methods don’t need sample）</li><li>MC samples（model-free methods need sample）</li><li>TD samples（model-free methods need sample）</li></ul><p>下图从宏观的视角显示了 RL 的几种基本方法的区别：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005231530.png" alt=""></p>]]></content>
    
    
    <summary type="html">Model-Free Prediction</summary>
    
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/categories/MARL/"/>
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/tags/MARL/"/>
    
  </entry>
  
  <entry>
    <title>Model-Free Control</title>
    <link href="https://changqingaas.github.io/MARL/MADRL/Model-Free%20Control/"/>
    <id>https://changqingaas.github.io/MARL/MADRL/Model-Free%20Control/</id>
    <published>2021-10-05T15:00:33.000Z</published>
    <updated>2021-10-06T03:46:55.610Z</updated>
    
    <content type="html"><![CDATA[<p>上篇总结了 Model-Free Predict 问题及方法，本文内容介绍 Model-Free Control 方法，即 “Optimise the value function of an unknown MDP”。</p><p>在这里说明下，Model-Free Predict/Control 不仅适用于 Model-Free 的情况，其同样适用于 MDP 已知的问题：</p><ul><li>MDP model is unknown, but experience can be sampled.</li><li>MDP model is known, but is too big to use, except by samples.</li></ul><p>在正式介绍 Model-Free Control 方法之前，我们先介绍下 On-policy Learning 及 Off-policy Learning。</p><h2 id="On-policy-Learning-vs-Off-policy-Learning"><a href="#On-policy-Learning-vs-Off-policy-Learning" class="headerlink" title="On-policy Learning vs. Off-policy Learning"></a>On-policy Learning vs. Off-policy Learning</h2><p>On-policy Learning：</p><ul><li>“Learn on the job”</li><li>Learn about policy π from experience sampled from π（即采样的策略与学习的策略一致）</li></ul><p>Off-policy Learning：</p><ul><li>“Look over someone’s shoulder”</li><li>Learn about policy π from experience sampled from μ（即采样的策略与学习的策略不一致）</li></ul><h2 id="On-Policy-Monte-Carlo-Learning"><a href="#On-Policy-Monte-Carlo-Learning" class="headerlink" title="On-Policy Monte-Carlo Learning"></a>On-Policy Monte-Carlo Learning</h2><h3 id="Generalized-Policy-Iteration"><a href="#Generalized-Policy-Iteration" class="headerlink" title="Generalized Policy Iteration"></a>Generalized Policy Iteration</h3><p>具体的 Control 方法，在《动态规划》一文中我们提到了 Model-based 下的广义策略迭代 GPI 框架，那在 Model-Free 情况下是否同样适用呢？<br>如下图为 Model-based 下的广义策略迭代 GPI 框架，主要分两部分：策略评估及基于 Greedy 策略的策略提升。</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006105433.png" alt=""></p><h4 id="Model-Free-策略评估"><a href="#Model-Free-策略评估" class="headerlink" title="Model-Free 策略评估"></a>Model-Free 策略评估</h4><p>在《Model-Free Predict》中我们分别介绍了两种 Model-Free 的策略评估方法：MC 和 TD。我们先讨论使用 MC 情况下的 Model-Free 策略评估。<br>如上图GPI框架所示：</p><ul><li>基于 V(s) 的贪婪策略提升需要 MDP 已知：</li></ul><script type="math/tex; mode=display">\pi'(s) = \arg\max_{a\in A}\Bigl(R_{s}^{a}+P_{ss'}^{a}V(s')\Bigr)</script><ul><li>基于 Q(s,a) 的贪婪策略提升不需要 MDP 已知，即 Model-Free：</li></ul><script type="math/tex; mode=display">\pi'(s) = \arg\max_{a\in A}Q(s, a)</script><p>因此 Model-Free 下需要对 Q(s,a) 策略评估，整个GPI策略迭代也要基于 Q(s,a) </p><h4 id="Model-Free-策略提升"><a href="#Model-Free-策略提升" class="headerlink" title="Model-Free 策略提升"></a>Model-Free 策略提升</h4><p>确定了策略评估的对象，那接下来要考虑的就是如何基于策略评估的结果 Q(s,a) 进行策略提升。<br>由于 Model-Free 的策略评估基于对经验的 samples（即评估的 q(s,a)  存在 bias），因此我们在这里不采用纯粹的 greedy 策略，防止因为策略评估的偏差导致整个策略迭代进入局部最优，而是采用具有 explore 功能的  ϵ-greedy 算法：</p><script type="math/tex; mode=display">\pi(a|s) = \begin{cases}&\frac{\epsilon}{m} + 1 - \epsilon, &\text{if } a^*=\arg\max_{a\in A}Q(s, a)\\&\frac{\epsilon}{m}, &\text{otherwise}\end{cases}</script><p>因此，我们确定了 Model-Free 下的 Monto-Carlo Control：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006105953.png" alt=""></p><h3 id="GLIE"><a href="#GLIE" class="headerlink" title="GLIE"></a>GLIE</h3><p>先直接贴下David的课件，GLIE 介绍如下：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006110036.png" alt=""></p><p>对于 ϵ-greedy 算法而言，如果 ϵ 随着迭代次数逐步减为0，那么  ϵ-greedy 是 GLIE，即：</p><script type="math/tex; mode=display">\epsilon_{k} = \frac{1}{k}</script><h4 id="GLIE-Monto-Carlo-Control"><a href="#GLIE-Monto-Carlo-Control" class="headerlink" title="GLIE Monto-Carlo Control"></a>GLIE Monto-Carlo Control</h4><ul><li>对于 episode 中的每个状态 S~t~  和动作 A~t~</li></ul><script type="math/tex; mode=display">N(S_t, A_t) ← N(S_t, A_t) + 1 \\Q(S_t, A_t) ← Q(S_t, A_t) + \frac{1}{N(S_t, A_t)}(G_t - Q(S_t, A_t))</script><ul><li>基于新的动作价值函数提升策略：</li></ul><script type="math/tex; mode=display">\epsilon ← \frac{1}{k}\\\pi ← \epsilon\text{-greedy}(Q)</script><p>定理：GLIE Monto-Carlo Control 收敛到最优的动作价值函数，即：</p><script type="math/tex; mode=display">Q(s, a) → q_*(s, a)</script><h2 id="On-Policy-Temporal-Difference-Learning"><a href="#On-Policy-Temporal-Difference-Learning" class="headerlink" title="On-Policy Temporal-Difference Learning"></a>On-Policy Temporal-Difference Learning</h2><h3 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h3><p>我们之前总结过 TD 相对 MC 的优势：</p><ul><li>低方差</li><li>Online</li><li>非完整序列</li></ul><p>那么一个很自然的想法就是在整个控制闭环中用 TD 代替 MC：</p><ul><li>使用 TD 来计算 Q(S,A) </li><li>仍然使用  ϵ-greedy 策略提升</li><li>每一个 step 进行更新</li></ul><p>通过上述改变就使得 On-Policy 的蒙特卡洛方法变成了著名的 Sarsa。</p><ul><li>更新动作价值函数</li></ul><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006110844.png" alt=""></p><ul><li>Control</li></ul><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006110937.png" alt=""></p><p>Sarsa算法的伪代码如下：<br><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006111038.png" alt="img"></p><h3 id="Sarsa-λ"><a href="#Sarsa-λ" class="headerlink" title="Sarsa(λ)"></a>Sarsa(λ)</h3><p>n-step Sarsa returns 可以表示如下：<br>n=1 时：$q_{t}^{(1)} = R_{t+1} + \gamma Q(S_{t+1})$<br>n=2 时：$q_{t}^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 Q(S_{t+2})$<br>…<br>n=∞ 时：$q_{t}^{\infty} = R_{t+1} + \gamma R_{t+2} + … + \gamma^{T-1} R_T)$<br>因此，n-step return $q_{t}^{(n)} = R_{t+1} + \gamma R_{t+2} + … + \gamma^{n}Q(S_{t+n})$</p><p>n-step Sarse 更新公式：</p><p>$Q(S_t,A_t) ← Q(S_t,A_t) + \alpha (q_t^{(n)} - Q(S_t,A_t))$</p><p> 具体的 Sarsa(λ) 算法伪代码如下：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006111817.png" alt="img"></p><p>其中 E(s,a)   为资格迹。</p><p>下图为 Sarsa(λ) 用于 Gridworld 例子的示意图：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006112106.png" alt=""></p><h2 id="Off-Policy-Learning"><a href="#Off-Policy-Learning" class="headerlink" title="Off-Policy Learning"></a>Off-Policy Learning</h2><p>Off-Policy Learning 的特点是评估目标策略 π(a|s) 来计算 v~π~(s) 或者 q~π~(s,a) 但是跟随行为策略 {S~1~,A~1~,R~2~,…,S~T~}∼μ(a|s) </p><p>Off-Policy Learning 有什么意义？</p><ul><li>Learn from observing humans or other agents</li><li>Re-use experience generated from old policies π~1~,π~2~,…,π~t−1~ </li><li>Learn about optimal policy while following exploratory policy</li><li>Learn about multiple policies while following one policy</li></ul><h3 id="重要性采样"><a href="#重要性采样" class="headerlink" title="重要性采样"></a>重要性采样</h3><p>重要性采样的目的是：Estimate the expectation of a different distribution。</p><script type="math/tex; mode=display">\begin{align}E_{X\sim P}[f(X)]&= \sum P(X)f(X)\\&= \sum Q(X)\frac{P(X)}{Q(X)}f(X)\\&= E_{X\sim Q}[\frac{P(X)}{Q(X)}f(X)]\end{align}</script><h4 id="Off-Policy-MC-重要性采样"><a href="#Off-Policy-MC-重要性采样" class="headerlink" title="Off-Policy MC 重要性采样"></a>Off-Policy MC 重要性采样</h4><p>使用策略 π  产生的 return 来评估 μ ：</p><script type="math/tex; mode=display">G_t^{\pi/\mu} = \frac{\pi(A_t|S_t)}{\mu(A_t|S_t)} \frac{\pi(A_{t+1}|S_{t+1})}{\mu(A_{t+1}|S_{t+1})}...\frac{\pi(A_T|S_T)}{\mu(A_T|S_T)}G_t</script><p>朝着正确的 return 方向去更新价值：</p><script type="math/tex; mode=display">V(S_t) ← V(S_t) + \alpha\Bigl(\textcolor{Red}{G_t^{\pi/\mu}}-V(S_t)\Bigr)</script><p>需要注意两点：</p><ul><li>Cannot use if μ  is zero when π  is non-zero</li><li>重要性采样会显著性地提升方差</li></ul><h4 id="Off-Policy-TD-重要性采样"><a href="#Off-Policy-TD-重要性采样" class="headerlink" title="Off-Policy TD 重要性采样"></a>Off-Policy TD 重要性采样</h4><p>TD 是单步的，所以使用策略 π  产生的 TD targets 来评估 μ ：</p><script type="math/tex; mode=display">V(S_t) ← V(S_t) + \alpha\Bigl(\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}(R_{t+1}+\gamma V(S_{t+1}))-V(S_t)\Bigr)</script><ul><li>方差比MC版本的重要性采样低很多</li></ul><h3 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h3><p>前面分别介绍了对价值函数 V(s)  进行 off-policy 学习，现在我们讨论如何对动作价值函数 Q(s,a)  进行 off-policy 学习：</p><ul><li><p>不需要重要性采样</p></li><li><p>使用行为策略选出下一步的动作：$A_{t+1}\sim\mu(·|S_t)$</p></li><li><p>但是仍需要考虑另一个后继动作：$A’\sim\pi(·|S_t)$</p></li><li><p>朝着另一个后继动作的价值更新 $Q(S_t, A_t)$</p></li></ul><script type="math/tex; mode=display">Q(S_t, A_t) ← Q(S_t, A_t) + \alpha\Bigl(R_{t+1}+\gamma Q(S_{t+1}, A')-Q(S_t, A_t)\Bigr)</script><p>讨论完对动作价值函数的学习，我们接着看如何通过 Q-Learning 进行 Control：</p><ul><li>行为策略和目标策略均改进</li><li>目标策略 π  以greedy方式改进：</li></ul><script type="math/tex; mode=display">\pi(S_t) = \arg\max_{a'}Q(S_{t+1}, a')</script><ul><li>行为策略 μ  以  ϵ-greedy 方式改进</li><li>Q-Learning target：</li></ul><script type="math/tex; mode=display">\begin{align}&R_{t+1}+\gamma Q(S_{t+1}, A')\\=&R_{t+1}+\gamma Q\Bigl(S_{t+1}, \arg\max_{a'}Q(S_{t+1}, a')\Bigr)\\=&R_{t+1}+\max_{a'}\gamma Q(S_{t+1}, a')\end{align}</script><p>Q-Learning 的 backup tree 如下所示：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006114145.png" alt=""></p><p>关于 Q-Learning 的结论：</p><blockquote><p>Q-learning control converges to the optimal action-value function, Q(s,a)→q~∗~(s,a) </p></blockquote><p>Q-Learning 算法具体的伪代码如下：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006114205.png" alt=""></p><p>对比 Sarsa 与 Q-Learning 可以发现两个最重要的区别：</p><ul><li>TD target 公式不同</li><li>Q-Learning 中下一步的动作从行为策略中选出，而不是目标策略</li></ul><h2 id="DP-vs-TD"><a href="#DP-vs-TD" class="headerlink" title="DP vs. TD"></a>DP vs. TD</h2><p>两者的区别见下表：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006114507.png" alt="">  </p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006114532.png" alt=""></p>]]></content>
    
    
    <summary type="html">Model-Free Control</summary>
    
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/categories/MARL/"/>
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/tags/MARL/"/>
    
  </entry>
  
  <entry>
    <title>动态规划</title>
    <link href="https://changqingaas.github.io/MARL/MADRL/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    <id>https://changqingaas.github.io/MARL/MADRL/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</id>
    <published>2021-10-05T15:00:33.000Z</published>
    <updated>2021-10-05T14:25:54.923Z</updated>
    
    <content type="html"><![CDATA[<h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><p>动态规划（Dynamic Programming，简称DP）是一种通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法。</p><p>动态规划常常适用于具有如下性质的问题：</p><ul><li>具有最优子结构（Optimal substructure）<ul><li>Principle of optimality applies</li><li>Optimal solution can be decomposed into subproblems</li></ul></li><li>重叠子问题（Overlapping subproblems）<ul><li>Subproblems recur many times</li><li>Solutions can be cached and reused</li></ul></li></ul><p>动态规划方法所耗时间往往远少于朴素解法。</p><p>马尔可夫决策过程MDP满足上述两个性质：</p><ul><li>贝尔曼方程提供了递归分解的结构；</li><li>价值函数可以保存和重复使用递归时的结果。</li></ul><h2 id="使用动态规划解决MDP-MRP"><a href="#使用动态规划解决MDP-MRP" class="headerlink" title="使用动态规划解决MDP/MRP"></a>使用动态规划解决MDP/MRP</h2><p>动态规划需要满足MDP过程是已知的（model-based）。</p><ul><li>For Predict：<ul><li>Input：MDP  <S,A,P,R,$\gamma$> 和策略 π 或者是 MRP <S,P,R,$\gamma$> </li><li>Output：价值函数 v~π~</li></ul></li><li>For Control：<ul><li>Input：MDP <S,A,P,R,$\gamma$> </li><li>Output：最优价值函数 v∗  或者最优策略 π∗</li></ul></li></ul><h2 id="策略评估"><a href="#策略评估" class="headerlink" title="策略评估"></a>策略评估</h2><p>策略评估（Policy Evaluation）指的是计算给定策略的价值，解决的问题是 “How to evaluate a policy”。</p><p>策略评估的思路：迭代使用贝尔曼期望方程（关于 MDP 的贝尔曼期望方程形式见《马尔可夫决策过程》）。</p><p>策略评估过程如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005220219.png" style="zoom: 67%;" /></p><script type="math/tex; mode=display">v_{k+1} = \sum_{a\in A}\pi(a|s) \Bigl( R_{s}^a + \gamma\sum_{s'\in S}P_{ss'}^a v_{k}(s') \Bigr)</script><p>使用向量形式表示：</p><script type="math/tex; mode=display">\mathbf{v^{k+1}} = \mathbf{R^{\pi}} + \gamma \mathbf{P^{\pi}v^{k}}</script><h2 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h2><p>策略迭代（Policy Iteration，简称PI）解决的问题是 “How to improve a policy”。</p><p>给定一个策略 π ：</p><ul><li>评估策略 π </li></ul><script type="math/tex; mode=display">v_{\pi}(s) = E[R_{t+1} + \gamma R_{t+2} + ...| S_t = s]</script><ul><li>提升策略：通过采用贪婪方法来提升策略：<script type="math/tex; mode=display">\pi ' = \text{greedy}(v_{\pi})</script></li></ul><p>可以证明，策略迭代不断进行总是能收敛到最优策略，即 π′=π∗</p><p>策略迭代可以使用下图来形式化的描述：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005221105.png" alt=""></p><h2 id="广义策略迭代"><a href="#广义策略迭代" class="headerlink" title="广义策略迭代"></a>广义策略迭代</h2><p>通过上述提到的策略评估我们不难发现，策略评估是一个不断迭代的过程：</p><script type="math/tex; mode=display">v_{\pi}(s) = E[R_{t+1} + \gamma R_{t+2} + ...| S_t = s]</script><p>那么问题来了，Does policy evaluation need to converge to vπvπ?<br>我们是不是可以引入一个停止规则或者规定在迭代 kk 次后停止策略评估？<br>再进一步想，我们为什么不在每次策略评估的迭代过程中进行策略提升（等同于策略评估迭代1次后停止）？<br>注：这和后续要介绍的值迭代等价。</p><p>因此我们可以把上述策略迭代的过程一般化，即广义策略迭代（Generalised Policy Iteration，简称GPI）框架：</p><p> <img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005221407.png" alt=""></p><h1 id="值迭代"><a href="#值迭代" class="headerlink" title="值迭代"></a>值迭代</h1><p>介绍值迭代之前，我们先介绍下最优化原理。</p><h2 id="最优化原理"><a href="#最优化原理" class="headerlink" title="最优化原理"></a>最优化原理</h2><p>最优化原理（Principle of Optimality）定义：</p><blockquote><p>一个过程的最优决策具有这样的性质：即无论其初始状态和初始决策如何，其今后诸策略对以第一个决策所形成的状态作为初始状态的过程而言，必须构成最优策略。</p></blockquote><p>最优化原理如果用数学化一点的语言来描述的话就是：</p><blockquote><p>以状态 ss 为起始点，策略 π(a|s) 可以得到最优值   $v_{\pi}(s) = v_*(s)$当且仅当：</p><ul><li>任意状态 s′ 对于状态 s  均可达；</li><li>以状态 s′ 为起始点，策略 π 可以得到最优值$v_{\pi}(s’) = v_*(s’)$</li></ul></blockquote><p>根据最优化原理可知，如果我们得到了子问题的解 v∗(s′)v∗(s′)，那么以状态 ss 为起始点的最优解 v∗(s)v∗(s) 可以通过一步回退（one-step lookahead）就能获取：</p><script type="math/tex; mode=display">v_*(s) ← \max_{a\in A}\Bigl(R_s^a + \gamma \sum_{s'\in S}P_{ss'}^{a}v_*(s') \Bigr)</script><p>也就是说，我们可以从最后开始向前回退从而得到最优解，值迭代就是基于上述思想进行迭代更新的。</p><h2 id="MDP值迭代"><a href="#MDP值迭代" class="headerlink" title="MDP值迭代"></a>MDP值迭代</h2><p>值迭代（Value Iteration，简称VI）解决的问题也是 “Find optimal policy ππ”。<br>但是不同于策略迭代使用贝尔曼期望方程的是，值迭代使用贝尔曼最优方程进行迭代提升。</p><p>值迭代与策略迭代不同的地方在于：</p><ul><li>Use Bellman optimal function, rather than Bellman expectation function</li><li>Unlike policy iteration, there is no explicit policy</li><li>Intermediate value functions may not correspond to any policy</li></ul><p>如下图所示：</p><p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181028144014755-573375074.png" alt=""></p><script type="math/tex; mode=display">v_{k+1}(s) = \max_{a\in A}\Bigl(R_s^a + \gamma\sum_{s'\in S}P_{ss'}^a v_k(s') \Bigr)</script><p>对应的向量表示为：</p><script type="math/tex; mode=display">\mathbf{v}_{k+1} = \max_{a\in A}\mathbf{R}^a + \gamma \mathbf{P^av}^k</script><p>下图为三种方法的总结：</p><p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181028144022645-1891229163.png" alt=""></p><h2 id="动态规划扩展"><a href="#动态规划扩展" class="headerlink" title="动态规划扩展"></a>动态规划扩展</h2><h3 id="异步动态规划（Asynchronous-Dynamic-Programming）"><a href="#异步动态规划（Asynchronous-Dynamic-Programming）" class="headerlink" title="异步动态规划（Asynchronous Dynamic Programming）"></a>异步动态规划（Asynchronous Dynamic Programming）</h3><ul><li>In-place dynamic programming</li><li>Prioritised sweeping</li><li>Real-time dynamic programming</li></ul><h3 id="Full-Width-Backups-vs-Sample-Backups"><a href="#Full-Width-Backups-vs-Sample-Backups" class="headerlink" title="Full-Width Backups vs. Sample Backups"></a>Full-Width Backups vs. Sample Backups</h3><h4 id="Full-Width-Backups"><a href="#Full-Width-Backups" class="headerlink" title="Full-Width Backups"></a>Full-Width Backups</h4><ul><li>DP uses full-width backups（DP is model-based）<ul><li>Every successor state and action is considered</li><li>Using knowledge of the MDP transitions and reward function</li></ul></li><li>DP is effective for medium-sized problems (millions of states)</li><li>For large problems, DP suffers Bellman’s curse of dimensionality（维度灾难）</li></ul><blockquote><p>维度灾难：Number of states n=|S|  grows exponentially with number of state variables</p></blockquote><ul><li>Even one backup can be too expensive</li></ul><h4 id="Sample-Backups"><a href="#Sample-Backups" class="headerlink" title="Sample Backups"></a>Sample Backups</h4><p>后续将要讨论的时序差分方法</p><ul><li>Using sample rewards and sample transitions ⟨S,A,R,S’⟩ </li><li>Instead of reward function R and transition dynamics P</li><li>Advantages:<ul><li>Model-free: no advance knowledge of MDP required</li><li>Breaks the curse of dimensionality through sampling</li><li>Cost of backup is constant, independent of n=|S|</li></ul></li></ul>]]></content>
    
    
    <summary type="html">Model-based 的通用方法——动态规划</summary>
    
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/categories/MARL/"/>
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/tags/MARL/"/>
    
  </entry>
  
  <entry>
    <title>强化学习介绍</title>
    <link href="https://changqingaas.github.io/MARL/MADRL/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/"/>
    <id>https://changqingaas.github.io/MARL/MADRL/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/</id>
    <published>2021-10-05T15:00:33.000Z</published>
    <updated>2021-10-05T14:26:00.215Z</updated>
    
    <content type="html"><![CDATA[<h2 id="强化学习四元素"><a href="#强化学习四元素" class="headerlink" title="强化学习四元素"></a>强化学习四元素</h2><ul><li>策略（Policy）：环境的感知状态到行动的映射方式。</li><li>反馈（Reward）：环境对智能体行动的反馈。</li><li>价值函数（Value Function）：评估状态的价值函数，状态的价值即从当前状态开始，期望在未来获得的奖赏。</li><li>环境模型（Model）：模拟环境的行为</li></ul><h2 id="强化学习的特点"><a href="#强化学习的特点" class="headerlink" title="强化学习的特点"></a>强化学习的特点</h2><ul><li>起源于动物学习心理学的试错法（trial-and-error），因此符合行为心理学。</li><li>寻求探索（exploration）和采用（exploitation）之间的权衡：强化学习一面要采用（exploitation）已经发现的有效行动，另一方面也要探索（exploration）那些没有被认可的行动，已找到更好的解决方案。</li><li>考虑整个问题而不是子问题。</li><li>通用AI解决方案。</li></ul><h2 id="强化学习-vs-机器学习"><a href="#强化学习-vs-机器学习" class="headerlink" title="强化学习 vs. 机器学习"></a>强化学习 vs. 机器学习</h2><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005205458.png" alt="img"></p><p>强化学习与其他机器学习的不同：</p><ul><li>强化学习更加专注于在线规划，需要在探索（explore 未知领域）和采用（exploit 现有知识）之间找到平衡。</li><li>强化学习不需要监督者，只需要获取环境的反馈。</li><li>反馈是延迟的，不是立即生成的。</li><li>时间在强化学习中很重要，其数据为序列数据，并不满足独立同分布假设（i.i.d）。</li></ul><h2 id="强化学习-vs-监督学习"><a href="#强化学习-vs-监督学习" class="headerlink" title="强化学习 vs. 监督学习"></a>强化学习 vs. 监督学习</h2><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005205751.png" alt="img"></p><p>两者的目标都是学习一个model，而区别在于：</p><p>监督学习：</p><ul><li>Open loop</li><li>Learning from labeled data</li><li>Passive data</li></ul><p>强化学习：</p><ul><li>Closed loop</li><li>Learning from decayed reward</li><li>Explore environment</li></ul><h2 id="强化学习-vs-进化算法"><a href="#强化学习-vs-进化算法" class="headerlink" title="强化学习 vs. 进化算法"></a>强化学习 vs. 进化算法</h2><p>进化算法（Evolutionary Algorithms，简称EA）是通过生物进化优胜略汰，适者生存的启发而发展的一类算法，通过种群不断地迭代达到优化的目标。 </p><p>进化算法最大的优点在于整个优化过程是gradients-free的，其思想可以通过下图表示：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005205858.gif" alt=""></p><p>RL和EA虽然都属于优化问题的求解框架，而且两者都需要大量的算力，但是两者有着本质上的区别。</p><p>Sutton在其强化学习介绍一书中也重点谈到了RL与EA的区别</p><ul><li>RL通过与环境交互来进行学习，而EA通过种群迭代来进行学习；</li><li>RL通过最大化累计回报来解决序列问题，而EAs通过最大化适应函数（Fitness Function）来寻求单步最优；</li><li><p>RL对于state过于依赖，而EA在agent不能准确感知环境的状态类问题上也能适用。</p><p>很多研究也尝试通过将EA和RL结合解决优化问题， </p></li></ul><h2 id="强化学习分类"><a href="#强化学习分类" class="headerlink" title="强化学习分类"></a>强化学习分类</h2><p>强化学习分类比较多样：</p><ul><li>按照环境是否已知可以分为Model-based &amp; Model-free；</li><li>按照学习方式可以分为On-Policy &amp; Off-Policy；</li><li>按照学习目标可以分为Value-based &amp; Policy-based。</li></ul><p>下图为根据环境是否已知进行细分的示意图：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005210120.png" alt=""></p><h2 id="强化学习相关推荐资料"><a href="#强化学习相关推荐资料" class="headerlink" title="强化学习相关推荐资料"></a>强化学习相关推荐资料</h2><ul><li>Reinforcement Learning: An Introduction, Richard S. Sutton and Andrew G. Barto：介绍强化学习很全面的一本书籍，相关的电子书及源码见<a href="http://incompleteideas.net/book/the-book.html">这里</a>。</li><li>David Silver在UCL的强化学习视频教程：介绍强化学习的视频教程，基本与Sutton的书籍可以配套学习，Silver来自于Google Deepmind，视频和课件可以从<a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">Silver的主页</a>获取，中文字幕版视频YouTube链接点<a href="https://www.youtube.com/playlist?list=PLjSwXXbVlK6K2enbNmPGjnmB8QBRgCv5s">这里</a>。</li><li>强化学习在阿里的技术演进与业务创新：介绍强化学习在阿里巴巴的落地，可以拓展强化学习应用的业务思路，电子版见<a href="https://pan.baidu.com/s/1jMj1e5zt_g3R7zgWRJ5t2A">这里</a>，密码：yh48。</li><li>Tutorial: Deep Reinforcement Learning：同样来自于Sliver的一个课件，主要针对RL与DL的结合进行介绍，电子版见<a href="https://pan.baidu.com/s/1jLAHZXJqsWg8JVHDBcunvw">这里</a>，密码：9mrp。</li><li>莫烦PYTHON强化学习视频教程：可以通过简短的视频概括地了解强化学习相关内容，适合于入门的同学，视频见<a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/">这里</a>。</li><li>OpenAI Gym：Gym is a toolkit for developing and comparing reinforcement learning algorithms，Gym包含了很多的控制游戏（比如过山车、二级立杆、Atari游戏等），并提供了非常好的接口可以学习，链接见<a href="https://gym.openai.com/docs/">这里</a>。</li><li>Lil’Log：介绍DL和RL的一个优质博客，RL相关包括RL介绍、Policy Gradients算法介绍及Deep RL结合Tensorflow和Gym的源码实现，主页链接见<a href="https://lilianweng.github.io/lil-log/">这里</a>。</li></ul><p>转载自：<a href="https://www.cnblogs.com/maybe2030/p/9862353.html#_label0">[Reinforcement Learning] 强化学习介绍 - Poll的笔记 - 博客园 (cnblogs.com)</a></p>]]></content>
    
    
    <summary type="html">强化学习介绍</summary>
    
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/categories/MARL/"/>
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/tags/MARL/"/>
    
  </entry>
  
  <entry>
    <title>马尔可夫决策过程</title>
    <link href="https://changqingaas.github.io/MARL/MADRL/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"/>
    <id>https://changqingaas.github.io/MARL/MADRL/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/</id>
    <published>2021-10-05T15:00:33.000Z</published>
    <updated>2021-10-05T14:25:37.449Z</updated>
    
    <content type="html"><![CDATA[<h2 id="情节性任务-vs-连续任务"><a href="#情节性任务-vs-连续任务" class="headerlink" title="情节性任务 vs. 连续任务"></a>情节性任务 vs. 连续任务</h2><ul><li>情节性任务（Episodic Tasks），所有的任务可以被可以分解成一系列情节，可以看作为有限步骤的任务。</li><li>连续任务（Continuing Tasks），所有的任务不能分解，可以看作为无限步骤任务。</li></ul><h2 id="马尔可夫性"><a href="#马尔可夫性" class="headerlink" title="马尔可夫性"></a>马尔可夫性</h2><p>马尔可夫性：当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态。</p><p>马尔可夫过程即为具有马尔可夫性的过程，即过程的条件概率仅仅与系统的当前状态相关，而与它的过去历史或未来状态都是独立、不相关的。</p><h2 id="马尔可夫奖赏过程"><a href="#马尔可夫奖赏过程" class="headerlink" title="马尔可夫奖赏过程"></a>马尔可夫奖赏过程</h2><p>马尔可夫奖赏过程（Markov Reward Process，MRP）是带有奖赏值的马尔可夫过程，其可以用一个四元组表示 <S,P,R,$\gamma$> </p><ul><li>S 为有限的状态集合；</li><li>P 为状态转移矩阵，$P_{ss^{‘}} = P[S_{t+1} = s^{‘}|S_t = s]$</li><li>R 是奖赏函数；</li><li>$\gamma$为折扣因子（discount factor），其中 $\gamma$∈[0,1]</li></ul><h3 id="奖赏函数"><a href="#奖赏函数" class="headerlink" title="奖赏函数"></a>奖赏函数</h3><p>在 t 时刻的奖赏值 Gt :</p><script type="math/tex; mode=display">G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}</script><h3 id="Why-Discount"><a href="#Why-Discount" class="headerlink" title="Why Discount"></a>Why Discount</h3><p>关于Return的计算为什么需要 $\gamma$ 折扣系数:</p><blockquote><ul><li>数学表达的方便</li><li>避免陷入无限循环</li><li>远期利益具有一定的不确定性</li><li>在金融学上，立即的回报相对于延迟的回报能够获得更多的利益</li><li>符合人类更看重眼前利益的特点</li></ul></blockquote><h3 id="价值函数"><a href="#价值函数" class="headerlink" title="价值函数"></a>价值函数</h3><p>状态 s 的长期价值函数表示为：</p><script type="math/tex; mode=display"> v(s)=E[Gt|St=s]</script><h3 id="Bellman-Equation-for-MRPs"><a href="#Bellman-Equation-for-MRPs" class="headerlink" title="Bellman Equation for MRPs"></a>Bellman Equation for MRPs</h3><script type="math/tex; mode=display">\begin{align}v(s) &= E[G_t|S_t=s]\\&= E[R_{t+1} + \gamma R_{t+2} + ... | S_t = s]\\&= E[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} ... ) | S_t = s]\\&= E[R_{t+1} + \gamma G_{t+1} | S_t = s]\\&= E[R_{t+1} + \gamma v(s_{t+1}) | S_t = s]\end{align}</script><p>下图为MRP的 backup tree 示意图：</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005211526.png" alt="img"></p><p>注：backup tree 中的白色圆圈代表状态，黑色圆点对应动作。</p><p>根据上图可以进一步得到：</p><script type="math/tex; mode=display">v(s) = R_s + \gamma \sum_{s' \in S}P_{ss'}v(s')</script><h2 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h2><p>马尔可夫决策过程（Markov Decision Process，MDP）是带有决策的MRP，其可以由一个五元组构成 <S,A,P,R,$\gamma$> 。</p><ul><li>S 为有限的状态集合；</li><li>A 为有限的动作集合；</li><li>P 为状态转移矩阵，$P_{ss^{‘}}^{a} = P[S_{t+1} = s^{‘}|S_t = s,A_t=a]$</li><li>R 是奖赏函数；</li><li>$\gamma$ 为折扣因子（discount factor），其中 $\gamma$∈[0,1] ]</li></ul><p>我们讨论的MDP一般指有限（离散）马尔可夫决策过程。</p><h2 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h2><p>策略（Policy）是给定状态下的动作概率分布，即：</p><script type="math/tex; mode=display">\pi(a|s) = P[A_t = a|S_t = a]</script><h2 id="状态价值函数-amp-最优状态价值函数"><a href="#状态价值函数-amp-最优状态价值函数" class="headerlink" title="状态价值函数 &amp; 最优状态价值函数"></a>状态价值函数 &amp; 最优状态价值函数</h2><p>给定策略 π 下状态 s 的状态价值函数（State-Value Function） $v_{\pi}(s)$</p><script type="math/tex; mode=display">v_{\pi}(s) = E_{\pi}[G_t|S_t = s]</script><p>状态 s 的最优状态价值函数（The Optimal State-Value Function）v~∗~(s) </p><script type="math/tex; mode=display">v_{*}(s) = \max_{\pi}v_{\pi}(s)</script><h2 id="动作价值函数-amp-最优动作价值函数"><a href="#动作价值函数-amp-最优动作价值函数" class="headerlink" title="动作价值函数 &amp; 最优动作价值函数"></a>动作价值函数 &amp; 最优动作价值函数</h2><p>给定策略 π，状态 s，采取动作 a 的动作价值函数（Action-Value Function）q~π~(s,a)</p><script type="math/tex; mode=display">q_{\pi}(s, a) = E_{\pi}[G_t|S_t = s, A_t = a]</script><p>状态 s 下采取动作 a 的最优动作价值函数（The Optimal Action-Value Function）q∗(s,a):</p><script type="math/tex; mode=display">q_{*}(s, a) = \max_{\pi}q_{\pi}(s, a)</script><h2 id="最优策略"><a href="#最优策略" class="headerlink" title="最优策略"></a>最优策略</h2><p>如果策略 π 优于策略 π′：</p><script type="math/tex; mode=display">\pi \ge \pi^{'} \text{ if } v_{\pi}(s) \ge v_{\pi^{'}}(s), \forall{s}</script><p>最优策略 v∗ 满足：</p><ul><li>v∗≥π,∀π</li><li>v~π∗~(s)=v~∗~(s) </li><li>q~π∗~(s,a)=q~∗~(s,a) </li></ul><p>如何找到最优策略？</p><p>可以通过最大化 q~∗~(s,a)  来找到最优策略：</p><script type="math/tex; mode=display">v_{*}(a|s) =\begin{cases}& 1 \text{ if } a=\arg\max_{a \in A}q_{*}(s,a)\\& 0 \text{ otherwise }\end{cases}</script><p><strong>对于MDP而言总存在一个确定的最优策略，而且一旦我们获得了q~∗~(s,a) ，我们就能立即找到最优策略。</strong></p><h2 id="Bellman-Expectation-Equation-for-MDPs"><a href="#Bellman-Expectation-Equation-for-MDPs" class="headerlink" title="Bellman Expectation Equation for MDPs"></a>Bellman Expectation Equation for MDPs</h2><p>我们先看下状态价值函数 v^π^。</p><p>状态 s 对应的 backup tree 如下图所示：</p><p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181027180345098-1901972119.png" alt=""></p><p>根据上图可得：</p><script type="math/tex; mode=display">v_{\pi}(s) = \sum_{a \in A}\pi(a|s)q_{\pi}(s, a)  \qquad (1)</script><p>再来看动作价值函数 q~π~(s,a) </p><p>状态 s，动作 a 对应的 backup tree 如下图所示：</p><p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181027180402049-1747500206.png" alt=""></p><p>因此可得：</p><script type="math/tex; mode=display">q_{\pi}(s,a)=R_s^a + \gamma \sum_{s'\in S}P_{ss'}^a v_{\pi}(s')  \qquad (2)</script><p>进一步细分 backup tree 再来看 v^π^  与 q~π~(s,a)  对应的表示形式。</p><p>细分状态 ss 对应的 backup tree 如下图所示：</p><p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181027180411412-1063042128.png" alt=""></p><p>将式子(2)代入式子(1)可以进一步得到 vπ(s)vπ(s) 的贝尔曼期望方程：</p><script type="math/tex; mode=display">v_{\pi}(s) = \sum_{a \in A} \pi(a | s) \Bigl( R_s^a + \gamma \sum_{s'\in S}P_{ss'}^a v_{\pi}(s') \Bigr)  \qquad (3)</script><p>细分状态 ss，动作 aa 对应的 backup tree 如下图所示：</p><p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181027180421183-498067530.png" alt=""></p><p>将式子(1)代入式子(2)可以得到 qπ(s,a)的贝尔曼期望方程：</p><script type="math/tex; mode=display">q_{\pi}(s,a)=R_s^a + \gamma \sum_{s'\in S}P_{ss'}^a \Bigl(\sum_{a' \in A}\pi(a'|s')q_{\pi}(s', a') \Bigr)  \qquad (4)</script><h2 id="Bellman-Optimality-Equation-for-MDPs"><a href="#Bellman-Optimality-Equation-for-MDPs" class="headerlink" title="Bellman Optimality Equation for MDPs"></a>Bellman Optimality Equation for MDPs</h2><p>同样我们先看 v~∗~(s)：</p><p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181027180430574-1927151238.png" alt=""></p><p>对应可以写出公式：</p><script type="math/tex; mode=display">v_{*}(s) = \max_{a}q_{*}(s, a)   \qquad (5)</script><p> 再来看q~∗~(s,a)：</p><p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181027180436870-598952431.png" alt=""></p><p>对应公式为：</p><script type="math/tex; mode=display">q_{*}(s, a) =  R_s^a + \gamma \sum_{s'\in S}P_{ss'}^a v_{*}(s') \qquad (6)</script><p>同样的套路获取 v∗(s) 对应的 backup tree 以及贝尔曼最优方程：</p><p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181027180450776-607271585.png" alt=""></p><p>贝尔曼最优方程：</p><script type="math/tex; mode=display">v_{*}(s) = \max_{a} \Bigl( R_s^a + \gamma \sum_{s'\in S}P_{ss'}^a v_{*}(s') \Bigr) \qquad (7)</script><p>q∗(s,a)  对应的 backup tree 以及贝尔曼最优方程：</p><p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181027180500521-1105213902.png" alt=""></p><p>对应的贝尔曼最优方程：</p><script type="math/tex; mode=display">R_s^a + \gamma \sum_{s'\in S}P_{ss'}^a\max_{a}q_{*}(s, a) \qquad (8)</script><h3 id="贝尔曼最优方程特点"><a href="#贝尔曼最优方程特点" class="headerlink" title="贝尔曼最优方程特点"></a>贝尔曼最优方程特点</h3><ul><li>非线性（non-linear）</li><li>通常情况下没有解析解（no closed form solution）</li></ul><h3 id="贝尔曼最优方程解法"><a href="#贝尔曼最优方程解法" class="headerlink" title="贝尔曼最优方程解法"></a>贝尔曼最优方程解法</h3><ul><li>Value Iteration</li><li>Policy Iteration</li><li>Sarsa</li><li>Q-Learning</li></ul><h2 id="MDPs的相关扩展问题"><a href="#MDPs的相关扩展问题" class="headerlink" title="MDPs的相关扩展问题"></a><del>MDPs的相关扩展问题</del></h2><ul><li><del>无限MDPs/连续MDPs</del></li><li><del>部分可观测的MDPs</del></li><li><del>Reward无折扣因子形式的MDPs/平均Reward形式的MDPs</del></li></ul>]]></content>
    
    
    <summary type="html">马尔可夫决策过程</summary>
    
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/categories/MARL/"/>
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/tags/MARL/"/>
    
  </entry>
  
  <entry>
    <title>GAMES101</title>
    <link href="https://changqingaas.github.io/CG/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/GAMES101/"/>
    <id>https://changqingaas.github.io/CG/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/GAMES101/</id>
    <published>2021-09-13T11:50:19.000Z</published>
    <updated>2021-10-05T11:35:16.735Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Lecture1-intro"><a href="#Lecture1-intro" class="headerlink" title="Lecture1:  intro"></a>Lecture1:  intro</h2><h3 id="计算机图形学"><a href="#计算机图形学" class="headerlink" title="计算机图形学"></a>计算机图形学</h3><p>使用计算机synthesize(合成)  manipulate（操作) 可视化信息</p><h3 id="why-study-computer-graphics"><a href="#why-study-computer-graphics" class="headerlink" title="why study computer graphics?"></a>why study computer graphics?</h3><ul><li>Application<ul><li>video games 电子游戏</li><li>animations 动画</li><li>visualization 可视化</li><li>virtual reality</li><li>augmented reality 增强现实</li><li>digital illustration 数码插画</li><li>simulation 模拟</li><li>graphical user interfaces 图形用户界面</li><li>typography  排版</li></ul></li><li>technical chanllenges </li></ul><h3 id="Course-topics"><a href="#Course-topics" class="headerlink" title="Course topics"></a>Course topics</h3><ul><li><p>Rasterization  光栅化</p><ul><li><p>project geometry primitives (3D triangles / polygons) onto the screen</p><p>将几何图形（3D三角形 / 多边形）投射到屏幕上</p></li><li><p>break projected primitives into fragments (pixels)</p><p>将投影图元分解到片段(像素)</p></li><li><p>gold standard in video games (real-time applications)</p></li></ul></li><li><p>curves and meshes 曲线和栅格</p><ul><li>怎样represent geometry in CG</li></ul></li><li><p>ray tracing 光线追踪</p><ul><li>shoot rays from camera though each pixel<ul><li>calculate intersection and shading 交叉点和阴影</li><li>continue to bounce the rays till they hit light sources</li></ul></li><li>gold standard in animations / movies (offline离线 application)</li></ul></li><li><p>animation simulation</p><ul><li>key frame animation 关键帧动画</li><li>mass-spring system 弹簧振子系统</li></ul></li></ul><h3 id=""><a href="#" class="headerlink" title=" "></a> </h3><h3 id="differences-between-cg-and-cv"><a href="#differences-between-cg-and-cv" class="headerlink" title="differences between cg and cv"></a>differences between cg and cv</h3><p>No clear boundaries</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210913231736.png" alt="image-20210913210044004"></p><h2 id="Lecture2-review-of-linear-algebra"><a href="#Lecture2-review-of-linear-algebra" class="headerlink" title="Lecture2:  review of linear algebra"></a>Lecture2:  review of linear algebra</h2><h3 id="Graphics’-dependcies"><a href="#Graphics’-dependcies" class="headerlink" title="Graphics’ dependcies"></a>Graphics’ dependcies</h3><ul><li>basic mathematics<ul><li>Linear algebra 线性代数<ul><li>mostly dependent on linear algebra</li><li>vectors（dot products点乘,cross products叉乘<ul><li>An operation like translating or rotating objects  can be matrix-vector multiplication</li></ul></li><li>matrices 矩阵（复数</li></ul></li><li>calculus 微积分</li><li>statistics 统计</li></ul></li><li>basic physics <ul><li>Optics, 光学的</li><li>Mechanics  机械的</li></ul></li><li>misc 杂项<ul><li>Numerical analysis  数值分析</li><li>signal processing 信号处理</li><li>aesthetics 审美</li></ul></li></ul><h3 id="vectors"><a href="#vectors" class="headerlink" title="vectors"></a>vectors</h3><p>noting: 只记了part</p><ul><li><p>unit vector </p><ul><li>单位向量，</li><li>用来代表方向</li></ul></li><li><p>dot  product in graphics</p><ul><li><p>$\vec{a}\cdot\vec{b} = |\vec{a}|\cdot|\vec{b}|cos\theta$ </p></li><li><p>Find angle between two vectors  (e.g. cosine of angle between light source 光源 and surface表面)</p></li><li><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210913231823.png" alt="image-20210913215253056"></p></li><li><p>Finding <strong>projection</strong> of one vector on another</p><ul><li><p>measure how close two directions are</p></li><li><p>decompose分解 a vector </p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210913231937.png" alt="image-20210913215715680" style="zoom:50%;" /></p></li><li><p>determine forward or backward</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210913231938.png" alt="image-20210913215758162" style="zoom: 50%;" /></p></li></ul></li></ul></li><li><p>cross product in graphics</p><p>​    <img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210913231939.png" alt="image-20210913220439863" style="zoom: 50%;" /></p><ul><li>Direction determined by right-hand rule</li></ul></li><li><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210913231940.png" alt="image-20210913220739158"></p><ul><li>Useful in constructing coordinate systems (later)</li><li>Determine left / right</li><li>Determine inside / outside</li></ul></li><li>Orthonormal bases and coordinate frames  正交基底和坐标系<ul><li>Critical issue is transforming between these systems/ bases</li></ul></li></ul><h3 id="matrices"><a href="#matrices" class="headerlink" title="matrices"></a>matrices</h3><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210913231941.png" alt="image-20210913223344462" style="zoom:50%;" /></p><ul><li>$(AB)^{T} = B^{T}A^{T}$</li><li>$AA^{-1} = A^{-1}A = I$</li><li>$(AB)^{-1} = B^{-1}A^{-1}$</li></ul><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210913231942.png" alt="image-20210913223854494" style="zoom: 50%;" /></p><p>In Graphics, pervasively used to represent transformations</p><ul><li>translation, rotation,shear剪切,scale缩放</li></ul><h2 id="Lecture-3-Transformation"><a href="#Lecture-3-Transformation" class="headerlink" title="Lecture 3: Transformation"></a>Lecture 3: Transformation</h2><h3 id="why-study-transformation"><a href="#why-study-transformation" class="headerlink" title="why study transformation"></a>why study transformation</h3><ul><li>modeling<ul><li>translation</li><li>rotation</li><li>scaling</li></ul></li><li>viewing<ul><li>3D (projection)</li><li>2D (projection)</li></ul></li></ul><h3 id="2D-transformations"><a href="#2D-transformations" class="headerlink" title="2D transformations:"></a>2D transformations:</h3><ul><li><p>representing transformations using matrices</p></li><li><p>rotation</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914135044.png" alt="image-20210914135044036" style="zoom:50%;" /></p><ul><li>$R_{\theta} = \begin{bmatrix} cos\theta &amp; -sin\theta \\ sin\theta &amp; cos\theta \end{bmatrix} $</li><li>$R_{-\theta} = \begin{bmatrix} cos\theta &amp; sin\theta \\ -sin\theta &amp; cos\theta \end{bmatrix} = R_{\theta}^{T} = R_{\theta}^{-1}(by \quad definition) $</li><li>正交矩阵： A·A^T^ = E</li><li>默认绕原点旋转</li><li>默认逆时针旋转</li></ul></li><li><p>scale matrix</p><p>$\begin{bmatrix} x^{‘} \\ y^{‘} \end{bmatrix} = \begin{bmatrix} s_{x} &amp; 0 \\ 0 &amp; s_{y} \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}$</p></li><li><p>reflection matrix 反射（镜像）矩阵</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914124142.png" alt="image-20210914124142211" style="zoom: 50%;" /></p><p>$\begin{bmatrix} x^{‘} \\ y^{‘} \end{bmatrix} = \begin{bmatrix} -1 &amp; 0 \\ 0 &amp; 1\end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}$</p></li><li><p>shear matrix</p><p>​    <img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914134639.png" alt="image-20210914134632720" style="zoom: 50%;" /></p><p>$\begin{bmatrix} x^{‘} \\ y^{‘} \end{bmatrix} = \begin{bmatrix} -1 &amp; a \\ 0 &amp; 1\end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}$</p><ul><li>Hints:<ul><li>horizontal shift is 0 at y = 0</li><li>horizontal shift is a at y = 1</li><li>vertical shift is always</li></ul></li></ul></li><li><p>Linear transforms </p><ul><li>线性变换：可以用一个矩阵表示的变换</li><li>x’ = ax + by</li><li>y’ = cx + dy</li><li>$\begin{bmatrix} x^{‘} \\ y^{‘} \end{bmatrix} = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}$</li></ul></li></ul><h3 id="Homogeneous-coordinates-齐次坐标"><a href="#Homogeneous-coordinates-齐次坐标" class="headerlink" title="Homogeneous coordinates 齐次坐标"></a>Homogeneous coordinates 齐次坐标</h3><ul><li><p>Why homogeneous coordinates  </p><p>for example:  translation</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914135553.png" alt="image-20210914135548707" style="zoom:50%;" /></p><p>$\begin{bmatrix} x^{‘} \\ y^{‘} \end{bmatrix} = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix} + \begin{bmatrix} t_{x} \\ t_{y} \end{bmatrix} $</p><ul><li><p>so, translation is not linear transform!</p></li><li><p>因为平移变换不能直接用一个矩阵表示，必须加一个向量；  </p></li><li><p>add a third coordinates</p><p>引入齐次坐标可以解决问题，让平移也能只用一个矩阵表示</p></li><li><p>2D point = (x,y,1)^T^</p></li><li><p>2D vector = (x,y,0)^T^</p></li><li><p>向量 + 向量，结果齐次项是0，还是向量</p></li><li><p>点 - 点，得到的是一个向量，齐次项也变成0 </p></li><li><p>点 + 向量，表示一个点的移动，结果 还是点 ！</p></li><li><p>点 + 点是什么呢？齐次项变成2。将所有项除以2，齐次项又变为1 。所以点 + 点结果实际上是两个点的中点。</p></li></ul></li><li><p>Affine  transformation 仿射变换</p><ul><li><p>仿射变换：先线性变换再加上一次平移</p></li><li><p>$\begin{bmatrix} x^{‘} \\ y^{‘} \\ 1 \end{bmatrix} = \begin{bmatrix} a &amp; b &amp; t_{x}\\ c &amp; d  &amp; t_{y}  \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \begin{bmatrix} x \\  y \\ 1 \end{bmatrix} $</p></li><li><p>scale </p><p>$ S(s_{x}, s_{y}) = \begin{bmatrix} s_{x} &amp; 0 &amp; 0 \\ 0 &amp; s_{y}  &amp; 0  \\ 0 &amp; 0 &amp; 1 \end{bmatrix}$</p></li><li><p>rotation </p><p>$ R(\alpha) = \begin{bmatrix} cos\alpha &amp; -sin\alpha &amp; 0 \\ sin\alpha &amp; cos\alpha  &amp; 0  \\ 0 &amp; 0 &amp; 1 \end{bmatrix}$</p></li><li><p>translation</p><p>$ T(t_{x}, t_{y}) = \begin{bmatrix} 1 &amp; 0 &amp; t_{x} \\ 0 &amp; 1  &amp; t_{y} \\ 0 &amp; 0 &amp; 1 \end{bmatrix}$</p></li></ul></li><li><p>transform ordering matters</p><ul><li>matrix multiplication is not commutative 可交换的</li></ul></li></ul><h3 id="composing-transforms"><a href="#composing-transforms" class="headerlink" title="composing transforms"></a>composing transforms</h3><ul><li><p>decomposing complex transforms</p><ul><li>translate center to origin</li><li>rotate</li><li>translate back</li></ul><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914142248.png" alt="image-20210914142248083"></p><p>which means $T(c) · R(\alpha) · T(-c)$</p><ul><li><p>分解：变换可以分解，注意先后顺序是从右到左</p></li><li><p>2D变换矩阵（缩放，旋转，平移变换）</p></li></ul></li><li></li></ul><h2 id="Lecture-4：-Transformation-Cont"><a href="#Lecture-4：-Transformation-Cont" class="headerlink" title="Lecture 4： Transformation Cont"></a>Lecture 4： Transformation Cont</h2><h3 id="3D-transformations"><a href="#3D-transformations" class="headerlink" title="3D transformations"></a>3D transformations</h3><ul><li><p>3D point = (x,y,z,1)^T^</p></li><li><p>3D vector = (x,y,z,0)^T^</p></li><li><p>$\begin{bmatrix} x^{‘} \\ y^{‘} \\ z_{‘} \\ 1 \end{bmatrix} = \begin{bmatrix} a &amp; b &amp; c &amp; t_{x}\\ d &amp; e &amp; f &amp; t_{y}\\g &amp; h &amp; i &amp; t_{z}  \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix} \begin{bmatrix} x \\ y \\z \\ 1 \end{bmatrix} $</p></li><li><p>三维空间中的齐次变换，最后一行和二维变换类似，是0 0 0 1，平移还是在矩阵最后一列</p></li><li><p>对于仿射变换，是先应用线性变换，再加上平移</p></li><li><p>what is order?</p><ul><li><p>linear transform first or translation first?</p></li><li><p>scale </p><p> $S(s_{x}, s_{y},s_{z}) = \begin{bmatrix} s_{x} &amp; 0 &amp; 0 &amp; 0\\ 0 &amp;  s_{y} &amp; 0 &amp; 0\\0 &amp; 0 &amp;  s_{z} &amp; 0  \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}  $</p></li><li><p>translation</p><p> $T(t_{x}, t_{y}, t_{z}) = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; t_{x}\\ 0 &amp;  1 &amp; 0 &amp; t_{y} \\0 &amp; 0 &amp;  1 &amp; t_{z}  \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}  $</p></li><li><p>rolation</p><ul><li>rolation around x-, y-,or  z-axis</li></ul><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914203113.png" alt="image-20210914203043532" style="zoom:50%;" /></p><ul><li>绕哪个轴旋转，哪个坐标就不变</li><li><p>不过𝑅𝑦矩阵稍微不同，其他两个都是右上角𝑠𝑖𝑛是负的，只有他是左下角𝑠𝑖𝑛是负的 因为𝑥叉乘𝑦得到𝑧，𝑧叉乘𝑦得到𝑥，但是𝑧叉乘𝑥得到𝑦（而不是𝑥叉乘𝑧），所以是反的</p></li><li><p>$R_{xyz}(\alpha, \beta, \gamma) = R_{x}(\alpha)R_{y}(\beta)R_{z}(\gamma)$</p></li></ul></li><li><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914220209.png" alt="image-20210914220208915"></p></li></ul></li></ul><h3 id="Viewing-观测-transformation"><a href="#Viewing-观测-transformation" class="headerlink" title="Viewing (观测) transformation"></a>Viewing (观测) transformation</h3><ul><li><p>View (视图) / Camera transformation  </p><ul><li>Think about how to take a photo <ul><li>Find a good place and arrange people (model transformation) </li><li>Find a good “angle” to put the camera (view transformation)</li><li>Cheese! (projection transformation)<ul><li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914224154.png" alt="image-20210914224154439"></li><li>定义相机</li><li>位置</li><li>往哪看</li><li>向上方向</li><li>现实中是移动相机，变换景物</li><li>图形学中，相机不动，永远在原点</li><li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914223830.png" alt="image-20210914223829896" style="zoom:50%;" /></li><li>经过变换，把相机的位置移动到原点，同时保持看到的景物不变</li></ul></li></ul></li></ul></li><li><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914225726.png" alt="image-20210914225726865"></p></li><li><p>这个从“歪”的坐标轴旋转回正的坐标轴，不太好写。 但是这个变换的逆过程，即：从正的坐标轴旋转到“歪”的坐标轴，是好写的， 于是我们先写从“正”坐标轴变换到“歪”坐标轴的变换矩阵，再求其逆矩阵，就可以得到待求的变换矩阵。 又因为旋转矩阵是正交矩阵，所以他的逆矩阵就只需要转置一下就可以得到了！ 注意，不但相机要做这个变换，其他物体也要做这个变换，因为我们想让相机看到的景物相对不变。 （以上部分个人认为非常巧妙和关键！）</p></li><li><p>Projection (投影) transformation</p><ul><li><p>3D to 2D</p></li><li><p>Orthographic (正交) projection  </p><p> <img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914230822.png" alt="image-20210914230821994"></p><ul><li><em>没有近大远小</em></li><li>平行投影<ul><li>首先定义空间中一个立方体，将其translate，使其中心在原点，再scale成标准立方体（边长为2</li></ul></li><li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915120758.png" alt="image-20210915120758604"></li><li>再次提醒，注意𝑧轴是近大远小 OpenGL等API是反过来的</li></ul></li><li><p>Perspective (透视) projection</p><p> <img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914230840.png" alt="image-20210914230840011"></p><ul><li><p>更像人眼看到的场景</p></li><li><p>Most common in Computer Graphics, art, visual system </p></li><li><p>Further objects are smaller </p></li><li><p>Parallel lines not parallel; converge to single point</p></li><li>Recall: property of homogeneous coordinates<ul><li>(x, y, z, 1), (kx, ky, kz, k != 0), (xz, yz, z2, z != 0) all represent  the same point (x, y, z) in 3D </li><li>e.g. (1, 0, 0, 1) and (2, 0, 0, 2) both represent (1, 0, 0)</li></ul></li><li>how to do perspective projection<ul><li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915122816.png" alt="image-20210915122816233"></li><li>先将frustum远平面，挤压成和近平面一样大（从左图变成右图）</li><li>再做正交投影，投影到近平面</li><li>上述操作过程中几点假设：<ul><li>1）近平面保持不变 </li><li>2）z值保持不变，只是向内收缩</li></ul></li></ul></li><li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915122907.png" alt="image-20210915122907760"></li><li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915122956.png" alt="image-20210915122956833"></li><li>挤压这一步怎么做？ 上图是从侧面观察frustum 用相似三角形知识可以得到新坐标的表达式，但是第三个分量目前还不知道（这里利用之前讲的那个性质： 齐次坐标，如果我们对点的坐标所有分量同时乘以k，他表示的还是原来那个点！  </li><li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915123123.png" alt="image-20210915123123207"></li><li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915123140.png" alt="image-20210915123140120"></li><li>如何求解第三行<ul><li>任何近平面上的点不会改变（也就是对于任意的(𝑥, 𝑦, 𝑛, 1)，经过这个矩阵变换后，点的位置仍然不变）</li><li>任何远平面上的点，𝑧值不会改变</li><li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915123415.png" alt="image-20210915123415483"></li><li>点(𝑥, 𝑦, 𝑧, 1)是可以通过矩阵变换得到(𝑛𝑥, 𝑛𝑦, 𝑢𝑛𝑘𝑛𝑤𝑜𝑛, 𝑧)向量的。 根据上文提到的性质（1），经过这个变换，点实际没有改变 而同时，(𝑥, 𝑦, 𝑧, 1)本身可以写成(𝑥, 𝑦, 𝑛, 1)（为什么把𝑧替换成𝑛？因为近平面的𝑧坐标就是都是𝑛，所以可以做这个替换。）然后同时乘以𝑛， 变成(𝑛𝑥, 𝑛𝑦, 𝑛 ଶ , 𝑛) 经过上面两个推导，可以看出，第三行前两个数一定是0 因为𝑛 ଶ这个分量和𝑥和𝑦都毫无关系，因此前两个数必定是0 这样，我们就解出了第三行前两个数，都是0 接下来求A和B</li><li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915123500.png" alt="image-20210915123459854"></li><li>远平面上有一个特殊点，(0, 0, 𝑓)经过变换挤压仍然不变 所以(0, 0, 𝑓)经过变换仍然是(0, 0, 𝑓) 根据近平面我们得到$An + B = n^{2}$,根据远平面的中心点我们得到$Af + B = f^{2}</li><li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915123625.png" alt="image-20210915123625508"></li><li>这样我们就能解出A和B了， 这样终于把从透视投影挤压成正交投影的矩阵，解出来了</li><li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915123651.png" alt="image-20210915123651566"></li></ul></li></ul></li><li><p>思考题</p><ul><li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915123710.png" alt="image-20210915123709902"></li></ul></li></ul></li></ul><h2 id="Lecture05-Rasterization-1-Triangles"><a href="#Lecture05-Rasterization-1-Triangles" class="headerlink" title="Lecture05: Rasterization 1(Triangles)"></a>Lecture05: Rasterization 1(Triangles)</h2><h3 id="Finishing-up-Viewing"><a href="#Finishing-up-Viewing" class="headerlink" title="Finishing up Viewing"></a>Finishing up Viewing</h3><ul><li>Viewport(视口) transformation</li></ul><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005190732.png" alt="image-20211005190731847" style="zoom: 50%;" /></p><p>上节课把透视投影转化成正交投影 这里引入另外一个概念 Field of View，表示你能看到的角度的范围 注意看上图中红色线的夹角，就是垂直可视角度，他越大，可视角度越大 同理还有水平可视角度</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005190927.png" alt="image-20211005190926969"></p><p>MVP这三个变换之后，所有东西都会停留在一个1，1，1的位于原点的标准立方体中 下一步就要把这立方体画在屏幕上</p><h3 id="Rasterization（光栅化，即把东西花在屏幕上"><a href="#Rasterization（光栅化，即把东西花在屏幕上" class="headerlink" title="Rasterization（光栅化，即把东西花在屏幕上"></a>Rasterization（光栅化，即把东西花在屏幕上</h3><h4 id="屏幕"><a href="#屏幕" class="headerlink" title="屏幕"></a>屏幕</h4><ul><li><p>像素是最小的屏幕单位</p></li><li><p>每个像素有不同的颜色</p></li><li><p>屏幕空间：就是给屏幕定义一个坐标系 比如，可以定义左下角是原点。 </p></li><li><p>实际上像素的中心是(𝑥 + 0.5, 𝑦 + 0.5)</p></li><li><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005191523.png" alt="image-20211005191523440"></p></li><li><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005191557.png" alt="image-20211005191556944"></p><p>我们要做的就是把标准立方体空间映射到屏幕这个二维世界中去 𝑧暂时不管 其他两个坐标是[−1, 1] ଶ转换到 [0, 𝑤𝑖𝑑𝑡ℎ] ∗ [0, ℎ𝑒𝑖𝑔ℎ𝑡] 使用上面这个矩阵做变换</p></li></ul><h4 id="Rasterizing-a-triangle"><a href="#Rasterizing-a-triangle" class="headerlink" title="Rasterizing  a triangle"></a>Rasterizing  a triangle</h4><ul><li><p>三角形可以拼接在三维空间中的面，或者二维空间中复杂的图形 </p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005191718.png" alt="image-20211005191718242"></p></li><li><p>三角形内部一定是平面的 </p></li><li>给三角形顶点定义不同属性，可以在三角形内部进行插值</li><li>通过采样的方式，来画出三角形<ul><li>采样就是把函数离散化的过程</li><li>可以对时间，面积，方向，体积… 进行采样</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005191941.png" alt="image-20211005191941468"></p><p>定义二值函数：</p><script type="math/tex; mode=display">inside(tri, x,y) = \left\{\begin{array}{rcl}1  &      & {Point(x,y) \ in  \ triangle \ t}\\0  &      & {otherwise}\\\end{array}\right.</script><ul><li><p>这里我们要做的就是给定一个三角形，判断像素中心是否在三角形内部。</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005192820.png" alt="image-20211005192820679"></p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005192920.png" alt="image-20211005192920137"></p><p>那么，如何判断一个点是否在三角形内？用叉乘！！ 比如对上图，判断Q是否在三角形内部 首先$𝑃1𝑃2 \  X \ 𝑃1𝑄 $，将会得到一个z为正数的向量，也就是结果向量朝向屏幕外的，利 用右手定则，可以得知𝑄在𝑃1𝑃2的左侧（因为如果在右侧，那么结果将会是向量𝑧为负 数，那么结果向量就朝向屏幕内部） 类似的𝑃2𝑃0 𝑋 𝑃2𝑄，得到𝑄在右侧，不对劲！ 𝑃0𝑃1 𝑋 𝑃0𝑄，得到𝑄在左侧</p><p>注意，向量按照一定的顺序去判断，比如我们上面是按照P1, P2, P0去判断的</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005193137.png" alt="image-20211005193137111"></p><p>检查屏幕所有的像素太花时间！ 可以只检查蓝色的包围盒（Bounding box）部分</p><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005193213.png" alt="image-20211005193213667"></p><p>也可以每一行设置一个包围盒，进一步减小包围盒 很适用于那种三角形很小，但是包围盒很大的（窄长三角形</p><h2 id="some-words"><a href="#some-words" class="headerlink" title="some words"></a>some words</h2><p>syllabus  教学大纲</p><p>cube  立方体</p><p>canonical 标准的</p><p>aspect ratio 横纵比</p><p>requirements 要求</p><p>submission 提交</p><p>slides 幻灯片</p><p>Bulletin Board System BBS</p><p>Semantic Segmentation 语义切割</p><p>code skeletons 代码框架</p><p>IDE: Integrated Development Environment</p><p>parse 解析</p><p>Academic integrity 学术诚信</p><p>Valentine’s Day 情人节</p><p>brutal 粗暴的</p><p>coordinates 坐标</p><p> Parallelogram law 平行四边形法则</p><p>Triangle law 三角法则</p><p>orthogonal 正交的</p><p> scalar 标量</p><p>decompose 分解</p><p>haunt 出没</p><p>pervasively 普遍地</p><p>multiplication 乘法</p><p>trivial 琐碎的</p>]]></content>
    
    
    <summary type="html">GAMES101:现代计算机图形学入门</summary>
    
    
    
    <category term="CG" scheme="https://changqingaas.github.io/categories/CG/"/>
    
    
    <category term="CG" scheme="https://changqingaas.github.io/tags/CG/"/>
    
  </entry>
  
  <entry>
    <title>墨子平台训练教程</title>
    <link href="https://changqingaas.github.io/MARL/MADRL/%E5%A2%A8%E5%AD%90%E5%B9%B3%E5%8F%B0%E8%AE%AD%E7%BB%83%E6%95%99%E7%A8%8B/"/>
    <id>https://changqingaas.github.io/MARL/MADRL/%E5%A2%A8%E5%AD%90%E5%B9%B3%E5%8F%B0%E8%AE%AD%E7%BB%83%E6%95%99%E7%A8%8B/</id>
    <published>2021-09-03T15:00:33.000Z</published>
    <updated>2021-09-04T11:34:06.947Z</updated>
    
    <content type="html"><![CDATA[<p>下面介绍一下windows版本墨子平台的使用流程：</p><ol><li><h2 id="进入华戍防务-官网-hs-defense-com-下载软件"><a href="#进入华戍防务-官网-hs-defense-com-下载软件" class="headerlink" title="进入华戍防务 官网(hs-defense.com)下载软件"></a>进入<a href="http://www.hs-defense.com/col.jsp?id=105">华戍防务 官网(hs-defense.com)</a>下载软件</h2></li></ol><p>我下载的是<code>墨子·联合作战推演系统（个人版）</code> </p><p><code>墨子·AI开发包</code>主要是一些开发会用到的代码和文档。代码也可以在gitee.com上找到：<a href="https://gitee.com/hs-defense/moziai">moziai: 墨子AI开发包及“子牙”智能体开源代码  </a></p><p><code>竞赛客户端(互联网)</code> 主要是用来军事推演比赛的，与强化学习训练关系不大，可以不下载。</p><p><code>墨子·AI版（Linux)</code> 因为一开始没办法下载，后面断断续续也没有弄出什么效果</p><p>注：</p><ul><li><p>文件采用ftp协议下载，下载链接格式为：ftp: ip/port/文件夹s/文件</p></li><li><p>由于部分浏览器不支持，推荐使用QQ浏览器下载</p></li><li><p>一般点击下载按钮后，会默认让迅雷接管下载</p></li></ul><ol><li><h2 id="安装软件"><a href="#安装软件" class="headerlink" title="安装软件"></a>安装软件</h2><p>​    根据安装手册安装该软件，基本上按照手册进行就可以了</p></li><li><h2 id="启动该软件"><a href="#启动该软件" class="headerlink" title="启动该软件"></a>启动该软件</h2><ol><li>进入安装目录下的\MoziData，右键点击<code>mysql.bat</code>，以管理员身份运行它，随后退出</li><li>进入安装目录下的\MoziServer，运行MoziServer.exe，尽量关掉杀毒软件，最好以管理员身份运行<ul><li>这里大概会遇到一个问题：<code>临时许可码过期，请联系华戍防务重新授权</code>，具体解决方案略</li><li>我在数据库方面也出过问题，不过忘了具体细节了</li></ul></li></ol></li><li><h2 id="运行代码，进行仿真训练"><a href="#运行代码，进行仿真训练" class="headerlink" title="运行代码，进行仿真训练"></a>运行代码，进行仿真训练</h2><ol><li><p>获取代码，在IDE打开代码</p></li><li><p>加载代码所需的scen想定文件</p><ul><li>进入墨子平台，可以在想定一栏下找到<code>加载想定</code>的选项，想定文件是从<code>安装路径\MoziServer\bin\Scenarios\</code>获取的，因此需要把代码包里的scen文件放到这里。</li><li>加载想定文件，选择推演方</li><li>随后可以看到墨子平台上有了具体的想定环境</li><li>如果要更改想定文件，改完之后，需要保存，再重新加载<ul><li>因为训练的每个回合都会刷新环境，如果不保存更改到新scen文件，那么这个更改只能用于一个回合</li></ul></li></ul></li><li><p>运行main.py代码即可看到效果</p><ul><li><p>注：</p><ul><li><p>这里可能需要指定一下 墨子平台的路径，可以执行代码</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">os.environ[&#39;MOZIPATH&#39;] &#x3D; &#39;D:\\MoZiSystem\\Mozi\\MoziServer\\bin&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>也可以在<code>编辑系统环境变量</code>中添加路径</p></li></ul></li></ul></li></ol></li></ol><p>注：Linux版本的墨子平台还在折腾，因为虚拟机比较卡，服务器上又没办法弄出显示界面，还折腾了一堆配置，最近还没弄，如果有结果了，会在这里更新的</p>]]></content>
    
    
    <summary type="html">墨子推演平台强化学习训练教程，主要是写给队友用的</summary>
    
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/categories/MARL/"/>
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/tags/MARL/"/>
    
    <category term="机器学习" scheme="https://changqingaas.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>一些比较好的文章</title>
    <link href="https://changqingaas.github.io/MARL/MADRL/%E4%B8%80%E4%BA%9B%E6%AF%94%E8%BE%83%E5%A5%BD%E7%9A%84%E6%96%87%E7%AB%A0/"/>
    <id>https://changqingaas.github.io/MARL/MADRL/%E4%B8%80%E4%BA%9B%E6%AF%94%E8%BE%83%E5%A5%BD%E7%9A%84%E6%96%87%E7%AB%A0/</id>
    <published>2021-09-01T14:00:33.000Z</published>
    <updated>2021-09-03T10:43:00.139Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://blog.justlovesmile.top/posts/bfa4054.html">深度学习 | 《深度学习入门之PyTorch》阅读笔记 | Justlovesmile’s BLOG</a></p><p>注： 这本书的github代码已经4年没更新了，只适合速览一下，做个overview, 并不适合手敲学习</p><p><a href="https://blog.justlovesmile.top/posts/43678.html">深度学习 | “花书”，Deep Learning笔记 | Justlovesmile’s BLOG</a></p><p> 注：偏数学</p>]]></content>
    
    
    <summary type="html">放在本地的收藏夹不方便分享，所以就放在这里了</summary>
    
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/categories/MARL/"/>
    
    
    <category term="MARL" scheme="https://changqingaas.github.io/tags/MARL/"/>
    
    <category term="机器学习，碎碎念" scheme="https://changqingaas.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8C%E7%A2%8E%E7%A2%8E%E5%BF%B5/"/>
    
  </entry>
  
  <entry>
    <title>数据库基础__笔记</title>
    <link href="https://changqingaas.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%94%B6%E8%97%8F%E5%A4%B9/"/>
    <id>https://changqingaas.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%94%B6%E8%97%8F%E5%A4%B9/</id>
    <published>2021-09-01T14:00:33.000Z</published>
    <updated>2021-09-03T05:50:36.607Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://blog.justlovesmile.top/posts/41347.html">大学课程 | 数据库基础 | Justlovesmile’s BLOG</a></p>]]></content>
    
    
    <summary type="html">放在本地的收藏夹不方便分享，所以就放在这里了</summary>
    
    
    
    <category term="数据库" scheme="https://changqingaas.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
    <category term="数据库" scheme="https://changqingaas.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>算法分析与设计__笔记</title>
    <link href="https://changqingaas.github.io/%E7%AE%97%E6%B3%95/%E7%AE%97%E6%B3%95/%E6%94%B6%E8%97%8F%E5%A4%B9/"/>
    <id>https://changqingaas.github.io/%E7%AE%97%E6%B3%95/%E7%AE%97%E6%B3%95/%E6%94%B6%E8%97%8F%E5%A4%B9/</id>
    <published>2021-09-01T14:00:33.000Z</published>
    <updated>2021-09-03T04:38:34.845Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://blog.justlovesmile.top/posts/16050.html">大学课程 | 《算法分析与设计》笔记 | Justlovesmile’s BLOG</a></p>]]></content>
    
    
    <summary type="html">放在本地的收藏夹不方便分享，所以就放在这里了</summary>
    
    
    
    <category term="算法" scheme="https://changqingaas.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="算法" scheme="https://changqingaas.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>什么是node.js</title>
    <link href="https://changqingaas.github.io/js/js/%E4%BB%80%E4%B9%88%E6%98%AFnode.js/"/>
    <id>https://changqingaas.github.io/js/js/%E4%BB%80%E4%B9%88%E6%98%AFnode.js/</id>
    <published>2021-08-31T08:56:30.000Z</published>
    <updated>2021-08-31T01:09:23.068Z</updated>
    
    <content type="html"><![CDATA[<h1 id="node-js简介"><a href="#node-js简介" class="headerlink" title="node.js简介"></a>node.js简介</h1><ul><li><p>node.js是运行在服务端的JavaScript,是一个事件驱动I\O服务端JavaScript环境</p></li><li><p>查看版本</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">node -v<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>node版本管理工具nvm</p></li></ul><h1 id="Node-js应用"><a href="#Node-js应用" class="headerlink" title="Node.js应用"></a>Node.js应用</h1><ul><li>Node.js应用的构成：<ul><li>引入required模块</li><li>创建服务器</li><li>接收请求与相应请求</li></ul></li></ul><h2 id="创建Node-js应用"><a href="#创建Node-js应用" class="headerlink" title="创建Node.js应用"></a>创建Node.js应用</h2><ul><li><p>步骤一：引入required模块</p><ul><li>使用require指令来载入http模块，并将实例化的HTTP复制给变量http，实例如下：  <pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token keyword">var</span> http <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"http"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li><li><p>步骤二： 创建服务器</p><ul><li>创建一个server.js的文件：<pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token keyword">var</span> http <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"http"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>http<span class="token punctuation">,</span><span class="token function">createServer</span><span class="token punctuation">(</span><span class="token keyword">function</span><span class="token punctuation">(</span><span class="token parameter">request<span class="token punctuation">,</span>response</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>    <span class="token comment">// 发送http头部</span>    <span class="token comment">//HTTP状态值：200：OK</span>    <span class="token comment">//内容类型：text/plain</span>    response<span class="token punctuation">.</span><span class="token function">writeHead</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">,</span><span class="token punctuation">&#123;</span><span class="token string">'Content-Type'</span><span class="token operator">:</span>'text<span class="token operator">/</span>plain<span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment">//发送响应数据 “Hello World"</span>    response<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token string">'Hello World\n'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token comment">//终端打印如下信息</span>console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span><span class="token string">'Server running at http://127.0.0.1:8888/'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>使用node命令执行以上代码</p>  <pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">node server.js<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>访问<a href="http://127.0.0.1:8888">http://127.0.0.1:8888</a></p></li></ul></li></ul><h2 id="npm使用介绍"><a href="#npm使用介绍" class="headerlink" title="npm使用介绍"></a>npm使用介绍</h2><ul><li><p>查看npm版本</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">npm -v <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>升级版本</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">npm install npm -g<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>安装包</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">npm install &lt;Module Name&gt; # 本地安装npm install &lt;Module Name&gt; -g # 全局安装<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p>查看安装信息</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">npm list -g # 查看全局安装的模块npm ist &lt;Module Name&gt;# 查看某个模块的版本号npm ls# 查看当前目录下的包信息<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>卸载模块</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">npm un &lt;Module Name&gt; <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>更新模块</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">npm update &lt;Module Name&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>搜索模块</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">npm search &lt;Module Name&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>创建模块</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">$npm init# 会自动生成package.json$npm adduser # 在npm资源库中注册用户Username:XXXXPassword:XXXXEmail:XXXX$npm publish# 发布模块<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h2 id="Node-js-REPL-交互式解释器"><a href="#Node-js-REPL-交互式解释器" class="headerlink" title="Node.js REPL(交互式解释器)"></a>Node.js REPL(交互式解释器)</h2><ul><li><p>REPL(Read Eval Print Loop:交互式解释器)，Node自带交互式解释器，可以执行读取,执行,打印,循环等任务</p></li><li><p>在Node的REPL中可以执行：</p><ul><li>简单的表达式计算</li></ul><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">$node&gt;1 + 45&gt; 5 &#x2F; 22.5&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>使用变量</li></ul><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">$ node&gt; 1+45&gt; 5&#x2F;22.5&gt; x&#x3D;1010&gt; var y&#x3D;10undefined&gt; x+y20&gt; console.log(&quot;Hello World!&quot;)Hello World!undefined&gt;   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>多行表达式</li></ul><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">$ node&gt; do &#123;... x++;... console.log(&quot;x:&quot;+x);... &#125;while(x&lt;5);x:1x:2x:3x:4x:5undefined&gt;  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>下划线变量<strong>[使用下划线(_)获取上一个表达式的运算结果]</strong></p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">$ node&gt; var x&#x3D;10;undefined&gt; y&#x3D;10;10&gt; x+y20&gt; var sum&#x3D;_undefined&gt; console.log(sum)20undefined&gt; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p>两次ctrl+c停止REPL</p><h2 id="Node-js的回调函数"><a href="#Node-js的回调函数" class="headerlink" title="Node.js的回调函数"></a>Node.js的回调函数</h2><ul><li><p>Node.js异步编程的直接体现就是回调</p></li><li><p>阻塞代码实例</p><ul><li><p>创建一个文件 input.txt ，内容如下：</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">Hello world!<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>创建 main.js 文件, 代码如下：</p><pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token keyword">var</span> fs <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"fs"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">var</span> data <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">readFileSync</span><span class="token punctuation">(</span><span class="token string">'input.txt'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span>data<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span><span class="token string">"程序执行结束!"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>以上代码执行结果如下：</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">$ node main.jsHello World！程序执行结束!<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p>非阻塞代码实例</p><pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token keyword">var</span> fs <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"fs"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>fs<span class="token punctuation">.</span><span class="token function">readFile</span><span class="token punctuation">(</span><span class="token string">'input.txt'</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token parameter">err<span class="token punctuation">,</span> data</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>err<span class="token punctuation">)</span> <span class="token keyword">return</span> console<span class="token punctuation">.</span><span class="token function">error</span><span class="token punctuation">(</span>err<span class="token punctuation">)</span><span class="token punctuation">;</span>    console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span>data<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span><span class="token string">"程序执行结束!"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>以上代码执行结果如下：</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">$ node main.js程序执行结束!Hello World!<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p>第一个实例在文件读取完后才执行完程序。 第二个实例我们不需要等待文件读取完，这样就可以在读取文件时同时执行接下来的代码，大大提高了程序的性能。</p></li><li><p>因此，阻塞是按顺序执行的，而非阻塞是不需要按顺序的，所以如果需要处理回调函数的参数，我们就需要写在回调函数内。</p></li></ul></li></ul>]]></content>
    
    
    <summary type="html">学习node.js</summary>
    
    
    
    <category term="js" scheme="https://changqingaas.github.io/categories/js/"/>
    
    
  </entry>
  
  <entry>
    <title>How-to-be-a-good-programmer</title>
    <link href="https://changqingaas.github.io/%E7%A2%8E%E7%A2%8E%E5%BF%B5/others/To-be-a-good-programmer/"/>
    <id>https://changqingaas.github.io/%E7%A2%8E%E7%A2%8E%E5%BF%B5/others/To-be-a-good-programmer/</id>
    <published>2021-08-29T12:30:16.000Z</published>
    <updated>2021-08-29T11:02:02.838Z</updated>
    
    <content type="html"><![CDATA[<h2 id="实例驱动学习"><a href="#实例驱动学习" class="headerlink" title="实例驱动学习"></a>实例驱动学习</h2><p>在知识爆炸的年代, 想成为一个好的程序员, 要具备这样一个精神:</p><p><strong>开发者精神</strong></p><blockquote><p>开发者精神是指, 从学习编程第一天起, 你的目标, 你所做的事, 永远不是以<code>要学会XX</code>为目标, 而是以<code>开发出XX</code>为目标, 深刻意识到你学习的一切, 最终是为了你的开发而服务的。</p></blockquote><p>而开发者精神的反面则是<code>学生气</code>:</p><p><strong>学生气</strong></p><blockquote><p>从学习编程的第一天起, 就把自己当学生, 上课听讲记笔记为第一位, 记忆知识点为第一位, 买教材读教材为第一位, 追求<code>学会XX</code>, 而不是<code>用它开发出XX</code>。</p></blockquote><p>我们举个例子， 假如你要学习Java，<code>学生气</code>的学生行为模式:</p><pre><code>读教材, 听课, 记笔记, 追求把这个语言的每个知识点都记得很清楚, 追求一种&quot;内功&quot;的修炼, 在这个过程中, 从来不想着用它去&quot;创造&quot;什么. 在学习的第一天起, 他给自己定下了一个目标:&quot;我这个学期一定要把这个语言的基础打牢, 最后在考试中取得高分, 并且为以后的学习提供更坚实的基础.&quot;</code></pre><p>那么具有<code>开发者精神</code>的人会这样做:</p><pre><code>先大体了解一下Java语言在哪些领域比较强势, 做一个简单的调研, 得出了Java在Web领域很强势这一个信号, 于是在学习Java的第一天起, 他就给自己定下了一个目标: &quot;我要在半年内通过学习Java, 运用Java做出一个中小型的, 基于Web的企业管理后台&quot;</code></pre><p>这两种人, 起点都一样, 都是<code>完全不会Java</code>, 但是最后的结果往往是, 后者无论是知识熟练度还是实用性都会超过前者, 而且整个学习过程会有源源不断地动力.</p><p><code>夯实基础</code>这个目标, 其实是空洞的, 什么叫做夯实? 什么叫做基础? 而且就算你真的夯实了基础, 你也极有可能陷入一种<code>虚无感</code>中, 因为你的所有知识, 都会遗忘.</p><p>这就是为什么很多计算机专业学生很爱问的一个问题:</p><pre><code>&quot;学了这么多知识, 忘了怎么办?&quot;</code></pre><p>如果你具备<code>开发者精神</code>, 那么你根本不会理会这个问题, 因为, 你开发出的东西, 你做出的产品, 它就是永久存在在这个世界上的, 你的成就感来源于真实的, 具体的, 可持久延续的项目中, 而不是来源于”我学会了什么”.</p><p>我们一定要明白:</p><pre><code>一切不谈成就感, 不谈反馈的学习劝导, 都是在耍流氓</code></pre><p>既然我们想获得反馈, 那么一个很现实的建议就是, 在Github上创建你的第一个开源代码仓库, 长期地, 稳定地commit, 当然, 至于这个仓库到底是干啥的, 这个因人而异, 有可能是你自己开发的一个VScode插件, 有可能是你自己写的读书笔记, 在这个不断地commit的过程中, 感受你的代码仓库不断增长不断完善的过程中, 你会获得一个比较持久的反馈和成就感, 一旦有了反馈和成就感, 那么你的学习动力就会一直保持, 也可以从学习中获得快乐.</p><p><strong>记住, 你不是学生, 你是开发者.</strong></p><h2 id="知识输入与输出"><a href="#知识输入与输出" class="headerlink" title="知识输入与输出"></a>知识输入与输出</h2><p>当你学习一个东西的时候, 如果学完马上用语言讲给别人听, 你会学的更好, 而且会发现新问题.</p><p>很多学习理论都指出, 知识的学习, 输入固然重要, 输出更为重要.</p><p>输入就是学习别人的知识, 输出就是把自己学会的知识用文字, 语言的形式表达出来, 很多人的学习, 只有输入, 没有输出, 这样的学习肯定是不行的.</p><p>几乎所有优秀的程序员, 都有攥写技术文章的习惯, 很多时候, 并不是他们什么都懂, 而是他们刚学会了什么, 然后就围绕着这个刚学会的东西, 用自己的语言讲出来, 久而久之, 就会被别人觉得是大牛, 但是他和你的区别, 有可能仅仅是是否输出的区别.</p><p>建议所有计算机学习者, 都要有写技术文章/读书笔记的习惯, 可以发表在自己的个人主页, 其他平台上, 攥写技术文章也可以很好地增加面试官对你的好感.</p><p>写技术文章的时候, 最好用<code>MarkDown</code>哦, 程序员是不需要用<code>Word</code>这种东西的, 我们的内容是要方便发表在网站上, <code>Word</code>不能直接在网站里显示, 而<code>MarkDown</code>可以轻易地转换成<code>html</code>格式文件, 在浏览器中显示.</p><h2 id="代码风格篇"><a href="#代码风格篇" class="headerlink" title="代码风格篇"></a>代码风格篇</h2><p>现在几乎所有主流语言都有相应的代码风格检查工具, 一般已IDE或Editor的插件或扩展形式给出</p><h3 id="设计模式"><a href="#设计模式" class="headerlink" title="设计模式"></a>设计模式</h3><p>设计模式并不是针对任何一种语言, 而是一种用编程语言描述现实世界问题广泛采用的各种模式, 比如, 观察者模式, 工厂模式, 依赖注入模式等.</p><p>好的设计模式, 是好代码的保证, 只有先学会了设计模式, 才能在软件设计中游刃有余, 而且更现实的问题是, 现在很多框架, 比如Spring, Angular等, 都充斥了大量的设计模式, 比如<code>Factory Pattern</code>, <code>Dependency Injection Pattern</code>, 你必须理解这些设计模式, 你才能理解这个框架.</p><p>个人认为设计模式没有必要买本教材, 结合视频和技术文章, 就足以理解和运用.</p><h3 id="小习惯"><a href="#小习惯" class="headerlink" title="小习惯"></a>小习惯</h3><ol><li>不要嫌变量名长, 最好可以直接通过变量名推测变量的作用.</li><li>重复出现的代码, 封装成独立的类或函数.</li><li>提前降低代码的耦合度, 不同作用, 不同类别的代码, 不要混在一起, 最好分成独立的文件</li><li>将代码进行业务分层, 比如在Web开发中, 有数据层, 服务层, DTO层, Controller层, 渲染层等, 只有将层次分开了, 才能获得足够的可拓展性, 不然代码多了你就全乱了.</li><li>用良好的设计模式去”设计”软件, 在执行一些算法的时候, 可以想一想它的时空复杂度, 想一想怎么可以让它执行地更快.</li></ol><h2 id="如何管理自己的电脑"><a href="#如何管理自己的电脑" class="headerlink" title="如何管理自己的电脑"></a>如何管理自己的电脑</h2><h3 id="包管理工具"><a href="#包管理工具" class="headerlink" title="包管理工具"></a>包管理工具</h3><p>为了更好的管理我们的各种应用包，最好使用包管理工具来进行各种包，软件的安装和使用, 方便进行卸载, 更新, 安装, 无需打开浏览器即可完成一系列操作.</p><blockquote><p>linux : apt-get (ubuntu distro)<br>    osx : brew<br>    windows: Chocolatey</p></blockquote><p>另外, windows平台上的包管理工具, 在国内的网络环境下经常慢的感人, 所以<code>Chocolatey</code>可能使用体验并不好.  推荐使用今年微软新发布的 <code>WSL2</code>, 这个东西不是那种传统的虚拟机, 有了它你可以像操作linux系统那样操作windows系统, 而且支持 <code>docker</code>, 甚至我推荐以后大家所有命令都可以在<code>WSL2</code>里执行.</p><h3 id="容器-Docker"><a href="#容器-Docker" class="headerlink" title="容器 - Docker"></a>容器 - Docker</h3><p>为了更方便的进行开发环境配置，我推荐所有计算机学生尽早了解和使用Docker。</p><p>Docker 就是为了解决复杂的环境配置问题而生的。</p><p>它将你的软件和软件所依赖的所有环境打包成一个镜像(Image), 该镜像可以在任何一台装有docker 的电脑上运行, 和操作系统无关, 也就是docker 把运行环境和你的操作系统隔离开来了，中间隔了一层docker engine 。</p><p>写过不少代码的你一定见过这种现象：你的代码在自己电脑上能运行，但是在别人电脑上就会报错，无法运行，原因很简单，任何软件的运行都需要环境。</p><p>比如，jar包的运行需要jre ，python脚本的运行需要python 解释器安装在电脑上，以后你可能还需要运行一些服务，比如数据库mysql server , redis , rabbitmq , 随着软件运行环境复杂度的增加, 你的软件运行条件也变得苛刻，如果你想把本地的应用部署到服务器上，那事更多，需要的环境得一个个地装到你的linux服务器上，如何彻底解决这个问题？</p><p>只需在你的电脑中安装docker , 你就可以毫无后顾之忧。</p><p>所有的环境，服务，软件都是以Image 的形式打包的，Image 中包含了运行你软件的所有东西，比如你的软件是个python 脚本, 并且使用了第三方库flask，那这个Image 中就包含了python ,也就是它的base image , 也同时包含了flask, 这样的话任何一个装有docker的电脑都可以运行你的image。</p>]]></content>
    
    
    <summary type="html">记住, 你不是学生, 你是开发者.</summary>
    
    
    
    <category term="碎碎念" scheme="https://changqingaas.github.io/categories/%E7%A2%8E%E7%A2%8E%E5%BF%B5/"/>
    
    
    <category term="Advice" scheme="https://changqingaas.github.io/tags/Advice/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫之Xpath解析</title>
    <link href="https://changqingaas.github.io/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%ABxpath%E8%A7%A3%E6%9E%90/"/>
    <id>https://changqingaas.github.io/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%ABxpath%E8%A7%A3%E6%9E%90/</id>
    <published>2021-08-27T06:25:32.000Z</published>
    <updated>2021-08-27T09:03:13.862Z</updated>
    
    <content type="html"><![CDATA[<h2 id="xpath解析"><a href="#xpath解析" class="headerlink" title="xpath解析"></a>xpath解析</h2><ul><li>// :从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。</li><li>/ : 匹配当前目录下的直接子节点。</li><li>.. : 匹配当前节点的父节点。</li><li>@：选取属性。</li><li>//* : 选取文档中所有元素</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">text &#x3D; &quot;&quot;&quot;            &lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;            &lt;bookstore&gt;            &lt;book&gt;              &lt;title lang&#x3D;&quot;eng&quot;&gt;Harry Potter&lt;&#x2F;title&gt;              &lt;price&gt;29.99&lt;&#x2F;price&gt;            &lt;&#x2F;book&gt;            &lt;book&gt;              &lt;title lang&#x3D;&quot;cn&quot;&gt;Learning XML&lt;&#x2F;title&gt;              &lt;price&gt;39.95&lt;&#x2F;price&gt;              &lt;aa lang&#x3D;&quot;cn eng aa bb&quot; name&#x3D;&quot;cc&quot;&gt;Learning XML&lt;&#x2F;aa&gt;            &lt;&#x2F;book&gt;            &lt;&#x2F;bookstore&gt;&quot;&quot;&quot;from lxml import etreehtml &#x3D; etree.HTML(text)# print(etree.tostring(html).decode(&#39;utf-8&#39;))# 选取所有指定的节点res &#x3D; html.xpath(&#39;&#x2F;&#x2F;book&#39;)# 获取指定节点的所有直接子节点res &#x3D; html.xpath(&#39;&#x2F;&#x2F;book&#x2F;aa&#39;)# 获取指定节点的父节点res &#x3D; html.xpath(&quot;&#x2F;&#x2F;aa&#x2F;..&quot;)# 通过属性匹配选择节点res &#x3D; html.xpath(&#39;&#x2F;&#x2F;title[@lang&#x3D;&quot;cn&quot;]&#39;)# 获取文本值res &#x3D; html.xpath(&#39;&#x2F;&#x2F;title[@lang&#x3D;&quot;cn&quot;]&#x2F;text()&#39;)res &#x3D; html.xpath(&#39;&#x2F;&#x2F;price&#x2F;text()&#39;)# 获取属性值 [&#39;eng&#39;, &#39;cn&#39;]res &#x3D; html.xpath(&#39;&#x2F;&#x2F;title&#x2F;@lang&#39;)# 属性多值匹配res &#x3D; html.xpath(&#39;&#x2F;&#x2F;aa[contains(@lang,&quot;aa&quot;)]&#39;)# 对于属性值有多个的节点，不用contains函数的话，匹配到的是空[]res &#x3D; html.xpath(&#39;&#x2F;&#x2F;aa[@lang&#x3D;&quot;aa&quot;]&#39;)# 文本匹配res &#x3D; html.xpath(&#39;&#x2F;&#x2F;title[contains(text(), &quot;XML&quot;)]&#39;)# 运算符res &#x3D; html.xpath(&#39;&#x2F;&#x2F;aa[contains(@lang,&quot;aa&quot;) and @name&#x3D;&quot;cc&quot;]&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    
    <summary type="html">Python爬虫之Xpath解析</summary>
    
    
    
    <category term="爬虫" scheme="https://changqingaas.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>Python爬虫之BeautifulSoup</title>
    <link href="https://changqingaas.github.io/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB%E4%B9%8BBeautifulSoup/"/>
    <id>https://changqingaas.github.io/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB%E4%B9%8BBeautifulSoup/</id>
    <published>2021-08-27T06:25:32.000Z</published>
    <updated>2021-08-27T09:03:44.333Z</updated>
    
    <content type="html"><![CDATA[<h4 id="格式化输出"><a href="#格式化输出" class="headerlink" title="格式化输出"></a>格式化输出</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">html_doc &#x3D; &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;&lt;body&gt;&lt;p class&#x3D;&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;&#x2F;b&gt;&lt;&#x2F;p&gt;&lt;p class&#x3D;&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href&#x3D;&quot;http:&#x2F;&#x2F;example.com&#x2F;elsie&quot; class&#x3D;&quot;sister&quot; id&#x3D;&quot;link1&quot;&gt;Elsie&lt;&#x2F;a&gt;,&lt;a href&#x3D;&quot;http:&#x2F;&#x2F;example.com&#x2F;lacie&quot; class&#x3D;&quot;sister&quot; id&#x3D;&quot;link2&quot;&gt;Lacie&lt;&#x2F;a&gt; and&lt;a href&#x3D;&quot;http:&#x2F;&#x2F;example.com&#x2F;tillie&quot; class&#x3D;&quot;sister bro&quot; id&#x3D;&quot;link3&quot;&gt;Tillie&lt;&#x2F;a&gt;;and they lived at the bottom of a well.&lt;&#x2F;p&gt;&lt;p class&#x3D;&quot;story&quot;&gt;...&lt;&#x2F;p&gt;&quot;&quot;&quot;soup &#x3D; BeautifulSoup(html_doc, &#39;html.parser&#39;)print(soup.prettify())<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="find-all-name-attrs-recursive-string-kwargs"><a href="#find-all-name-attrs-recursive-string-kwargs" class="headerlink" title="find_all(name , attrs , recursive , string , **kwargs)"></a>find_all(name , attrs , recursive , string , **kwargs)</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 查找所有的a标签res &#x3D; soup.find_all(&#39;a&#39;)# # 查找所有的a标签和p标签res &#x3D; soup.find_all([&#39;a&#39;, &#39;p&#39;])# 查找class&#x3D;title的p标签res &#x3D; soup.find_all(&#39;p&#39;, &#39;title&#39;)# 指定属性查找  可支持字符串，正则表达式，或者函数# 指定id查找元素res &#x3D; soup.find_all(id&#x3D;&quot;link1&quot;)# 指定href查找 [&lt;a class&#x3D;&quot;sister&quot; href&#x3D;&quot;http:&#x2F;&#x2F;example.com&#x2F;elsie&quot; id&#x3D;&quot;link1&quot;&gt;Elsie&lt;&#x2F;a&gt;]res &#x3D; soup.find_all(href&#x3D;re.compile(&#39;elsie&#39;))# 指定多个属性查找res &#x3D; soup.find_all(id&#x3D;&#39;link1&#39;, href&#x3D;re.compile(&#39;elsie&#39;))# 指定多个属性查找 attrs参数res &#x3D; soup.find_all(attrs&#x3D;&#123;&#39;id&#39;: &#39;link1&#39;, &#39;href&#39;: re.compile(&#39;elsie&#39;)&#125;)# 通过css搜索res &#x3D; soup.find_all(class_&#x3D;&quot;sister bro&quot;)# 通过函数过滤,查找类名长度大于6的元素res &#x3D; soup.find_all(class_&#x3D;lambda x: x is not None and len(x) &gt; 6)# recursive参数，如果只想搜索直接子节点  recursive&#x3D;Falseres &#x3D; soup.find_all(&#39;title&#39;, recursive&#x3D;False)# find_all() 方法的返回结果是值包含一个元素的列表# 而find()方法直接返回第一个结果，没有则返回None.res &#x3D; soup.find(&#39;a&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="CSS选择器"><a href="#CSS选择器" class="headerlink" title="CSS选择器"></a>CSS选择器</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 类查找res &#x3D; soup.select(&#39;.sister&#39;)# ID查找res &#x3D; soup.select(&#39;#link1&#39;)res &#x3D; soup.select(&#39;a#link1&#39;)# 通过是否存在某个属性查找res &#x3D; soup.select(&#39;a[href]&#39;)# 指定属性值查找res &#x3D; soup.select(&#39;a[href&#x3D;&quot;http:&#x2F;&#x2F;example.com&#x2F;tillie&quot;]&#39;)# 查找返回第一个元素res &#x3D; soup.select_one(&#39;a[href]&#39;)# 获取元素的属性值res &#x3D; soup.select_one(&#39;a[href]&#39;).get(&#39;href&#39;)# 获取元素的文本res &#x3D; soup.select_one(&#39;a[href]&#39;).text<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    
    <summary type="html">Python爬虫之BeautifulSoup</summary>
    
    
    
    <category term="爬虫" scheme="https://changqingaas.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>python爬虫之requests库</title>
    <link href="https://changqingaas.github.io/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB%E4%B9%8Brequests%E5%BA%93/"/>
    <id>https://changqingaas.github.io/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB%E4%B9%8Brequests%E5%BA%93/</id>
    <published>2021-08-27T06:25:32.000Z</published>
    <updated>2021-08-27T09:03:50.284Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一-发送请求"><a href="#一-发送请求" class="headerlink" title="一.发送请求"></a>一.发送请求</h2><p>requests提供了http的所有基本请求方式：<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">import requestsr &#x3D; requests.post(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;post&quot;)r &#x3D; requests.put(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;put&quot;)r &#x3D; requests.delete(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;delete&quot;)r &#x3D; requests.head(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;get&quot;)r &#x3D; requests.options(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;get&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><p>基本get请求中参数的传递：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># requests允许使用params关键字参数，以字典的形式来提供get请求url中的参数。payload &#x3D; &#123;&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;&#125;r &#x3D; requests.get(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;get&quot;, params&#x3D;payload)print(r.url)  # http:&#x2F;&#x2F;httpbin.org&#x2F;get?key2&#x3D;value2&amp;key1&#x3D;value1# 字典中的value还可以以列表的形式传入payload &#x3D; &#123;&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: [&#39;value2&#39;, &#39;value3&#39;]&#125;r &#x3D; requests.get(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;get&#39;, params&#x3D;payload)print(r.url)http:&#x2F;&#x2F;httpbin.org&#x2F;get?key1&#x3D;value1&amp;key2&#x3D;value2&amp;key2&#x3D;value3<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>添加请求头headers<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">url &#x3D; &#39;https:&#x2F;&#x2F;api.github.com&#x2F;some&#x2F;endpoint&#39;headers &#x3D; &#123;&#39;user-agent&#39;: &#39;my-app&#x2F;0.0.1&#39;&#125;r &#x3D; requests.get(url, headers&#x3D;headers)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></p><p>Post请求<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">payload &#x3D; &#123;&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;&#125;r &#x3D; requests.post(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;post&quot;, data&#x3D;payload)print(r.text)# 可以为 data 参数传入一个元组列表# 在表单中多个元素使用同一 key 的时候，这种方式尤其有效：payload &#x3D; ((&#39;key1&#39;, &#39;value1&#39;), (&#39;key1&#39;, &#39;value2&#39;))r &#x3D; requests.post(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;post&#39;, data&#x3D;payload)print(r.text)&#123;  ...  &quot;form&quot;: &#123;    &quot;key1&quot;: [      &quot;value1&quot;,      &quot;value2&quot;    ]  &#125;,  ...&#125;# post的为json对象url &#x3D; &#39;https:&#x2F;&#x2F;api.github.com&#x2F;some&#x2F;endpoint&#39;payload &#x3D; &#123;&#39;some&#39;: &#39;data&#39;&#125;r &#x3D; requests.post(url, json&#x3D;payload)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><p>超时设置：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">requests.get(&#39;http:&#x2F;&#x2F;github.com&#39;, timeout&#x3D;0.001)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="二-响应内容"><a href="#二-响应内容" class="headerlink" title="二.响应内容"></a>二.响应内容</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">import requestsr &#x3D; requests.get(&#39;https:&#x2F;&#x2F;github.com&#x2F;timeline.json&#39;)r.encoding&#x3D;&#39;utf-8&#39;r.text# [&#123;&quot;repository&quot;:&#123;&quot;open_issues&quot;:0,&quot;url&quot;:&quot;https:&#x2F;&#x2F;github.com&#x2F;...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>网页乱码问题:<br><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 查看网页编码print(res.apparent_encoding)# 设置编码res.encoding &#x3D; &#39;GB2312&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></p><p>二进制响应内容(r.content)<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">from PIL import Imagefrom io import BytesIO#BytesIO用于操作内存中的二进制数据img&#x3D;Image.open(BytesIO(r.content))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></p><p>JSON响应内容（r.json()）<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">import requestsr &#x3D; requests.get(&#39;https:&#x2F;&#x2F;github.com&#x2F;timeline.json&#39;)r.json()# [&#123;u&#39;repository&#39;: &#123;u&#39;open_issues&#39;: 0, u&#39;url&#39;: &#39;https:&#x2F;&#x2F;github.com&#x2F;...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></p><p>响应状态码（r.status_code）<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">r &#x3D; requests.get(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;get&#39;)r.status_code200<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></p><p>响应头(r.headers)<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">r.headers&#123;    &#39;content-encoding&#39;: &#39;gzip&#39;,    &#39;transfer-encoding&#39;: &#39;chunked&#39;,    &#39;connection&#39;: &#39;close&#39;,    &#39;server&#39;: &#39;nginx&#x2F;1.0.4&#39;,    &#39;x-runtime&#39;: &#39;148ms&#39;,    &#39;etag&#39;: &#39;&quot;e1ca502697e5c9317743dc078f67693f&quot;&#39;,    &#39;content-type&#39;: &#39;application&#x2F;json&#39;&#125;r.headers[&#39;Content-Type&#39;]&#39;application&#x2F;json&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h2 id="三-Cookies"><a href="#三-Cookies" class="headerlink" title="三.Cookies"></a>三.Cookies</h2><p>如果某个响应中包含一些 cookie，你可以快速访问它们：<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">url &#x3D; &#39;http:&#x2F;&#x2F;example.com&#x2F;some&#x2F;cookie&#x2F;setting&#x2F;url&#39;r &#x3D; requests.get(url)r.cookies[&#39;example_cookie_name&#39;]# &#39;example_cookie_value&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><p>要想发送你的cookies到服务器，可以使用 cookies 参数：<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">url &#x3D; &#39;http:&#x2F;&#x2F;httpbin.org&#x2F;cookies&#39;cookies &#x3D; dict(cookies_are&#x3D;&#39;working&#39;)r &#x3D; requests.get(url, cookies&#x3D;cookies)r.text# &#39;&#123;&quot;cookies&quot;: &#123;&quot;cookies_are&quot;: &quot;working&quot;&#125;&#125;&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h2 id="四-会话"><a href="#四-会话" class="headerlink" title="四.会话"></a>四.会话</h2><p>requests.Session()这样可以在会话中保留状态，保持cookie等<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">import requestss &#x3D; requests.Session()s.headers.update(&#123;&#39;x-test&#39;: &#39;true&#39;&#125;)r &#x3D; s.get(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;headers&#39;, headers&#x3D;&#123;&#39;x-test2&#39;: &#39;true&#39;&#125;)print(r.text)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h2 id="五-代理"><a href="#五-代理" class="headerlink" title="五.代理"></a>五.代理</h2><p>如果需要使用代理，你可以通过为任意请求方法提供 proxies 参数来配置单个请求<br><pre class="line-numbers language-python" data-language="python"><code class="language-python"># http代理import requestsproxies &#x3D; &#123;  &quot;https&quot;: &quot;http:&#x2F;&#x2F;41.118.132.69:4433&quot;&#125;r &#x3D; requests.post(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;post&quot;, proxies&#x3D;proxies)# socks代理proxies &#x3D; &#123;    &#39;http&#39;: &#39;socks5:&#x2F;&#x2F;user:pass@host:port&#39;,    &#39;https&#39;: &#39;socks5:&#x2F;&#x2F;user:pass@host:port&#39;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p><h2 id="六-Prepared-Request"><a href="#六-Prepared-Request" class="headerlink" title="六.Prepared Request"></a>六.Prepared Request</h2><p>构造requests.Request对象，将Request对象作为参数传入requests.Session()对象的prepare_request()方法中，最后通过Session对象的send()方法发送请求。<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">import requestsfrom requests import Requesturl &#x3D; &#39;http:&#x2F;&#x2F;httpbin.org&#x2F;get&#39;# 创建Session对象s &#x3D; requests.Session()# 构造Request对象req &#x3D; Request(&#39;GET&#39;,url)# 将Request对象转换成 PreparedRequest对象prepped &#x3D; s.prepare_request(req)# 利用Session对象的send()方法，发送PreparedRequest对象res &#x3D; s.send(prepped)print(res.text)print(type(prepped))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p>]]></content>
    
    
    <summary type="html">python爬虫之requests库</summary>
    
    
    
    <category term="爬虫" scheme="https://changqingaas.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>Python爬虫之selenium自动化</title>
    <link href="https://changqingaas.github.io/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB%E4%B9%8Bselenium%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    <id>https://changqingaas.github.io/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB%E4%B9%8Bselenium%E8%87%AA%E5%8A%A8%E5%8C%96/</id>
    <published>2021-08-27T06:25:32.000Z</published>
    <updated>2021-08-27T09:03:39.917Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-基础操作"><a href="#1-基础操作" class="headerlink" title="1.基础操作"></a>1.基础操作</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">from selenium import webdriverimport timefrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.common.by import Byif __name__ &#x3D;&#x3D; &#39;__main__&#39;:    # 谷歌浏览器驱动    chromedriver_path &#x3D; &#39;chromedriver72.exe&#39;    options &#x3D; webdriver.ChromeOptions()    # 躲避部分网站selenium检测    options.add_experimental_option(&#39;excludeSwitches&#39;, [&#39;enable-automation&#39;])    options.add_experimental_option(&quot;useAutomationExtension&quot;, False)    driver &#x3D; webdriver.Chrome(executable_path&#x3D;chromedriver_path, options&#x3D;options)    # 躲避部分网站selenium检测    script &#x3D; &quot;Object.defineProperty(navigator, &#39;webdriver&#39;, &#123;get: () &#x3D;&gt; undefined&#125;);&quot;    driver.execute_cdp_cmd(&quot;Page.addScriptToEvaluateOnNewDocument&quot;, &#123;&quot;source&quot;: script&#125;)    # 浏览器最大化    driver.maximize_window()    url &#x3D; &#39;https:&#x2F;&#x2F;www.python.org&#x2F;&#39;    driver.get(url)    # 显式等待    wait &#x3D; WebDriverWait(driver, 20, 1)    # 在主页输入框搜索requests，并点击搜索    input_ &#x3D; wait.until(EC.presence_of_element_located((By.ID, &#39;id-search-field&#39;)))    input_.send_keys(&#39;requests&#39;)    time.sleep(1)    btn &#x3D; driver.find_element_by_xpath(&#39;&#x2F;&#x2F;button[@title&#x3D;&quot;Submit this Search&quot;]&#39;)    btn.click()    time.sleep(10)    driver.close()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-元素定位"><a href="#2-元素定位" class="headerlink" title="2.元素定位"></a>2.元素定位</h2><h4 id="查找单个元素"><a href="#查找单个元素" class="headerlink" title="查找单个元素"></a>查找单个元素</h4><p>最常用的定位元素的两个方法是通过Xpath和id来定位。</p><ul><li>find_element_by_id</li><li>find_element_by_xpath</li></ul><h4 id="查找多个元素"><a href="#查找多个元素" class="headerlink" title="查找多个元素"></a>查找多个元素</h4><ul><li>find_elements_by_xpath</li><li>find_elements_by_name</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 通过xpath查找元素driver.find_element_by_xpath(&#39;&#x2F;&#x2F;button[@title&#x3D;&quot;Submit this Search&quot;]&#39;)# 通过id查找元素driver.find_element_by_id(&#39;id-search-field&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="获取元素内部内容"><a href="#获取元素内部内容" class="headerlink" title="获取元素内部内容"></a>获取元素内部内容</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">text &#x3D; driver.find_element_by_xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;ISDCaptcha&quot;]&#x2F;div[2]&#x2F;div&#39;).get_attribute(&#39;innerHTML&#39;)if &#39;请绘制图中手势&#39; in text:    print(&#39;出现行为认证&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h4 id="获取元素指定属性的属性值"><a href="#获取元素指定属性的属性值" class="headerlink" title="获取元素指定属性的属性值"></a>获取元素指定属性的属性值</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">driver.find_element_by_xpath(&#39;&#x2F;&#x2F;div[@id&#x3D;&quot;find-step3-phone&quot;]&#39;).get_attribute(&#39;style&#39;)driver.find_element_by_xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;imgVerifyCodeP&quot;]&#39;).get_attribute(&#39;src&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h4 id="切换到指定iframe"><a href="#切换到指定iframe" class="headerlink" title="切换到指定iframe"></a>切换到指定iframe</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 通过id或者名称driver.switch_to.frame(&quot;iframeLoginIfm&quot;)driver.switch_to.frame(0)frame &#x3D; driver.find_element_by_xpath(&#39;&#x2F;&#x2F;div[@id&#x3D;&quot;loginDiv&quot;]&#x2F;iframe&#39;)driver.switch_to.frame(frame)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="切换到指定窗口"><a href="#切换到指定窗口" class="headerlink" title="切换到指定窗口"></a>切换到指定窗口</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">driver.switch_to.window(browser.window_handles[1])<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="切换到alert弹窗"><a href="#切换到alert弹窗" class="headerlink" title="切换到alert弹窗"></a>切换到alert弹窗</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">text &#x3D; driver.switch_to.alert.textif &#39;图片验证码输入错误&#39; in text:    print(&#39;图片验证码识别错误&#39;)    driver.switch_to.alert.accept()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-元素交互"><a href="#3-元素交互" class="headerlink" title="3.元素交互"></a>3.元素交互</h2><h4 id="按钮点击"><a href="#按钮点击" class="headerlink" title="按钮点击"></a>按钮点击</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">btn &#x3D; driver.find_element_by_xpath(&#39;&#x2F;&#x2F;div[@role&#x3D;&quot;button&quot;]&#x2F;div&#x2F;span&#x2F;span&#39;)btn.click()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h4 id="执行js代码"><a href="#执行js代码" class="headerlink" title="执行js代码"></a>执行js代码</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">style_ &#x3D; driver.find_element_by_xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;passport-login-pop&quot;]&#39;).get_attribute(&#39;style&#39;)style_ &#x3D; style_.replace(&#39;display: none;&#39;, &#39;&#39;)if not style_:    style_ &#x3D; &#39;left: 259px; top: 212px; z-index: 60001;&#39;js &#x3D; &#39;document.getElementById(&quot;passport-login-pop&quot;).setAttribute(&quot;style&quot;,&quot;&#123;&#125;&quot;);&#39;.format(style_)driver.execute_script(js)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="表单输入"><a href="#表单输入" class="headerlink" title="表单输入"></a>表单输入</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">input_ &#x3D; driver.find_element_by_xpath(&#39;&#x2F;&#x2F;input[@name&#x3D;&quot;session[password]&quot; and @dir&#x3D;&quot;auto&quot;]&#39;)input_.send_keys(&#39;123qwe&#39;)from selenium.webdriver.common.keys import Keysinput_.send_keys(Keys.BACK_SPACE)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="页面滚动"><a href="#页面滚动" class="headerlink" title="页面滚动"></a>页面滚动</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">driver.execute_script(&quot;&quot;&quot;                (function () &#123;                    var y &#x3D; document.body.scrollTop;                    var step &#x3D; 100;                    window.scroll(0, y);                    function f() &#123;                        if (y &lt; document.body.scrollHeight) &#123;                            y +&#x3D; step;                            window.scroll(0, y);                            setTimeout(f, 50);                        &#125;                        else &#123;                            window.scroll(0, y);                            document.title +&#x3D; &quot;scroll-done&quot;;                        &#125;                    &#125;                    setTimeout(f, 1000);                &#125;)();                &quot;&quot;&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="模拟拖动"><a href="#模拟拖动" class="headerlink" title="模拟拖动"></a>模拟拖动</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">from selenium.webdriver.common.action_chains import ActionChainsdef get_track(self, distance):    track &#x3D; []    current &#x3D; 0    mid &#x3D; distance * 3 &#x2F; 4    t &#x3D; 0.2    v &#x3D; 0    while current &lt; distance:        if current &lt; mid:            a &#x3D; 2        else:            a &#x3D; -3        v0 &#x3D; v        v &#x3D; v0 + a * t        move &#x3D; v0 * t + 1 &#x2F; 2 * a * t * t        current +&#x3D; move        track.append(round(move))    return track# 模拟拖动btn &#x3D; wait.until(EC.presence_of_element_located((By.XPATH, xpath_)))track &#x3D; get_track(500)action &#x3D; ActionChains(browser)action.click_and_hold(btn).perform()action.reset_actions()for i in track:    action.move_by_offset(xoffset&#x3D;i, yoffset&#x3D;0).perform()    action.reset_actions()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-等待"><a href="#4-等待" class="headerlink" title="4.等待"></a>4.等待</h2><h4 id="显式等待"><a href="#显式等待" class="headerlink" title="显式等待"></a>显式等待</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 显式等待wait &#x3D; WebDriverWait(driver, 20, 1)input_ &#x3D; wait.until(EC.presence_of_element_located((By.ID, &#39;id-search-field&#39;)))input_.send_keys(&#39;requests&#39;)time.sleep(1)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="隐式等待"><a href="#隐式等待" class="headerlink" title="隐式等待"></a>隐式等待</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">from selenium import webdriverdriver &#x3D; webdriver.Chrome()# 隐式等待driver.implicitly_wait(10)driver.get(&#39;https:&#x2F;&#x2F;www.zhihu.com&#x2F;explore&#39;)logo &#x3D; driver.find_element_by_id(&#39;zh-top-link-logo&#39;)print(logo)driver.close()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="5-其他操作"><a href="#5-其他操作" class="headerlink" title="5.其他操作"></a>5.其他操作</h2><h4 id="解决页面加载时间过长问题"><a href="#解决页面加载时间过长问题" class="headerlink" title="解决页面加载时间过长问题"></a>解决页面加载时间过长问题</h4><p>有时候页面有些静态文件加载比较耗时，selenium可以不需要等待页面全部加载完全在去查找元素<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">options &#x3D; webdriver.ChromeOptions()# 解决页面加载阻塞问题options.set_capability(&#39;pageLoadStrategy&#39;, &#39;none&#39;)driver &#x3D; webdriver.Chrome(executable_path&#x3D;self.chromedriver_path, options&#x3D;options)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></p><h4 id="添加请求头"><a href="#添加请求头" class="headerlink" title="添加请求头"></a>添加请求头</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">options.add_argument(&quot;user-agent&#x3D;&#123;&#125;&quot;.format(&#39;Mozilla&#x2F;5.0 (Windows NT 10.0; WOW64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.100 Safari&#x2F;537.36&#39;))<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="添加代理"><a href="#添加代理" class="headerlink" title="添加代理"></a>添加代理</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">socks5 &#x3D; &quot;socks5:&#x2F;&#x2F;&#123;&#125;:&#123;&#125;&quot;.format(socks5_proxy_ip, socks5_proxy_port)options.add_argument(&quot;--proxy-server&#x3D;&#123;&#125;&quot;.format(socks5))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h4 id="捕捉F12控制台中所有请求记录"><a href="#捕捉F12控制台中所有请求记录" class="headerlink" title="捕捉F12控制台中所有请求记录"></a>捕捉F12控制台中所有请求记录</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">from selenium.webdriver.common.desired_capabilities import DesiredCapabilitiesd &#x3D; DesiredCapabilities.CHROMEd[&#39;loggingPrefs&#39;] &#x3D; &#123;&#39;performance&#39;: &#39;ALL&#39;&#125;d[&#39;goog:chromeOptions&#39;] &#x3D; &#123;    &#39;perfLoggingPrefs&#39;: &#123;        &#39;enableNetwork&#39;: True,    &#125;,    &#39;w3c&#39;: False,&#125;options.add_experimental_option(&#39;perfLoggingPrefs&#39;, &#123;&#39;enableNetwork&#39;: True&#125;)options.add_experimental_option(&#39;w3c&#39;, False)driver &#x3D; webdriver.Chrome(executable_path&#x3D;self.chromedriver_path, options&#x3D;options, desired_capabilities&#x3D;d)# 保存loglog_list &#x3D; []for entry in driver.get_log(&#39;performance&#39;):    log_list.append(entry)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="屏幕截图-可以截取图片验证码加以识别"><a href="#屏幕截图-可以截取图片验证码加以识别" class="headerlink" title="屏幕截图,可以截取图片验证码加以识别"></a>屏幕截图,可以截取图片验证码加以识别</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">import win32conimport win32guiimport win32printfrom win32api import GetSystemMetricsfrom PIL import Imagedef get_real_resolution():    &quot;&quot;&quot;获取真实的分辨率&quot;&quot;&quot;    hDC &#x3D; win32gui.GetDC(0)    # 横向分辨率    w &#x3D; win32print.GetDeviceCaps(hDC, win32con.DESKTOPHORZRES)    # 纵向分辨率    h &#x3D; win32print.GetDeviceCaps(hDC, win32con.DESKTOPVERTRES)    return w, hdef get_screen_size():    &quot;&quot;&quot;获取缩放后的分辨率&quot;&quot;&quot;    w &#x3D; GetSystemMetrics(0)    h &#x3D; GetSystemMetrics(1)    return w, hreal_resolution &#x3D; get_real_resolution()screen_size &#x3D; get_screen_size()screen_scale_rate &#x3D; round(real_resolution[0] &#x2F; screen_size[0], 2)pic_name &#x3D; &#39;***.png&#39;driver.save_screenshot(pic_name)# 找到图片验证码元素element &#x3D; driver.find_element_by_xpath(xpath_)left &#x3D; element.location[&#39;x&#39;] * screen_scale_ratetop &#x3D; element.location[&#39;y&#39;] * screen_scale_rateright &#x3D; (element.location[&#39;x&#39;] + element.size[&#39;width&#39;]) * screen_scale_ratebottom &#x3D; (element.location[&#39;y&#39;] + element.size[&#39;height&#39;]) * screen_scale_rateim &#x3D; Image.open(pic_name)# 裁剪图片im &#x3D; im.crop((left, top, right, bottom))im.save(pic_name)# 把图片转成base64,利用打码平台接口识别with open(pic_name, &#39;rb&#39;) as f:    code_img_base64 &#x3D; base64.b64encode(f.read()).decode()os.remove(pic_name)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    
    <summary type="html">Python爬虫之selenium自动化</summary>
    
    
    
    <category term="爬虫" scheme="https://changqingaas.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>爬虫概念</title>
    <link href="https://changqingaas.github.io/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%A6%82%E5%BF%B5/"/>
    <id>https://changqingaas.github.io/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%A6%82%E5%BF%B5/</id>
    <published>2021-08-27T06:25:32.000Z</published>
    <updated>2021-08-27T09:03:55.693Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-robots协议"><a href="#1-robots协议" class="headerlink" title="1.robots协议"></a>1.robots协议</h2><p>也叫robots.txt，是存放在网站根目录下的文本文件，用来告诉搜索引擎该网站哪些内容是不应该被抓取的，哪些是可以抓取的。</p><p>如<a href="https://www.csdn.net/robots.txt">https://www.csdn.net/robots.txt</a></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">User-agent: *Disallow: &#x2F;scriptsDisallow: &#x2F;publicDisallow: &#x2F;css&#x2F;Disallow: &#x2F;images&#x2F;Disallow: &#x2F;content&#x2F;Disallow: &#x2F;ui&#x2F;Disallow: &#x2F;js&#x2F;Disallow: &#x2F;scripts&#x2F;Disallow: &#x2F;article_preview.html*Disallow: &#x2F;tag&#x2F;Disallow: &#x2F;*?*Disallow: &#x2F;link&#x2F;Sitemap: https:&#x2F;&#x2F;www.csdn.net&#x2F;sitemap-aggpage-index.xmlSitemap: https:&#x2F;&#x2F;www.csdn.net&#x2F;article&#x2F;sitemap.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-常见的反爬虫措施"><a href="#2-常见的反爬虫措施" class="headerlink" title="2.常见的反爬虫措施"></a>2.常见的反爬虫措施</h2><h4 id="1-请求头校验"><a href="#1-请求头校验" class="headerlink" title="1.请求头校验"></a>1.请求头校验</h4><p>一般网站会对请求头进行校验，比如Host，UA，Content-Type字段等，模拟请求的时候，这些常见的请求头最好是带上。</p><h4 id="2-IP访问次数控制"><a href="#2-IP访问次数控制" class="headerlink" title="2.IP访问次数控制"></a>2.IP访问次数控制</h4><p>同一个IP地址短时间内大量发起请求，会引起IP限制，解决方法是用代理IP，或者构建自己的代理IP池。</p><h4 id="3-接口请求频率限制"><a href="#3-接口请求频率限制" class="headerlink" title="3.接口请求频率限制"></a>3.接口请求频率限制</h4><p>有的网站会控制接口访问的频率，比如有些查询接口，控制两三秒访问一次。</p><h4 id="4-接口访问次数限制"><a href="#4-接口访问次数限制" class="headerlink" title="4.接口访问次数限制"></a>4.接口访问次数限制</h4><p>每天限制某个IP或账号访问接口的次数，达到上限后出现二次验证或者直接封账号/IP.比如登录接口</p><h4 id="5-行为认证"><a href="#5-行为认证" class="headerlink" title="5.行为认证"></a>5.行为认证</h4><p>请求次数过多会出现人工认证，如图片验证码，滑动认证，点击认证等，可以对接打码平台。</p><h4 id="6，自动化环境检测"><a href="#6，自动化环境检测" class="headerlink" title="6，自动化环境检测"></a>6，自动化环境检测</h4><p>selenium自动化工具有的网站会检测出来，大部分可以通过下面两种方式跳过检测,下面两种方式无法处理的话，还可以尝试把页面改为移动端页面(手机模式)，最后还有一种方法就是代理服务器拦截修改js代码，把检测selenium的js修改掉。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">options &#x3D; webdriver.ChromeOptions()# 躲避部分网站selenium检测options.add_experimental_option(&#39;excludeSwitches&#39;, [&#39;enable-automation&#39;])options.add_experimental_option(&quot;useAutomationExtension&quot;, False)driver &#x3D; webdriver.Chrome(executable_path&#x3D;chromedriver_path, options&#x3D;options)# 躲避部分网站selenium检测script &#x3D; &quot;Object.defineProperty(navigator, &#39;webdriver&#39;, &#123;get: () &#x3D;&gt; undefined&#125;);&quot;driver.execute_cdp_cmd(&quot;Page.addScriptToEvaluateOnNewDocument&quot;, &#123;&quot;source&quot;: script&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对于移动端appium的检测，可以尝试替换为uiautomator2实现自动化</p><h4 id="7-数据动态加载"><a href="#7-数据动态加载" class="headerlink" title="7.数据动态加载"></a>7.数据动态加载</h4><p>有的数据不是通过html页面的接口请求返回的，抓包分析请求，找到正确的数据接口。</p><h4 id="8-请求参数加密"><a href="#8-请求参数加密" class="headerlink" title="8.请求参数加密"></a>8.请求参数加密</h4><p>网易云音乐的post请求的请求体就是前端经过js加密后计算得到的，需要逆向js代码</p><h4 id="9-返回数据加密"><a href="#9-返回数据加密" class="headerlink" title="9.返回数据加密"></a>9.返回数据加密</h4><p>需要逆向js代码，分析如何解密。还有一种像大众点评的评论，需要通过定位去找到文本。</p><h4 id="10-动态更新cookies"><a href="#10-动态更新cookies" class="headerlink" title="10.动态更新cookies"></a>10.动态更新cookies</h4><p>华为手机云服务，每次请求接口都会重新设置cookies，并且请求头参数也需要跟着cookies一起变化</p>]]></content>
    
    
    <summary type="html">爬虫概念</summary>
    
    
    
    <category term="爬虫" scheme="https://changqingaas.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
  </entry>
  
</feed>
