<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Policy Gradient Methods</title>
    <url>/MARL/MADRL/Policy%20Gradient%20Mwthods/</url>
    <content><![CDATA[<p>上一篇博文的内容整理了我们如何去近似价值函数或者是动作价值函数的方法：</p>
<script type="math/tex; mode=display">
V_{\theta}(s)\approx V^{\pi}(s) \\
Q_{\theta}(s)\approx Q^{\pi}(s, a)</script><p>通过机器学习的方法我们一旦近似了价值函数或者是动作价值函数就可以通过一些策略进行控制，比如  ϵ-greedy。</p>
<p>通过机器学习的方法我们一旦近似了价值函数或者是动作价值函数就可以通过一些策略进行控制，比如 ϵ -greedy。</p>
<p>那么我们简单回顾下 RL 的学习目标：通过 agent 与环境进行交互，获取累计回报最大化。既然我们最终要学习如何与环境交互的策略，那么我们可以直接学习策略 ，而之前先近似价值函数，再通过贪婪策略控制的思路更像是”曲线救国”。<br>这就是本篇文章的内容，我们如何直接来学习策略，用数学的形式表达就是：</p>
<p>$\pi_{\theta}(s, a) = P[a | s, \theta]$</p>
<p>这就是被称为策略梯度（Policy Gradient，简称PG）算法。</p>
<p>当然，本篇内容同样的是针对 model-free 的强化学习。</p>
<h1 id="Value-Based-vs-Policy-Based-RL"><a href="#Value-Based-vs-Policy-Based-RL" class="headerlink" title="Value-Based vs. Policy-Based RL"></a>Value-Based vs. Policy-Based RL</h1><p>Value-Based：</p>
<ul>
<li>学习价值函数</li>
<li>Implicit policy，比如 ϵϵ-greedy</li>
</ul>
<p>Policy-Based：</p>
<ul>
<li>没有价值函数</li>
<li>直接学习策略</li>
</ul>
<p>Actor-Critic：</p>
<ul>
<li>学习价值函数</li>
<li>学习策略</li>
</ul>
<p>三者的关系可以形式化地表示如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006124701.png" alt=""></p>
<p>认识到 Value-Based 与 Policy-Based 区别后，我们再来讨论Policy-Based RL 的优缺点：</p>
<p>优点：</p>
<ul>
<li>收敛性更好</li>
<li>对于具有高维或者连续动作空间的问题更加有效</li>
<li>可以学习随机策略</li>
</ul>
<p>缺点：</p>
<ul>
<li>绝大多数情况下收敛到局部最优点，而非全局最优</li>
<li>评估一个策略一般情况下低效且存在较高的方差</li>
</ul>
<h1 id="Policy-Search"><a href="#Policy-Search" class="headerlink" title="Policy Search"></a>Policy Search</h1><p>我们首先定义下目标函数。</p>
<h2 id="Policy-Objective-Functions"><a href="#Policy-Objective-Functions" class="headerlink" title="Policy Objective Functions"></a>Policy Objective Functions</h2><p>目标：给定一个带有参数 θ 的策略 π~θ~(s,a)  ，找到最优的参数 θ 。<br>但是我们如何评估不同参数下策略 π~θ(~s,a)  的优劣呢？</p>
<ul>
<li>对于episode 任务来说，我们可以使用start value：</li>
</ul>
<script type="math/tex; mode=display">
J_1(\theta)=V^{\pi_{\theta}}(s_1)=E_{\pi_{\theta}}[v_1]</script><ul>
<li>对于连续性任务来说，我们可以使用 average value：</li>
</ul>
<script type="math/tex; mode=display">
J_{avV}(\theta)=\sum_{s}d^{\pi_{\theta}}(s)V^{\pi_{\theta}}(s)</script><p>或者每一步的平均回报：</p>
<script type="math/tex; mode=display">
J_{avR}(\theta)=\sum_{s}d^{\pi_{\theta}}(s)\sum_{a}\pi_{\theta}(s, a)R_s^a</script><p>其中 d^πθ^(s)  是马尔卡夫链在 π~θ~  下的静态分布。</p>
<h1 id="Policy-Optimisation"><a href="#Policy-Optimisation" class="headerlink" title="Policy Optimisation"></a>Policy Optimisation</h1><p>在明确目标以后，我们再来看基于策略的 RL 为一个典型的优化问题：找出 θ 最大化 J(θ)<br>最优化的方法有很多，比如不依赖梯度（gradient-free）的算法：</p>
<ul>
<li>爬山算法</li>
<li>模拟退火</li>
<li>进化算法</li>
<li>…</li>
</ul>
<p>但是一般来说，如果我们能在问题中获得梯度的话，基于梯度的最优化方法具有比较好的效果：</p>
<ul>
<li>梯度下降</li>
<li>共轭梯度</li>
<li>拟牛顿法</li>
<li>…</li>
</ul>
<p>我们本篇讨论梯度下降的方法。</p>
<h1 id="策略梯度定理"><a href="#策略梯度定理" class="headerlink" title="策略梯度定理"></a>策略梯度定理</h1><h1 id="蒙特卡洛策略梯度算法（REINFORCE）"><a href="#蒙特卡洛策略梯度算法（REINFORCE）" class="headerlink" title="蒙特卡洛策略梯度算法（REINFORCE）"></a>蒙特卡洛策略梯度算法（REINFORCE）</h1><h1 id="Actir-Critic-策略梯度算法"><a href="#Actir-Critic-策略梯度算法" class="headerlink" title="Actir-Critic 策略梯度算法"></a>Actir-Critic 策略梯度算法</h1><p>Monte-Carlo策略梯度的方差较高，因此放弃用return来估计行动-价值函数Q，而是使用 critic 来估计Q：</p>
<p>$Q_w(s, a)\approx Q^{\pi_{\theta}}(s, a)$</p>
<p>这就是大名鼎鼎的 Actor-Critic 算法，它有两套参数：</p>
<ul>
<li>Critic：更新动作价值函数参数 w </li>
<li>Actor： 朝着 Critic 方向更新策略参数 θ</li>
</ul>
<p>Actor-Critic 算法是一个近似的策略梯度算法：</p>
<script type="math/tex; mode=display">
\triangledown_\theta J(\theta)\approx E_{\pi_{\theta}}[\triangledown_{\theta}\log \pi_{\theta}(s, a)Q_w(s, a)]\\
\Delta\theta = \alpha\triangledown_\theta\log\pi_{\theta}(s,a)Q_w(s,a)</script><p>Critic 本质就是在进行策略评估：How good is policy π~θ~  for current parameters θ </p>
<p>策略评估我们之前介绍过MC、TD、TD(λλ)，以及价值函数近似方法。如下所示，简单的 Actir-Critic 算法 Critic 为动作价值函数近似，使用最为简单的线性方程，即：$Q_w(s, a) = \phi(s, a)^T w$，具体的伪代码如下所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006125435.png" alt="img"></p>
<p>在 Actir-Critic 算法中，对策略进行了估计，这会产生误差（bias），但是当满足以下两个条件时，策略梯度是准确的：</p>
<ul>
<li>价值函数的估计值没有和策略相违背，即： $\triangledown_w Q_w(s,a) = \triangledown_\theta\log\pi_{\theta}(s,a)$</li>
<li>价值函数的参数w能够最小化误差，即： $\epsilon = E_{\pi_{\theta}}[(Q^{\pi_{\theta}}(s, a) - Q_w(s,a))^2]$</li>
</ul>
<p>最后总结一下策略梯度算法：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006125705.png" alt="img"></p>
]]></content>
      <categories>
        <category>MARL</category>
      </categories>
      <tags>
        <tag>MARL</tag>
      </tags>
  </entry>
  <entry>
    <title>Value Function Approximation</title>
    <url>/MARL/MADRL/Value%20Function%20Approximation/</url>
    <content><![CDATA[<h1 id="为什么需要值函数近似？"><a href="#为什么需要值函数近似？" class="headerlink" title="为什么需要值函数近似？"></a>为什么需要值函数近似？</h1><p>之前我们提到过各种计算值函数的方法，比如对于 MDP 已知的问题可以使用 Bellman 期望方程求得值函数；对于 MDP 未知的情况，可以通过 MC 以及 TD 方法来获得值函数，为什么需要再进行值函数近似呢？</p>
<p>其实到目前为止，我们介绍的值函数计算方法都是通过查表的方式获取的：</p>
<ul>
<li>表中每一个状态 s  均对应一个 V(s) </li>
<li>或者每一个状态-动作 <s,a ></li>
</ul>
<p>但是对于大型 MDP 问题，上述方法会遇到瓶颈：</p>
<ul>
<li>太多的 MDP 状态、动作需要存储</li>
<li>单独计算每一个状态的价值都非常的耗时</li>
</ul>
<p>因此我们需要有一种能够适用于解决大型 MDP 问题的通用方法，这就是本文介绍的值函数近似方法。即：</p>
<script type="math/tex; mode=display">
\hat{v}(s, \mathbf{w}) \approx v_{\pi}(s) \\
\text{or } \hat{q}(s, a, \mathbf{w}) \approx q_{\pi}(s, a)</script><p>那么为什么值函数近似的方法可以求解大型 MDP 问题？</p>
<blockquote>
<p>对于大型 MDP 问题而言，我们可以近似认为其所有的状态和动作都被采样和计算是不现实的，那么我们一旦获取了近似的值函数，我们就可以对于那些在历史经验或者采样中没有出现过的状态和动作进行泛化（generalize）。</p>
</blockquote>
<p>进行值函数近似的训练方法有很多，比如：</p>
<ul>
<li>线性回归</li>
<li>神经网络</li>
<li>决策树</li>
<li>…</li>
</ul>
<p>此外，针对 MDP 问题的特点，训练函数必须可以适用于非静态、非独立同分布（non-i.i.d）的数据。</p>
<h1 id="增量方法"><a href="#增量方法" class="headerlink" title="增量方法"></a>增量方法</h1><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><h3 id="通过随机梯度下降进行值函数近似"><a href="#通过随机梯度下降进行值函数近似" class="headerlink" title="通过随机梯度下降进行值函数近似"></a>通过随机梯度下降进行值函数近似</h3><p>我们优化的目标函数是找到一组参数 w  来最小化最小平方误差（MSE），即：</p>
<script type="math/tex; mode=display">
J(\mathbf{w}) = E_{\pi}[(v_{\pi}(S) - \hat{v}(S, \mathbf{w}))^2]</script><p>通过梯度下降方法来寻优：</p>
<script type="math/tex; mode=display">
\begin{align}
\Delta\mathbf{w}
&=-\frac{1}{2}\alpha\triangledown_{\mathbf{w}}J(\mathbf{w})\\
&=\alpha E_{\pi}\Bigl[\Bigl(v_{\pi}(S) - \hat{v}(S, \mathbf{w})\Bigr)\triangledown_{\mathbf{w}}J(\mathbf{w})\Bigr]
\end{align}</script><p>对于随机梯度下降（Stochastic Gradient Descent，SGD），对应的梯度：</p>
<script type="math/tex; mode=display">
\Delta\mathbf{w} = \alpha\underbrace{\Bigl(v_{\pi}(S) - \hat{v}(S, \mathbf{w})\Bigr)}_{\text{error}}\underbrace{\triangledown_{\mathbf{w}}\hat{v}(S, \mathbf{w})}_{\text{gradient}}</script><h2 id="值函数近似"><a href="#值函数近似" class="headerlink" title="值函数近似"></a>值函数近似</h2><p>上述公式中需要真实的策略价值函数 vπ(S)vπ(S) 作为学习的目标（supervisor），但是在RL中没有真实的策略价值函数，只有rewards。在实际应用中，我们用v~π~(S)来代替target vπ(S)：</p>
<ul>
<li>对于MC，target 为 return G~t ~：</li>
</ul>
<script type="math/tex; mode=display">
\Delta\mathbf{w}=\alpha\Bigl(G_t - \hat{v}(S_t, \mathbf{w})\Bigr)\triangledown_{\mathbf{w}}\hat{v}(S_t, \mathbf{w})</script><ul>
<li>对于TD(0)，target 为TD target $_{t+1}+\gamma\hat{v}(S_{t+1}, \mathbf{w})$：</li>
</ul>
<script type="math/tex; mode=display">
\Delta\mathbf{w}=\alpha\Bigl(R_{t+1} + \gamma\hat{v}(S_{t+1}, \mathbf{w})- \hat{v}(S_t, \mathbf{w})\Bigr)\triangledown_{\mathbf{w}}\hat{v}(S_t, \mathbf{w})</script><ul>
<li>对于TD(λ)，target 为 TD λ-return  $G_t^{\lambda}$：</li>
</ul>
<script type="math/tex; mode=display">
\Delta\mathbf{w}=\alpha\Bigl(G_t^{\lambda}- \hat{v}(S_t, \mathbf{w})\Bigr)\triangledown_{\mathbf{w}}\hat{v}(S_t, \mathbf{w})</script><p>在获取了值函数近似后就可以进行控制了，具体示意图如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006115744.png" alt=""></p>
<h2 id="动作价值函数近似"><a href="#动作价值函数近似" class="headerlink" title="动作价值函数近似"></a>动作价值函数近似</h2><p>动作价值函数近似：</p>
<script type="math/tex; mode=display">
\hat{q}(S, A, \mathbf{w})\approx q_{\pi}(S, A)</script><p>优化目标：最小化MSE</p>
<script type="math/tex; mode=display">
J(\mathbf{w}) = E_{\pi}[(q_{\pi}(S, A) - \hat{q}(S, A, \mathbf{w}))^2]</script><p>使用SGD寻优：</p>
<script type="math/tex; mode=display">
\begin{align} \Delta\mathbf{w} &=-\frac{1}{2}\alpha\triangledown_{\mathbf{w}}J(\mathbf{w})\\ &=\alpha\Bigl(q_{\pi}(S, A)-\hat{q}_{\pi}(S, A, \mathbf{w})\Bigr) \triangledown_{\mathbf{w}}\hat{q}_{\pi}(S, A, \mathbf{w}) \end{align}</script><h1 id="批量方法"><a href="#批量方法" class="headerlink" title="批量方法"></a>批量方法</h1><p>随机梯度下降SGD简单，但是批量的方法可以根据agent的经验来更好的拟合价值函数。</p>
<h2 id="值函数近似-1"><a href="#值函数近似-1" class="headerlink" title="值函数近似"></a>值函数近似</h2><p>优化目标：批量方法解决的问题同样是  $\hat{v}(s, \mathbf{w})\approx v_{\pi}(s)$</p>
<p>经验集合  D 包含了一系列的 <state, value> pair：</p>
<script type="math/tex; mode=display">
D=\{<s_1, v_1^{\pi}>, <s_2, v_2^{\pi}>, ..., <s_T, v_T^{\pi}>\}</script><p>根据最小化平方误差之和来拟合 $\hat{v}(s, \mathbf{w})$和 v~π~(s) ，即：</p>
<script type="math/tex; mode=display">
\begin{align}
LS(w) 
&= \sum_{t=1}^{T}(v_{t}^{\pi}-\hat{v}(s_t, \mathbf{w}))^2\\
&= E_{D}[(v^{\pi}-\hat{v}(s, \mathbf{w}))^2]
\end{align}</script><p>经验回放（Experience Replay）：</p>
<blockquote>
<p>给定经验集合：</p>
<script type="math/tex; mode=display">
D=\{<s_1, v_1^{\pi}>, <s_2, v_2^{\pi}>, ..., <s_T, v_T^{\pi}>\}</script><p>Repeat：</p>
<ol>
<li>从经验集合中采样状态和价值：<s,v^π^>∼D </li>
<li>使用SGD进行更新：$\Delta\mathbf{w}=\alpha\Bigl(v^{\pi}-\hat{v}(s, \mathbf{w})\Bigr)\triangledown_{\mathbf{w}}\hat{v}(s, \mathbf{w})$<br>通过上述经验回放，获得最小化平方误差的参数值：</li>
</ol>
<p>$\mathbf{w}^{\pi}=\arg\min_{\mathbf{w}}LS(\mathbf{w})$</p>
</blockquote>
<p>我们经常听到的 DQN 算法就使用了经验回放的手段，这个后续会在《深度强化学习》中整理。</p>
<p>通过上述经验回放和不断的迭代可以获取最小平方误差的参数值，然后就可以通过 greedy 的策略进行策略提升，具体如下图所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006120213.png" alt=""></p>
<h2 id="动作价值函数近似-1"><a href="#动作价值函数近似-1" class="headerlink" title="动作价值函数近似"></a>动作价值函数近似</h2><p>同样的套路：</p>
<ul>
<li>优化目标： $\hat{q}(s, a, \mathbf{w})\approx q_{\pi}(s, a)$</li>
<li>采取包含 <state, action, value> 的经验集合 D </li>
<li>通过最小化平方误差来拟合</li>
</ul>
<p>对于控制环节，我们采取与Q-Learning一样的思路：</p>
<ul>
<li>利用之前策略的经验</li>
<li>但是考虑另一个后继动作 $A’=\pi_{\text{new}}(S_{t+1})$</li>
<li>朝着另一个后继动作的方向去更新 $\hat{q}(S_t, A_t, \mathbf{w})$，即</li>
</ul>
<p>$\delta = R_{t+1} + \gamma\hat{q}(S_{t+1}, \pi{S_{t+1}, \mathbf{\pi}}) - \hat{q}(S_t, A_t, \mathbf{w})$</p>
<ul>
<li>梯度：线性拟合情况，$\Delta\mathbf{w}=\alpha\delta\mathbf{x}(S_t, A_t)$</li>
</ul>
]]></content>
      <categories>
        <category>MARL</category>
      </categories>
      <tags>
        <tag>MARL</tag>
      </tags>
  </entry>
  <entry>
    <title>Cross-entrpy Method</title>
    <url>/MARL/MADRL/Cross-entroy%20Method/</url>
    <content><![CDATA[<h3 id="CEM-amp-amp-RL"><a href="#CEM-amp-amp-RL" class="headerlink" title="CEM &amp;&amp; RL"></a>CEM &amp;&amp; RL</h3><p>注：以下内容引自博文《进化策略优化算法CEM(Cross Entropy Method)》[1]。</p>
<p>CEM也可以用来求解马尔可夫决策过程，也就是强化学习问题。我们知道，强化学习也是一种动态规划过程，在某个状态下选择某个动作就像在某个节点选择路径一样，整个过程就是一个从初始状态到末状态的路径规划问题，只不过我们希望得到一条能最大化收益的路径。在这种考虑下，就可以用CEM建模了，我们让一条完整的路径成为一个样本x=(s0,a0,s1,a1,…,sn,an)，路径获得的总收益为S(x)=∑Ni=0r(si,ai)，目标是最大化这个S(x)，那么如何采样出这些样本呢？我们可以构建一个pp矩阵：矩阵行表示状态，列表示动作，如pij表示在状态si下执行aj动作的概率，我们通过对这个pp矩阵进行多次采样就可以获得多个样本，然后选出S(x)较高的样本用来更新pp矩阵，不断迭代，最终找到最优p^矩阵。</p>
<p>这是一种类似于策略迭代(policy iteration)的强化学习方法：通过p矩阵找到在每一步状态下各个动作的概率来形成决策策略，但参数更新并没有用到梯度。从另外一个角度，你也可以认为这是一种值迭代(value iteration)的强化学习方法，此时p矩阵就是经典Q-learning中的Q矩阵，只不过Q矩阵中第i行第j列元素表示的是状态si下动作aj的未来收益的期望，基于贝尔曼方程(Bellman equation)来更新Q值；而p矩阵表示的是概率值，通过交叉墒来更新。</p>
<p>[1] <a href="https://blog.csdn.net/ppp8300885/article/details/80567682">进化策略优化算法CEM(Cross Entropy Method)</a></p>
]]></content>
      <categories>
        <category>MARL</category>
      </categories>
      <tags>
        <tag>MARL</tag>
      </tags>
  </entry>
  <entry>
    <title>Model-Free Prediction</title>
    <url>/MARL/MADRL/Model-Free%20Prediction/</url>
    <content><![CDATA[<p>上篇文章介绍了 Model-based 的通用方法——动态规划，本文内容介绍 Model-Free 情况下 Prediction 问题，即 “Estimate the value function of an unknown MDP”。</p>
<ul>
<li>Model-based：MDP已知，即转移矩阵和奖赏函数均已知</li>
<li>Model-Free：MDP未知</li>
</ul>
<h2 id="蒙特卡洛学习"><a href="#蒙特卡洛学习" class="headerlink" title="蒙特卡洛学习"></a>蒙特卡洛学习</h2><p>蒙特卡洛方法（Monte-Carlo Methods，简称MC）也叫做蒙特卡洛模拟，是指使用随机数（或更常见的伪随机数）来解决很多计算问题的方法。其实本质就是，通过尽可能随机的行为产生后验，然后通过后验来表征目标系统。</p>
<p>在Model-Free的情况下，MC在强化学习中的应用就是获取价值函数，其特点如下：</p>
<ul>
<li>MC 可以从完整的 episodes 中学习（no bootstrapping）</li>
<li>MC 以均值来计算价值，即 value = mean(return)</li>
<li>MC 只能适用于 episodic MDPs（有限MDPs）</li>
</ul>
<h3 id="First-Visit-蒙特卡洛策略评估"><a href="#First-Visit-蒙特卡洛策略评估" class="headerlink" title="First-Visit 蒙特卡洛策略评估"></a>First-Visit 蒙特卡洛策略评估</h3><p>First-Visit Monte-Carlo Policy Evaluation：</p>
<blockquote>
<p>评估状态 s  在给定策略 π  下的价值函数 v~π~(s))时，在一次 episode 中，状态 s  在时刻  t <strong>第一次</strong>被访问时，计数器 N(s)←N(s)+1 ，累计价值 S(s)←S(s)+Gt<br>当整个过程结束后，状态 s 的价值  $V(s) = \frac{S(s)}{N(s)}$根据大数定理（Law of Large Numbers）：$V(s) → v_{\pi}(s) \text{ as } N(s) → \infty$</p>
</blockquote>
<h3 id="Every-Visit-蒙特卡洛策略评估"><a href="#Every-Visit-蒙特卡洛策略评估" class="headerlink" title="Every-Visit 蒙特卡洛策略评估"></a>Every-Visit 蒙特卡洛策略评估</h3><p>Every-Visit Monte-Carlo Policy Evaluation：</p>
<blockquote>
<p>评估状态 s  在给定策略 π 下的价值函数 v~π(~s)时，在一次 episode 中，状态 s 在时刻 t <strong>每次</strong>被访问时，计数器 N(s)←N(s)+1，累计价值 S(s)←S(s)+Gt<br>当整个过程结束后，状态 s 的价值 V(s)=S(s)/N(s)<br>根据大数定理（Law of Large Numbers）：V(s)→v~π~(s) as N(s)→∞</p>
</blockquote>
<h3 id="Incremental-Monte-Carlo"><a href="#Incremental-Monte-Carlo" class="headerlink" title="Incremental Monte-Carlo"></a>Incremental Monte-Carlo</h3><p>增量式求平均：<br>The mean μ1,μ2,… of a sequence x1,x2,… . can be computed incrementally：</p>
<script type="math/tex; mode=display">
\begin{align}
\mu_k 
&= \frac{1}{k}\sum_{j=1}^{k}x_j\\
&= \frac{1}{k}\Bigl(x_k+\sum_{j=1}^{k-1}x_j \Bigr)\\
&= \frac{1}{k}(x_k + (k-1)\mu_{k-1})\\
&= \mu_{k-1} + \frac{1}{k}(x_k - \mu_{k-1})
\end{align}</script><p>根据上式我们可以得出增量式进行MC更新的公式：<br>每次 episode 结束后，增量式更新 V(s) ，对于每个状态 St ，其对应的 return 为 Gt :</p>
<p> $N(S_t) ← N(S_t) + 1 \\ V(S_t) ← V(S_t) + \frac{1}{N(S_t)}(G_t - V(S_t))$</p>
<p>在非静态问题中，更新公式形式可以改为如下：</p>
<p>$V(S_t) ← V(S_t) + \alpha (G_t - V(S_t))$</p>
<h2 id="时序差分学习"><a href="#时序差分学习" class="headerlink" title="时序差分学习"></a>时序差分学习</h2><p>时序差分方法（Temporal-Difference Methods，简称TD）特点：</p>
<ul>
<li>TD 可以通过 bootstrapping 从非完整的 episodes 中学习</li>
<li>TD updates a guess towards a guess</li>
</ul>
<h3 id="TD-λ"><a href="#TD-λ" class="headerlink" title="TD(λ)"></a>TD(λ)</h3><p>下图为 TD target 在不同 n 下的示意图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005224847.png" alt=""></p>
<p>从上图可以看出，当 n 达到终止时，即为一个episode，此时对应的方法为MC，因此从这个角度看，MC属于TD的特殊情况。</p>
<h3 id="n-step-Return"><a href="#n-step-Return" class="headerlink" title="n-step Return"></a>n-step Return</h3><p>n-step returns 可以表示如下：<br>n=1 时：$G_{t}^{(1)} = R_{t+1} + \gamma V(S_{t+1})$<br>n=2 时：$G_{t}^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 V(S_{t+2})$<br>…<br>n=∞ 时：$G_{t}^{\infty} = R_{t+1} + \gamma R_{t+2} + … + \gamma^{T-1} R_T)$<br>因此，n-step return $G_{t}^{(n)} = R_{t+1} + \gamma R_{t+2} + … + \gamma^{n}V(S_{t+n})$</p>
<p>n-step TD 更新公式：</p>
<p>$V(S_t) ← V(S_t) + \alpha (G_t^{(n)} - V(S_t))$</p>
<h3 id="Forward-View-of-TD-λ"><a href="#Forward-View-of-TD-λ" class="headerlink" title="Forward View of TD(λ)"></a>Forward View of TD(λ)</h3><p>我们能否把所有的 n-step return 组合起来？答案肯定是可以，组合后的return被称为是λ-return，其中λ是为了组合不同的n-step returns引入的权重因子。</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005225422.png" alt=""></p>
<p>λ-return:</p>
<p>$G_t^{\lambda} = (1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}G_t^{(n)}$</p>
<p>Forward-view TD(λλ)：</p>
<p>$V(S_t) ← V(S_t) + \alpha\Bigl(G_t^{\lambda} - V(S_t)\Bigr)$</p>
<p>TD(λ)对应的权重公式为 (1−λ)λ^n−1^，分布图如下所示：</p>
<p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181029093216246-346895182.png" alt=""></p>
<p>Forward-view TD(λ)的特点：</p>
<ul>
<li>Update value function towards the λ-return</li>
<li>Forward-view looks into the future to compute GλtGtλ</li>
<li>Like MC, can only be computed from complete episodes</li>
</ul>
<h3 id="Backward-View-TD-λ"><a href="#Backward-View-TD-λ" class="headerlink" title="Backward View TD(λ)"></a>Backward View TD(λ)</h3><ul>
<li>Forward view provides theory</li>
<li>Backward view provides mechanism</li>
<li>Update online, every step, from incomplete sequences</li>
</ul>
<p>带有资格迹的TD(λλ)：</p>
<script type="math/tex; mode=display">
\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\\
V(s) ← V(s) + \alpha \delta_t E_t(s)</script><p>其中δt为TD-error，Et(s)为资格迹。</p>
<p><strong>资格迹(Eligibility Traces)</strong></p>
<blockquote>
<p>资格迹本质就是对于频率高的，最近的状态赋予更高的信任（credit）/ 权重。</p>
</blockquote>
<p>下图是对资格迹的一个描述：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005230115.png" alt=""></p>
<p>关于TD(λ)有一个结论：</p>
<blockquote>
<p>The sum of offline updates is identical for forward-view and backward-view TD(λ).</p>
</blockquote>
<p>这一块的内容不再深入介绍了，感兴趣的可以看Sutton的书和David的教程。</p>
<h2 id="蒙特卡洛学习-vs-时序差分学习"><a href="#蒙特卡洛学习-vs-时序差分学习" class="headerlink" title="蒙特卡洛学习 vs. 时序差分学习"></a>蒙特卡洛学习 vs. 时序差分学习</h2><h3 id="MC与TD异同点"><a href="#MC与TD异同点" class="headerlink" title="MC与TD异同点"></a>MC与TD异同点</h3><p>相同点：都是从经验中在线的学习给定策略 π 的价值函数 v~π~</p>
<p>不同点：</p>
<ul>
<li>Incremental every-visit Monte-Carlo：朝着真实的 return G~t~  更新 V(S~t~) </li>
</ul>
<p>$V(S_t) ← V(S_t) + \alpha (\textcolor{Red}{G_t}   - V(S_t))$</p>
<ul>
<li><p>Simplest temporal-difference learning algorithm: TD(0)</p>
<ul>
<li>朝着已预估的 return  $\color{Red}{R_{t+1} + \gamma V(S_{t+1})}$ 更新 V(S~t~) </li>
</ul>
<script type="math/tex; mode=display">
V(S_t) ← V(S_t) + \alpha (\textcolor{Red}{R_{t+1} + \gamma V(S_{t+1})} - V(S_t))</script><ul>
<li>$\color{Red}{R_{t+1} + \gamma V(S_{t+1})}$称为是 TD target</li>
<li>$\color{Red}{R_{t+1} + \gamma V(S_{t+1})}−V(S_{t})$ 称为是 TD error</li>
</ul>
</li>
</ul>
<p>下图以 Drive Home 举例说明两者的不同，MC 只能在回家后才能改变对回家时间的预判，而 TD 在每一步中不断根据实际情况来调整自己的预判。</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005231045.png" alt=""></p>
<h2 id="MC与TD优缺点"><a href="#MC与TD优缺点" class="headerlink" title="MC与TD优缺点"></a>MC与TD优缺点</h2><h3 id="学习方式"><a href="#学习方式" class="headerlink" title="学习方式"></a>学习方式</h3><ul>
<li>TD 可以在知道最后结果之前学习（如上图举例）<ul>
<li>TD can learn online after every step</li>
<li>MC must wait until end of episode before return is known</li>
</ul>
</li>
<li>TD 可以在不存在最后结果的情况下学习（比如无限/连续MDPs）<ul>
<li>TD can learn from incomplete sequences</li>
<li>MC can only learn from complete sequences</li>
<li>TD works in continuing (non-terminating) environments</li>
<li>MC only works for episodic (terminating) environments</li>
</ul>
</li>
</ul>
<h3 id="方差与偏差"><a href="#方差与偏差" class="headerlink" title="方差与偏差"></a>方差与偏差</h3><ul>
<li>MC has high variance, zero bias（高方差，零偏差）<ul>
<li>Good convergence properties</li>
<li>Not very sensitive to initial value</li>
<li>Very simple to understand and use</li>
</ul>
</li>
<li>TD has low variance, some bias（低方差，存在一定偏差）<ul>
<li>Usually more efficient than MC</li>
<li>TD(0) converges to vπ(s)vπ(s)</li>
<li>More sensitive to initial value</li>
</ul>
</li>
</ul>
<p>关于 MC 和 TD 中方差和偏差问题的解释：</p>
<blockquote>
<ul>
<li>MC 更新基于真实的 return  $G_t = R_{t+1} + \gamma R_{t+2} + … + \gamma^{T-1}R_{T}$是 v~π~(St)  的无偏估计。</li>
<li>真实的TD target $R_{t+1} + \gamma v_{\pi}(S_{t+1})$ 也是 vπ(St) 的无偏估计。但是实际更新时用的 TD target $R_{t+1} + \gamma V(S_{t+1})$ 是 vπ(St)  的有偏估计。</li>
<li>TD target 具有更低的偏差：<ul>
<li>Return 每次模拟依赖于许多的随机动作、转移概率以及回报</li>
<li>TD target 每次只依赖一次随机动作、转移概率以及回报</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="马尔可夫性"><a href="#马尔可夫性" class="headerlink" title="马尔可夫性"></a>马尔可夫性</h3><ul>
<li>TD exploits Markov property<ul>
<li>Usually more efficient in Markov environments</li>
</ul>
</li>
<li>MC does not exploit Markov property<ul>
<li>Usually more effective in non-Markov environments</li>
</ul>
</li>
</ul>
<h2 id="DP、MC以及TD-0"><a href="#DP、MC以及TD-0" class="headerlink" title="DP、MC以及TD(0)"></a>DP、MC以及TD(0)</h2><p>首先我们从 backup tree 上去直观地认识三者的不同。</p>
<ul>
<li>DP backup tree：Full-Width step（完整的step）</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005231417.png" alt=""></p>
<ul>
<li>MC backup tree：完整的episode</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005231438.png" alt=""></p>
<ul>
<li>TD(0) backup tree：单个step</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005231514.png" alt=""></p>
<h2 id="Bootstrapping-vs-Sampling"><a href="#Bootstrapping-vs-Sampling" class="headerlink" title="Bootstrapping vs. Sampling"></a>Bootstrapping vs. Sampling</h2><p>Bootstrapping：基于已预测的值进行更新</p>
<ul>
<li>DP bootstraps</li>
<li>MC does not bootstrap</li>
<li>TD bootstraps</li>
</ul>
<p>Sampling：基于采样的期望来更新</p>
<ul>
<li>DP does not sample（model-based methods don’t need sample）</li>
<li>MC samples（model-free methods need sample）</li>
<li>TD samples（model-free methods need sample）</li>
</ul>
<p>下图从宏观的视角显示了 RL 的几种基本方法的区别：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005231530.png" alt=""></p>
]]></content>
      <categories>
        <category>MARL</category>
      </categories>
      <tags>
        <tag>MARL</tag>
      </tags>
  </entry>
  <entry>
    <title>Model-Free Control</title>
    <url>/MARL/MADRL/Model-Free%20Control/</url>
    <content><![CDATA[<p>上篇总结了 Model-Free Predict 问题及方法，本文内容介绍 Model-Free Control 方法，即 “Optimise the value function of an unknown MDP”。</p>
<p>在这里说明下，Model-Free Predict/Control 不仅适用于 Model-Free 的情况，其同样适用于 MDP 已知的问题：</p>
<ul>
<li>MDP model is unknown, but experience can be sampled.</li>
<li>MDP model is known, but is too big to use, except by samples.</li>
</ul>
<p>在正式介绍 Model-Free Control 方法之前，我们先介绍下 On-policy Learning 及 Off-policy Learning。</p>
<h2 id="On-policy-Learning-vs-Off-policy-Learning"><a href="#On-policy-Learning-vs-Off-policy-Learning" class="headerlink" title="On-policy Learning vs. Off-policy Learning"></a>On-policy Learning vs. Off-policy Learning</h2><p>On-policy Learning：</p>
<ul>
<li>“Learn on the job”</li>
<li>Learn about policy π from experience sampled from π（即采样的策略与学习的策略一致）</li>
</ul>
<p>Off-policy Learning：</p>
<ul>
<li>“Look over someone’s shoulder”</li>
<li>Learn about policy π from experience sampled from μ（即采样的策略与学习的策略不一致）</li>
</ul>
<h2 id="On-Policy-Monte-Carlo-Learning"><a href="#On-Policy-Monte-Carlo-Learning" class="headerlink" title="On-Policy Monte-Carlo Learning"></a>On-Policy Monte-Carlo Learning</h2><h3 id="Generalized-Policy-Iteration"><a href="#Generalized-Policy-Iteration" class="headerlink" title="Generalized Policy Iteration"></a>Generalized Policy Iteration</h3><p>具体的 Control 方法，在《动态规划》一文中我们提到了 Model-based 下的广义策略迭代 GPI 框架，那在 Model-Free 情况下是否同样适用呢？<br>如下图为 Model-based 下的广义策略迭代 GPI 框架，主要分两部分：策略评估及基于 Greedy 策略的策略提升。</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006105433.png" alt=""></p>
<h4 id="Model-Free-策略评估"><a href="#Model-Free-策略评估" class="headerlink" title="Model-Free 策略评估"></a>Model-Free 策略评估</h4><p>在《Model-Free Predict》中我们分别介绍了两种 Model-Free 的策略评估方法：MC 和 TD。我们先讨论使用 MC 情况下的 Model-Free 策略评估。<br>如上图GPI框架所示：</p>
<ul>
<li>基于 V(s) 的贪婪策略提升需要 MDP 已知：</li>
</ul>
<script type="math/tex; mode=display">
\pi'(s) = \arg\max_{a\in A}\Bigl(R_{s}^{a}+P_{ss'}^{a}V(s')\Bigr)</script><ul>
<li>基于 Q(s,a) 的贪婪策略提升不需要 MDP 已知，即 Model-Free：</li>
</ul>
<script type="math/tex; mode=display">
\pi'(s) = \arg\max_{a\in A}Q(s, a)</script><p>因此 Model-Free 下需要对 Q(s,a) 策略评估，整个GPI策略迭代也要基于 Q(s,a) </p>
<h4 id="Model-Free-策略提升"><a href="#Model-Free-策略提升" class="headerlink" title="Model-Free 策略提升"></a>Model-Free 策略提升</h4><p>确定了策略评估的对象，那接下来要考虑的就是如何基于策略评估的结果 Q(s,a) 进行策略提升。<br>由于 Model-Free 的策略评估基于对经验的 samples（即评估的 q(s,a)  存在 bias），因此我们在这里不采用纯粹的 greedy 策略，防止因为策略评估的偏差导致整个策略迭代进入局部最优，而是采用具有 explore 功能的  ϵ-greedy 算法：</p>
<script type="math/tex; mode=display">
\pi(a|s) = 
\begin{cases}
&\frac{\epsilon}{m} + 1 - \epsilon, &\text{if } a^*=\arg\max_{a\in A}Q(s, a)\\
&\frac{\epsilon}{m}, &\text{otherwise}
\end{cases}</script><p>因此，我们确定了 Model-Free 下的 Monto-Carlo Control：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006105953.png" alt=""></p>
<h3 id="GLIE"><a href="#GLIE" class="headerlink" title="GLIE"></a>GLIE</h3><p>先直接贴下David的课件，GLIE 介绍如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006110036.png" alt=""></p>
<p>对于 ϵ-greedy 算法而言，如果 ϵ 随着迭代次数逐步减为0，那么  ϵ-greedy 是 GLIE，即：</p>
<script type="math/tex; mode=display">
\epsilon_{k} = \frac{1}{k}</script><h4 id="GLIE-Monto-Carlo-Control"><a href="#GLIE-Monto-Carlo-Control" class="headerlink" title="GLIE Monto-Carlo Control"></a>GLIE Monto-Carlo Control</h4><ul>
<li>对于 episode 中的每个状态 S~t~  和动作 A~t~</li>
</ul>
<script type="math/tex; mode=display">
N(S_t, A_t) ← N(S_t, A_t) + 1 \\
Q(S_t, A_t) ← Q(S_t, A_t) + \frac{1}{N(S_t, A_t)}(G_t - Q(S_t, A_t))</script><ul>
<li>基于新的动作价值函数提升策略：</li>
</ul>
<script type="math/tex; mode=display">
\epsilon ← \frac{1}{k}\\
\pi ← \epsilon\text{-greedy}(Q)</script><p>定理：GLIE Monto-Carlo Control 收敛到最优的动作价值函数，即：</p>
<script type="math/tex; mode=display">
Q(s, a) → q_*(s, a)</script><h2 id="On-Policy-Temporal-Difference-Learning"><a href="#On-Policy-Temporal-Difference-Learning" class="headerlink" title="On-Policy Temporal-Difference Learning"></a>On-Policy Temporal-Difference Learning</h2><h3 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h3><p>我们之前总结过 TD 相对 MC 的优势：</p>
<ul>
<li>低方差</li>
<li>Online</li>
<li>非完整序列</li>
</ul>
<p>那么一个很自然的想法就是在整个控制闭环中用 TD 代替 MC：</p>
<ul>
<li>使用 TD 来计算 Q(S,A) </li>
<li>仍然使用  ϵ-greedy 策略提升</li>
<li>每一个 step 进行更新</li>
</ul>
<p>通过上述改变就使得 On-Policy 的蒙特卡洛方法变成了著名的 Sarsa。</p>
<ul>
<li>更新动作价值函数</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006110844.png" alt=""></p>
<ul>
<li>Control</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006110937.png" alt=""></p>
<p>Sarsa算法的伪代码如下：<br><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006111038.png" alt="img"></p>
<h3 id="Sarsa-λ"><a href="#Sarsa-λ" class="headerlink" title="Sarsa(λ)"></a>Sarsa(λ)</h3><p>n-step Sarsa returns 可以表示如下：<br>n=1 时：$q_{t}^{(1)} = R_{t+1} + \gamma Q(S_{t+1})$<br>n=2 时：$q_{t}^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 Q(S_{t+2})$<br>…<br>n=∞ 时：$q_{t}^{\infty} = R_{t+1} + \gamma R_{t+2} + … + \gamma^{T-1} R_T)$<br>因此，n-step return $q_{t}^{(n)} = R_{t+1} + \gamma R_{t+2} + … + \gamma^{n}Q(S_{t+n})$</p>
<p>n-step Sarse 更新公式：</p>
<p>$Q(S_t,A_t) ← Q(S_t,A_t) + \alpha (q_t^{(n)} - Q(S_t,A_t))$</p>
<p> 具体的 Sarsa(λ) 算法伪代码如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006111817.png" alt="img"></p>
<p>其中 E(s,a)   为资格迹。</p>
<p>下图为 Sarsa(λ) 用于 Gridworld 例子的示意图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006112106.png" alt=""></p>
<h2 id="Off-Policy-Learning"><a href="#Off-Policy-Learning" class="headerlink" title="Off-Policy Learning"></a>Off-Policy Learning</h2><p>Off-Policy Learning 的特点是评估目标策略 π(a|s) 来计算 v~π~(s) 或者 q~π~(s,a) 但是跟随行为策略 {S~1~,A~1~,R~2~,…,S~T~}∼μ(a|s) </p>
<p>Off-Policy Learning 有什么意义？</p>
<ul>
<li>Learn from observing humans or other agents</li>
<li>Re-use experience generated from old policies π~1~,π~2~,…,π~t−1~ </li>
<li>Learn about optimal policy while following exploratory policy</li>
<li>Learn about multiple policies while following one policy</li>
</ul>
<h3 id="重要性采样"><a href="#重要性采样" class="headerlink" title="重要性采样"></a>重要性采样</h3><p>重要性采样的目的是：Estimate the expectation of a different distribution。</p>
<script type="math/tex; mode=display">
\begin{align}
E_{X\sim P}[f(X)]
&= \sum P(X)f(X)\\
&= \sum Q(X)\frac{P(X)}{Q(X)}f(X)\\
&= E_{X\sim Q}[\frac{P(X)}{Q(X)}f(X)]
\end{align}</script><h4 id="Off-Policy-MC-重要性采样"><a href="#Off-Policy-MC-重要性采样" class="headerlink" title="Off-Policy MC 重要性采样"></a>Off-Policy MC 重要性采样</h4><p>使用策略 π  产生的 return 来评估 μ ：</p>
<script type="math/tex; mode=display">
G_t^{\pi/\mu} = \frac{\pi(A_t|S_t)}{\mu(A_t|S_t)} \frac{\pi(A_{t+1}|S_{t+1})}{\mu(A_{t+1}|S_{t+1})}...\frac{\pi(A_T|S_T)}{\mu(A_T|S_T)}G_t</script><p>朝着正确的 return 方向去更新价值：</p>
<script type="math/tex; mode=display">
V(S_t) ← V(S_t) + \alpha\Bigl(\textcolor{Red}{G_t^{\pi/\mu}}-V(S_t)\Bigr)</script><p>需要注意两点：</p>
<ul>
<li>Cannot use if μ  is zero when π  is non-zero</li>
<li>重要性采样会显著性地提升方差</li>
</ul>
<h4 id="Off-Policy-TD-重要性采样"><a href="#Off-Policy-TD-重要性采样" class="headerlink" title="Off-Policy TD 重要性采样"></a>Off-Policy TD 重要性采样</h4><p>TD 是单步的，所以使用策略 π  产生的 TD targets 来评估 μ ：</p>
<script type="math/tex; mode=display">
V(S_t) ← V(S_t) + \alpha\Bigl(\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}(R_{t+1}+\gamma V(S_{t+1}))-V(S_t)\Bigr)</script><ul>
<li>方差比MC版本的重要性采样低很多</li>
</ul>
<h3 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h3><p>前面分别介绍了对价值函数 V(s)  进行 off-policy 学习，现在我们讨论如何对动作价值函数 Q(s,a)  进行 off-policy 学习：</p>
<ul>
<li><p>不需要重要性采样</p>
</li>
<li><p>使用行为策略选出下一步的动作：$A_{t+1}\sim\mu(·|S_t)$</p>
</li>
<li><p>但是仍需要考虑另一个后继动作：$A’\sim\pi(·|S_t)$</p>
</li>
<li><p>朝着另一个后继动作的价值更新 $Q(S_t, A_t)$</p>
</li>
</ul>
<script type="math/tex; mode=display">
Q(S_t, A_t) ← Q(S_t, A_t) + \alpha\Bigl(R_{t+1}+\gamma Q(S_{t+1}, A')-Q(S_t, A_t)\Bigr)</script><p>讨论完对动作价值函数的学习，我们接着看如何通过 Q-Learning 进行 Control：</p>
<ul>
<li>行为策略和目标策略均改进</li>
<li>目标策略 π  以greedy方式改进：</li>
</ul>
<script type="math/tex; mode=display">
\pi(S_t) = \arg\max_{a'}Q(S_{t+1}, a')</script><ul>
<li>行为策略 μ  以  ϵ-greedy 方式改进</li>
<li>Q-Learning target：</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
&R_{t+1}+\gamma Q(S_{t+1}, A')\\
=&R_{t+1}+\gamma Q\Bigl(S_{t+1}, \arg\max_{a'}Q(S_{t+1}, a')\Bigr)\\
=&R_{t+1}+\max_{a'}\gamma Q(S_{t+1}, a')
\end{align}</script><p>Q-Learning 的 backup tree 如下所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006114145.png" alt=""></p>
<p>关于 Q-Learning 的结论：</p>
<blockquote>
<p>Q-learning control converges to the optimal action-value function, Q(s,a)→q~∗~(s,a) </p>
</blockquote>
<p>Q-Learning 算法具体的伪代码如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006114205.png" alt=""></p>
<p>对比 Sarsa 与 Q-Learning 可以发现两个最重要的区别：</p>
<ul>
<li>TD target 公式不同</li>
<li>Q-Learning 中下一步的动作从行为策略中选出，而不是目标策略</li>
</ul>
<h2 id="DP-vs-TD"><a href="#DP-vs-TD" class="headerlink" title="DP vs. TD"></a>DP vs. TD</h2><p>两者的区别见下表：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006114507.png" alt="">  </p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211006114532.png" alt=""></p>
]]></content>
      <categories>
        <category>MARL</category>
      </categories>
      <tags>
        <tag>MARL</tag>
      </tags>
  </entry>
  <entry>
    <title>动态规划</title>
    <url>/MARL/MADRL/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    <content><![CDATA[<h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><p>动态规划（Dynamic Programming，简称DP）是一种通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法。</p>
<p>动态规划常常适用于具有如下性质的问题：</p>
<ul>
<li>具有最优子结构（Optimal substructure）<ul>
<li>Principle of optimality applies</li>
<li>Optimal solution can be decomposed into subproblems</li>
</ul>
</li>
<li>重叠子问题（Overlapping subproblems）<ul>
<li>Subproblems recur many times</li>
<li>Solutions can be cached and reused</li>
</ul>
</li>
</ul>
<p>动态规划方法所耗时间往往远少于朴素解法。</p>
<p>马尔可夫决策过程MDP满足上述两个性质：</p>
<ul>
<li>贝尔曼方程提供了递归分解的结构；</li>
<li>价值函数可以保存和重复使用递归时的结果。</li>
</ul>
<h2 id="使用动态规划解决MDP-MRP"><a href="#使用动态规划解决MDP-MRP" class="headerlink" title="使用动态规划解决MDP/MRP"></a>使用动态规划解决MDP/MRP</h2><p>动态规划需要满足MDP过程是已知的（model-based）。</p>
<ul>
<li>For Predict：<ul>
<li>Input：MDP  <S,A,P,R,$\gamma$> 和策略 π 或者是 MRP <S,P,R,$\gamma$> </li>
<li>Output：价值函数 v~π~</li>
</ul>
</li>
<li>For Control：<ul>
<li>Input：MDP <S,A,P,R,$\gamma$> </li>
<li>Output：最优价值函数 v∗  或者最优策略 π∗</li>
</ul>
</li>
</ul>
<h2 id="策略评估"><a href="#策略评估" class="headerlink" title="策略评估"></a>策略评估</h2><p>策略评估（Policy Evaluation）指的是计算给定策略的价值，解决的问题是 “How to evaluate a policy”。</p>
<p>策略评估的思路：迭代使用贝尔曼期望方程（关于 MDP 的贝尔曼期望方程形式见《马尔可夫决策过程》）。</p>
<p>策略评估过程如下图所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005220219.png" style="zoom: 67%;" /></p>
<script type="math/tex; mode=display">
v_{k+1} = \sum_{a\in A}\pi(a|s) \Bigl( R_{s}^a + \gamma\sum_{s'\in S}P_{ss'}^a v_{k}(s') \Bigr)</script><p>使用向量形式表示：</p>
<script type="math/tex; mode=display">
\mathbf{v^{k+1}} = \mathbf{R^{\pi}} + \gamma \mathbf{P^{\pi}v^{k}}</script><h2 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h2><p>策略迭代（Policy Iteration，简称PI）解决的问题是 “How to improve a policy”。</p>
<p>给定一个策略 π ：</p>
<ul>
<li>评估策略 π </li>
</ul>
<script type="math/tex; mode=display">
v_{\pi}(s) = E[R_{t+1} + \gamma R_{t+2} + ...| S_t = s]</script><ul>
<li>提升策略：通过采用贪婪方法来提升策略：<script type="math/tex; mode=display">
\pi ' = \text{greedy}(v_{\pi})</script></li>
</ul>
<p>可以证明，策略迭代不断进行总是能收敛到最优策略，即 π′=π∗</p>
<p>策略迭代可以使用下图来形式化的描述：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005221105.png" alt=""></p>
<h2 id="广义策略迭代"><a href="#广义策略迭代" class="headerlink" title="广义策略迭代"></a>广义策略迭代</h2><p>通过上述提到的策略评估我们不难发现，策略评估是一个不断迭代的过程：</p>
<script type="math/tex; mode=display">
v_{\pi}(s) = E[R_{t+1} + \gamma R_{t+2} + ...| S_t = s]</script><p>那么问题来了，Does policy evaluation need to converge to vπvπ?<br>我们是不是可以引入一个停止规则或者规定在迭代 kk 次后停止策略评估？<br>再进一步想，我们为什么不在每次策略评估的迭代过程中进行策略提升（等同于策略评估迭代1次后停止）？<br>注：这和后续要介绍的值迭代等价。</p>
<p>因此我们可以把上述策略迭代的过程一般化，即广义策略迭代（Generalised Policy Iteration，简称GPI）框架：</p>
<p> <img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005221407.png" alt=""></p>
<h1 id="值迭代"><a href="#值迭代" class="headerlink" title="值迭代"></a>值迭代</h1><p>介绍值迭代之前，我们先介绍下最优化原理。</p>
<h2 id="最优化原理"><a href="#最优化原理" class="headerlink" title="最优化原理"></a>最优化原理</h2><p>最优化原理（Principle of Optimality）定义：</p>
<blockquote>
<p>一个过程的最优决策具有这样的性质：即无论其初始状态和初始决策如何，其今后诸策略对以第一个决策所形成的状态作为初始状态的过程而言，必须构成最优策略。</p>
</blockquote>
<p>最优化原理如果用数学化一点的语言来描述的话就是：</p>
<blockquote>
<p>以状态 ss 为起始点，策略 π(a|s) 可以得到最优值   $v_{\pi}(s) = v_*(s)$当且仅当：</p>
<ul>
<li>任意状态 s′ 对于状态 s  均可达；</li>
<li>以状态 s′ 为起始点，策略 π 可以得到最优值$v_{\pi}(s’) = v_*(s’)$</li>
</ul>
</blockquote>
<p>根据最优化原理可知，如果我们得到了子问题的解 v∗(s′)v∗(s′)，那么以状态 ss 为起始点的最优解 v∗(s)v∗(s) 可以通过一步回退（one-step lookahead）就能获取：</p>
<script type="math/tex; mode=display">
v_*(s) ← \max_{a\in A}\Bigl(R_s^a + \gamma \sum_{s'\in S}P_{ss'}^{a}v_*(s') \Bigr)</script><p>也就是说，我们可以从最后开始向前回退从而得到最优解，值迭代就是基于上述思想进行迭代更新的。</p>
<h2 id="MDP值迭代"><a href="#MDP值迭代" class="headerlink" title="MDP值迭代"></a>MDP值迭代</h2><p>值迭代（Value Iteration，简称VI）解决的问题也是 “Find optimal policy ππ”。<br>但是不同于策略迭代使用贝尔曼期望方程的是，值迭代使用贝尔曼最优方程进行迭代提升。</p>
<p>值迭代与策略迭代不同的地方在于：</p>
<ul>
<li>Use Bellman optimal function, rather than Bellman expectation function</li>
<li>Unlike policy iteration, there is no explicit policy</li>
<li>Intermediate value functions may not correspond to any policy</li>
</ul>
<p>如下图所示：</p>
<p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181028144014755-573375074.png" alt=""></p>
<script type="math/tex; mode=display">v_{k+1}(s) = \max_{a\in A}\Bigl(R_s^a + \gamma\sum_{s'\in S}P_{ss'}^a v_k(s') \Bigr)</script><p>对应的向量表示为：</p>
<script type="math/tex; mode=display">
\mathbf{v}_{k+1} = \max_{a\in A}\mathbf{R}^a + \gamma \mathbf{P^av}^k</script><p>下图为三种方法的总结：</p>
<p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181028144022645-1891229163.png" alt=""></p>
<h2 id="动态规划扩展"><a href="#动态规划扩展" class="headerlink" title="动态规划扩展"></a>动态规划扩展</h2><h3 id="异步动态规划（Asynchronous-Dynamic-Programming）"><a href="#异步动态规划（Asynchronous-Dynamic-Programming）" class="headerlink" title="异步动态规划（Asynchronous Dynamic Programming）"></a>异步动态规划（Asynchronous Dynamic Programming）</h3><ul>
<li>In-place dynamic programming</li>
<li>Prioritised sweeping</li>
<li>Real-time dynamic programming</li>
</ul>
<h3 id="Full-Width-Backups-vs-Sample-Backups"><a href="#Full-Width-Backups-vs-Sample-Backups" class="headerlink" title="Full-Width Backups vs. Sample Backups"></a>Full-Width Backups vs. Sample Backups</h3><h4 id="Full-Width-Backups"><a href="#Full-Width-Backups" class="headerlink" title="Full-Width Backups"></a>Full-Width Backups</h4><ul>
<li>DP uses full-width backups（DP is model-based）<ul>
<li>Every successor state and action is considered</li>
<li>Using knowledge of the MDP transitions and reward function</li>
</ul>
</li>
<li>DP is effective for medium-sized problems (millions of states)</li>
<li>For large problems, DP suffers Bellman’s curse of dimensionality（维度灾难）</li>
</ul>
<blockquote>
<p>维度灾难：Number of states n=|S|  grows exponentially with number of state variables</p>
</blockquote>
<ul>
<li>Even one backup can be too expensive</li>
</ul>
<h4 id="Sample-Backups"><a href="#Sample-Backups" class="headerlink" title="Sample Backups"></a>Sample Backups</h4><p>后续将要讨论的时序差分方法</p>
<ul>
<li>Using sample rewards and sample transitions ⟨S,A,R,S’⟩ </li>
<li>Instead of reward function R and transition dynamics P</li>
<li>Advantages:<ul>
<li>Model-free: no advance knowledge of MDP required</li>
<li>Breaks the curse of dimensionality through sampling</li>
<li>Cost of backup is constant, independent of n=|S|</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>MARL</category>
      </categories>
      <tags>
        <tag>MARL</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习介绍</title>
    <url>/MARL/MADRL/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<h2 id="强化学习四元素"><a href="#强化学习四元素" class="headerlink" title="强化学习四元素"></a>强化学习四元素</h2><ul>
<li>策略（Policy）：环境的感知状态到行动的映射方式。</li>
<li>反馈（Reward）：环境对智能体行动的反馈。</li>
<li>价值函数（Value Function）：评估状态的价值函数，状态的价值即从当前状态开始，期望在未来获得的奖赏。</li>
<li>环境模型（Model）：模拟环境的行为</li>
</ul>
<h2 id="强化学习的特点"><a href="#强化学习的特点" class="headerlink" title="强化学习的特点"></a>强化学习的特点</h2><ul>
<li>起源于动物学习心理学的试错法（trial-and-error），因此符合行为心理学。</li>
<li>寻求探索（exploration）和采用（exploitation）之间的权衡：强化学习一面要采用（exploitation）已经发现的有效行动，另一方面也要探索（exploration）那些没有被认可的行动，已找到更好的解决方案。</li>
<li>考虑整个问题而不是子问题。</li>
<li>通用AI解决方案。</li>
</ul>
<h2 id="强化学习-vs-机器学习"><a href="#强化学习-vs-机器学习" class="headerlink" title="强化学习 vs. 机器学习"></a>强化学习 vs. 机器学习</h2><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005205458.png" alt="img"></p>
<p>强化学习与其他机器学习的不同：</p>
<ul>
<li>强化学习更加专注于在线规划，需要在探索（explore 未知领域）和采用（exploit 现有知识）之间找到平衡。</li>
<li>强化学习不需要监督者，只需要获取环境的反馈。</li>
<li>反馈是延迟的，不是立即生成的。</li>
<li>时间在强化学习中很重要，其数据为序列数据，并不满足独立同分布假设（i.i.d）。</li>
</ul>
<h2 id="强化学习-vs-监督学习"><a href="#强化学习-vs-监督学习" class="headerlink" title="强化学习 vs. 监督学习"></a>强化学习 vs. 监督学习</h2><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005205751.png" alt="img"></p>
<p>两者的目标都是学习一个model，而区别在于：</p>
<p>监督学习：</p>
<ul>
<li>Open loop</li>
<li>Learning from labeled data</li>
<li>Passive data</li>
</ul>
<p>强化学习：</p>
<ul>
<li>Closed loop</li>
<li>Learning from decayed reward</li>
<li>Explore environment</li>
</ul>
<h2 id="强化学习-vs-进化算法"><a href="#强化学习-vs-进化算法" class="headerlink" title="强化学习 vs. 进化算法"></a>强化学习 vs. 进化算法</h2><p>进化算法（Evolutionary Algorithms，简称EA）是通过生物进化优胜略汰，适者生存的启发而发展的一类算法，通过种群不断地迭代达到优化的目标。 </p>
<p>进化算法最大的优点在于整个优化过程是gradients-free的，其思想可以通过下图表示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005205858.gif" alt=""></p>
<p>RL和EA虽然都属于优化问题的求解框架，而且两者都需要大量的算力，但是两者有着本质上的区别。</p>
<p>Sutton在其强化学习介绍一书中也重点谈到了RL与EA的区别</p>
<ul>
<li>RL通过与环境交互来进行学习，而EA通过种群迭代来进行学习；</li>
<li>RL通过最大化累计回报来解决序列问题，而EAs通过最大化适应函数（Fitness Function）来寻求单步最优；</li>
<li><p>RL对于state过于依赖，而EA在agent不能准确感知环境的状态类问题上也能适用。</p>
<p>很多研究也尝试通过将EA和RL结合解决优化问题， </p>
</li>
</ul>
<h2 id="强化学习分类"><a href="#强化学习分类" class="headerlink" title="强化学习分类"></a>强化学习分类</h2><p>强化学习分类比较多样：</p>
<ul>
<li>按照环境是否已知可以分为Model-based &amp; Model-free；</li>
<li>按照学习方式可以分为On-Policy &amp; Off-Policy；</li>
<li>按照学习目标可以分为Value-based &amp; Policy-based。</li>
</ul>
<p>下图为根据环境是否已知进行细分的示意图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005210120.png" alt=""></p>
<h2 id="强化学习相关推荐资料"><a href="#强化学习相关推荐资料" class="headerlink" title="强化学习相关推荐资料"></a>强化学习相关推荐资料</h2><ul>
<li>Reinforcement Learning: An Introduction, Richard S. Sutton and Andrew G. Barto：介绍强化学习很全面的一本书籍，相关的电子书及源码见<a href="http://incompleteideas.net/book/the-book.html">这里</a>。</li>
<li>David Silver在UCL的强化学习视频教程：介绍强化学习的视频教程，基本与Sutton的书籍可以配套学习，Silver来自于Google Deepmind，视频和课件可以从<a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">Silver的主页</a>获取，中文字幕版视频YouTube链接点<a href="https://www.youtube.com/playlist?list=PLjSwXXbVlK6K2enbNmPGjnmB8QBRgCv5s">这里</a>。</li>
<li>强化学习在阿里的技术演进与业务创新：介绍强化学习在阿里巴巴的落地，可以拓展强化学习应用的业务思路，电子版见<a href="https://pan.baidu.com/s/1jMj1e5zt_g3R7zgWRJ5t2A">这里</a>，密码：yh48。</li>
<li>Tutorial: Deep Reinforcement Learning：同样来自于Sliver的一个课件，主要针对RL与DL的结合进行介绍，电子版见<a href="https://pan.baidu.com/s/1jLAHZXJqsWg8JVHDBcunvw">这里</a>，密码：9mrp。</li>
<li>莫烦PYTHON强化学习视频教程：可以通过简短的视频概括地了解强化学习相关内容，适合于入门的同学，视频见<a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/">这里</a>。</li>
<li>OpenAI Gym：Gym is a toolkit for developing and comparing reinforcement learning algorithms，Gym包含了很多的控制游戏（比如过山车、二级立杆、Atari游戏等），并提供了非常好的接口可以学习，链接见<a href="https://gym.openai.com/docs/">这里</a>。</li>
<li>Lil’Log：介绍DL和RL的一个优质博客，RL相关包括RL介绍、Policy Gradients算法介绍及Deep RL结合Tensorflow和Gym的源码实现，主页链接见<a href="https://lilianweng.github.io/lil-log/">这里</a>。</li>
</ul>
<p>转载自：<a href="https://www.cnblogs.com/maybe2030/p/9862353.html#_label0">[Reinforcement Learning] 强化学习介绍 - Poll的笔记 - 博客园 (cnblogs.com)</a></p>
]]></content>
      <categories>
        <category>MARL</category>
      </categories>
      <tags>
        <tag>MARL</tag>
      </tags>
  </entry>
  <entry>
    <title>马尔可夫决策过程</title>
    <url>/MARL/MADRL/%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="情节性任务-vs-连续任务"><a href="#情节性任务-vs-连续任务" class="headerlink" title="情节性任务 vs. 连续任务"></a>情节性任务 vs. 连续任务</h2><ul>
<li>情节性任务（Episodic Tasks），所有的任务可以被可以分解成一系列情节，可以看作为有限步骤的任务。</li>
<li>连续任务（Continuing Tasks），所有的任务不能分解，可以看作为无限步骤任务。</li>
</ul>
<h2 id="马尔可夫性"><a href="#马尔可夫性" class="headerlink" title="马尔可夫性"></a>马尔可夫性</h2><p>马尔可夫性：当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态。</p>
<p>马尔可夫过程即为具有马尔可夫性的过程，即过程的条件概率仅仅与系统的当前状态相关，而与它的过去历史或未来状态都是独立、不相关的。</p>
<h2 id="马尔可夫奖赏过程"><a href="#马尔可夫奖赏过程" class="headerlink" title="马尔可夫奖赏过程"></a>马尔可夫奖赏过程</h2><p>马尔可夫奖赏过程（Markov Reward Process，MRP）是带有奖赏值的马尔可夫过程，其可以用一个四元组表示 <S,P,R,$\gamma$> </p>
<ul>
<li>S 为有限的状态集合；</li>
<li>P 为状态转移矩阵，$P_{ss^{‘}} = P[S_{t+1} = s^{‘}|S_t = s]$</li>
<li>R 是奖赏函数；</li>
<li>$\gamma$为折扣因子（discount factor），其中 $\gamma$∈[0,1]</li>
</ul>
<h3 id="奖赏函数"><a href="#奖赏函数" class="headerlink" title="奖赏函数"></a>奖赏函数</h3><p>在 t 时刻的奖赏值 Gt :</p>
<script type="math/tex; mode=display">
G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}</script><h3 id="Why-Discount"><a href="#Why-Discount" class="headerlink" title="Why Discount"></a>Why Discount</h3><p>关于Return的计算为什么需要 $\gamma$ 折扣系数:</p>
<blockquote>
<ul>
<li>数学表达的方便</li>
<li>避免陷入无限循环</li>
<li>远期利益具有一定的不确定性</li>
<li>在金融学上，立即的回报相对于延迟的回报能够获得更多的利益</li>
<li>符合人类更看重眼前利益的特点</li>
</ul>
</blockquote>
<h3 id="价值函数"><a href="#价值函数" class="headerlink" title="价值函数"></a>价值函数</h3><p>状态 s 的长期价值函数表示为：</p>
<script type="math/tex; mode=display">
 v(s)=E[Gt|St=s]</script><h3 id="Bellman-Equation-for-MRPs"><a href="#Bellman-Equation-for-MRPs" class="headerlink" title="Bellman Equation for MRPs"></a>Bellman Equation for MRPs</h3><script type="math/tex; mode=display">
\begin{align}
v(s) 
&= E[G_t|S_t=s]\\
&= E[R_{t+1} + \gamma R_{t+2} + ... | S_t = s]\\
&= E[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} ... ) | S_t = s]\\
&= E[R_{t+1} + \gamma G_{t+1} | S_t = s]\\
&= E[R_{t+1} + \gamma v(s_{t+1}) | S_t = s]
\end{align}</script><p>下图为MRP的 backup tree 示意图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005211526.png" alt="img"></p>
<p>注：backup tree 中的白色圆圈代表状态，黑色圆点对应动作。</p>
<p>根据上图可以进一步得到：</p>
<script type="math/tex; mode=display">
v(s) = R_s + \gamma \sum_{s' \in S}P_{ss'}v(s')</script><h2 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h2><p>马尔可夫决策过程（Markov Decision Process，MDP）是带有决策的MRP，其可以由一个五元组构成 <S,A,P,R,$\gamma$> 。</p>
<ul>
<li>S 为有限的状态集合；</li>
<li>A 为有限的动作集合；</li>
<li>P 为状态转移矩阵，$P_{ss^{‘}}^{a} = P[S_{t+1} = s^{‘}|S_t = s,A_t=a]$</li>
<li>R 是奖赏函数；</li>
<li>$\gamma$ 为折扣因子（discount factor），其中 $\gamma$∈[0,1] ]</li>
</ul>
<p>我们讨论的MDP一般指有限（离散）马尔可夫决策过程。</p>
<h2 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h2><p>策略（Policy）是给定状态下的动作概率分布，即：</p>
<script type="math/tex; mode=display">
\pi(a|s) = P[A_t = a|S_t = a]</script><h2 id="状态价值函数-amp-最优状态价值函数"><a href="#状态价值函数-amp-最优状态价值函数" class="headerlink" title="状态价值函数 &amp; 最优状态价值函数"></a>状态价值函数 &amp; 最优状态价值函数</h2><p>给定策略 π 下状态 s 的状态价值函数（State-Value Function） $v_{\pi}(s)$</p>
<script type="math/tex; mode=display">
v_{\pi}(s) = E_{\pi}[G_t|S_t = s]</script><p>状态 s 的最优状态价值函数（The Optimal State-Value Function）v~∗~(s) </p>
<script type="math/tex; mode=display">
v_{*}(s) = \max_{\pi}v_{\pi}(s)</script><h2 id="动作价值函数-amp-最优动作价值函数"><a href="#动作价值函数-amp-最优动作价值函数" class="headerlink" title="动作价值函数 &amp; 最优动作价值函数"></a>动作价值函数 &amp; 最优动作价值函数</h2><p>给定策略 π，状态 s，采取动作 a 的动作价值函数（Action-Value Function）q~π~(s,a)</p>
<script type="math/tex; mode=display">
q_{\pi}(s, a) = E_{\pi}[G_t|S_t = s, A_t = a]</script><p>状态 s 下采取动作 a 的最优动作价值函数（The Optimal Action-Value Function）q∗(s,a):</p>
<script type="math/tex; mode=display">
q_{*}(s, a) = \max_{\pi}q_{\pi}(s, a)</script><h2 id="最优策略"><a href="#最优策略" class="headerlink" title="最优策略"></a>最优策略</h2><p>如果策略 π 优于策略 π′：</p>
<script type="math/tex; mode=display">
\pi \ge \pi^{'} \text{ if } v_{\pi}(s) \ge v_{\pi^{'}}(s), \forall{s}</script><p>最优策略 v∗ 满足：</p>
<ul>
<li>v∗≥π,∀π</li>
<li>v~π∗~(s)=v~∗~(s) </li>
<li>q~π∗~(s,a)=q~∗~(s,a) </li>
</ul>
<p>如何找到最优策略？</p>
<p>可以通过最大化 q~∗~(s,a)  来找到最优策略：</p>
<script type="math/tex; mode=display">
v_{*}(a|s) =
\begin{cases}
& 1 \text{ if } a=\arg\max_{a \in A}q_{*}(s,a)\\
& 0 \text{ otherwise }
\end{cases}</script><p><strong>对于MDP而言总存在一个确定的最优策略，而且一旦我们获得了q~∗~(s,a) ，我们就能立即找到最优策略。</strong></p>
<h2 id="Bellman-Expectation-Equation-for-MDPs"><a href="#Bellman-Expectation-Equation-for-MDPs" class="headerlink" title="Bellman Expectation Equation for MDPs"></a>Bellman Expectation Equation for MDPs</h2><p>我们先看下状态价值函数 v^π^。</p>
<p>状态 s 对应的 backup tree 如下图所示：</p>
<p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181027180345098-1901972119.png" alt=""></p>
<p>根据上图可得：</p>
<script type="math/tex; mode=display">
v_{\pi}(s) = \sum_{a \in A}\pi(a|s)q_{\pi}(s, a)  \qquad (1)</script><p>再来看动作价值函数 q~π~(s,a) </p>
<p>状态 s，动作 a 对应的 backup tree 如下图所示：</p>
<p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181027180402049-1747500206.png" alt=""></p>
<p>因此可得：</p>
<script type="math/tex; mode=display">
q_{\pi}(s,a)=R_s^a + \gamma \sum_{s'\in S}P_{ss'}^a v_{\pi}(s')  \qquad (2)</script><p>进一步细分 backup tree 再来看 v^π^  与 q~π~(s,a)  对应的表示形式。</p>
<p>细分状态 ss 对应的 backup tree 如下图所示：</p>
<p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181027180411412-1063042128.png" alt=""></p>
<p>将式子(2)代入式子(1)可以进一步得到 vπ(s)vπ(s) 的贝尔曼期望方程：</p>
<script type="math/tex; mode=display">
v_{\pi}(s) = \sum_{a \in A} \pi(a | s) \Bigl( R_s^a + \gamma \sum_{s'\in S}P_{ss'}^a v_{\pi}(s') \Bigr)  \qquad (3)</script><p>细分状态 ss，动作 aa 对应的 backup tree 如下图所示：</p>
<p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181027180421183-498067530.png" alt=""></p>
<p>将式子(1)代入式子(2)可以得到 qπ(s,a)的贝尔曼期望方程：</p>
<script type="math/tex; mode=display">
q_{\pi}(s,a)=R_s^a + \gamma \sum_{s'\in S}P_{ss'}^a \Bigl(\sum_{a' \in A}\pi(a'|s')q_{\pi}(s', a') \Bigr)  \qquad (4)</script><h2 id="Bellman-Optimality-Equation-for-MDPs"><a href="#Bellman-Optimality-Equation-for-MDPs" class="headerlink" title="Bellman Optimality Equation for MDPs"></a>Bellman Optimality Equation for MDPs</h2><p>同样我们先看 v~∗~(s)：</p>
<p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181027180430574-1927151238.png" alt=""></p>
<p>对应可以写出公式：</p>
<script type="math/tex; mode=display">
v_{*}(s) = \max_{a}q_{*}(s, a)   \qquad (5)</script><p> 再来看q~∗~(s,a)：</p>
<p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181027180436870-598952431.png" alt=""></p>
<p>对应公式为：</p>
<script type="math/tex; mode=display">
q_{*}(s, a) =  R_s^a + \gamma \sum_{s'\in S}P_{ss'}^a v_{*}(s') \qquad (6)</script><p>同样的套路获取 v∗(s) 对应的 backup tree 以及贝尔曼最优方程：</p>
<p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181027180450776-607271585.png" alt=""></p>
<p>贝尔曼最优方程：</p>
<script type="math/tex; mode=display">
v_{*}(s) = \max_{a} \Bigl( R_s^a + \gamma \sum_{s'\in S}P_{ss'}^a v_{*}(s') \Bigr) \qquad (7)</script><p>q∗(s,a)  对应的 backup tree 以及贝尔曼最优方程：</p>
<p><img src="https://img2018.cnblogs.com/blog/764050/201810/764050-20181027180500521-1105213902.png" alt=""></p>
<p>对应的贝尔曼最优方程：</p>
<script type="math/tex; mode=display">
R_s^a + \gamma \sum_{s'\in S}P_{ss'}^a\max_{a}q_{*}(s, a) \qquad (8)</script><h3 id="贝尔曼最优方程特点"><a href="#贝尔曼最优方程特点" class="headerlink" title="贝尔曼最优方程特点"></a>贝尔曼最优方程特点</h3><ul>
<li>非线性（non-linear）</li>
<li>通常情况下没有解析解（no closed form solution）</li>
</ul>
<h3 id="贝尔曼最优方程解法"><a href="#贝尔曼最优方程解法" class="headerlink" title="贝尔曼最优方程解法"></a>贝尔曼最优方程解法</h3><ul>
<li>Value Iteration</li>
<li>Policy Iteration</li>
<li>Sarsa</li>
<li>Q-Learning</li>
</ul>
<h2 id="MDPs的相关扩展问题"><a href="#MDPs的相关扩展问题" class="headerlink" title="MDPs的相关扩展问题"></a><del>MDPs的相关扩展问题</del></h2><ul>
<li><del>无限MDPs/连续MDPs</del></li>
<li><del>部分可观测的MDPs</del></li>
<li><del>Reward无折扣因子形式的MDPs/平均Reward形式的MDPs</del></li>
</ul>
]]></content>
      <categories>
        <category>MARL</category>
      </categories>
      <tags>
        <tag>MARL</tag>
      </tags>
  </entry>
  <entry>
    <title>GAMES101</title>
    <url>/CG/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/GAMES101/</url>
    <content><![CDATA[<h2 id="Lecture1-intro"><a href="#Lecture1-intro" class="headerlink" title="Lecture1:  intro"></a>Lecture1:  intro</h2><h3 id="计算机图形学"><a href="#计算机图形学" class="headerlink" title="计算机图形学"></a>计算机图形学</h3><p>使用计算机synthesize(合成)  manipulate（操作) 可视化信息</p>
<h3 id="why-study-computer-graphics"><a href="#why-study-computer-graphics" class="headerlink" title="why study computer graphics?"></a>why study computer graphics?</h3><ul>
<li>Application<ul>
<li>video games 电子游戏</li>
<li>animations 动画</li>
<li>visualization 可视化</li>
<li>virtual reality</li>
<li>augmented reality 增强现实</li>
<li>digital illustration 数码插画</li>
<li>simulation 模拟</li>
<li>graphical user interfaces 图形用户界面</li>
<li>typography  排版</li>
</ul>
</li>
<li>technical chanllenges </li>
</ul>
<h3 id="Course-topics"><a href="#Course-topics" class="headerlink" title="Course topics"></a>Course topics</h3><ul>
<li><p>Rasterization  光栅化</p>
<ul>
<li><p>project geometry primitives (3D triangles / polygons) onto the screen</p>
<p>将几何图形（3D三角形 / 多边形）投射到屏幕上</p>
</li>
<li><p>break projected primitives into fragments (pixels)</p>
<p>将投影图元分解到片段(像素)</p>
</li>
<li><p>gold standard in video games (real-time applications)</p>
</li>
</ul>
</li>
<li><p>curves and meshes 曲线和栅格</p>
<ul>
<li>怎样represent geometry in CG</li>
</ul>
</li>
<li><p>ray tracing 光线追踪</p>
<ul>
<li>shoot rays from camera though each pixel<ul>
<li>calculate intersection and shading 交叉点和阴影</li>
<li>continue to bounce the rays till they hit light sources</li>
</ul>
</li>
<li>gold standard in animations / movies (offline离线 application)</li>
</ul>
</li>
<li><p>animation simulation</p>
<ul>
<li>key frame animation 关键帧动画</li>
<li>mass-spring system 弹簧振子系统</li>
</ul>
</li>
</ul>
<h3 id=""><a href="#" class="headerlink" title=" "></a> </h3><h3 id="differences-between-cg-and-cv"><a href="#differences-between-cg-and-cv" class="headerlink" title="differences between cg and cv"></a>differences between cg and cv</h3><p>No clear boundaries</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210913231736.png" alt="image-20210913210044004"></p>
<h2 id="Lecture2-review-of-linear-algebra"><a href="#Lecture2-review-of-linear-algebra" class="headerlink" title="Lecture2:  review of linear algebra"></a>Lecture2:  review of linear algebra</h2><h3 id="Graphics’-dependcies"><a href="#Graphics’-dependcies" class="headerlink" title="Graphics’ dependcies"></a>Graphics’ dependcies</h3><ul>
<li>basic mathematics<ul>
<li>Linear algebra 线性代数<ul>
<li>mostly dependent on linear algebra</li>
<li>vectors（dot products点乘,cross products叉乘<ul>
<li>An operation like translating or rotating objects  can be matrix-vector multiplication</li>
</ul>
</li>
<li>matrices 矩阵（复数</li>
</ul>
</li>
<li>calculus 微积分</li>
<li>statistics 统计</li>
</ul>
</li>
<li>basic physics <ul>
<li>Optics, 光学的</li>
<li>Mechanics  机械的</li>
</ul>
</li>
<li>misc 杂项<ul>
<li>Numerical analysis  数值分析</li>
<li>signal processing 信号处理</li>
<li>aesthetics 审美</li>
</ul>
</li>
</ul>
<h3 id="vectors"><a href="#vectors" class="headerlink" title="vectors"></a>vectors</h3><p>noting: 只记了part</p>
<ul>
<li><p>unit vector </p>
<ul>
<li>单位向量，</li>
<li>用来代表方向</li>
</ul>
</li>
<li><p>dot  product in graphics</p>
<ul>
<li><p>$\vec{a}\cdot\vec{b} = |\vec{a}|\cdot|\vec{b}|cos\theta$ </p>
</li>
<li><p>Find angle between two vectors  (e.g. cosine of angle between light source 光源 and surface表面)</p>
</li>
<li><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210913231823.png" alt="image-20210913215253056"></p>
</li>
<li><p>Finding <strong>projection</strong> of one vector on another</p>
<ul>
<li><p>measure how close two directions are</p>
</li>
<li><p>decompose分解 a vector </p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210913231937.png" alt="image-20210913215715680" style="zoom:50%;" /></p>
</li>
<li><p>determine forward or backward</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210913231938.png" alt="image-20210913215758162" style="zoom: 50%;" /></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>cross product in graphics</p>
<p>​    <img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210913231939.png" alt="image-20210913220439863" style="zoom: 50%;" /></p>
<ul>
<li>Direction determined by right-hand rule</li>
</ul>
</li>
<li><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210913231940.png" alt="image-20210913220739158"></p>
<ul>
<li>Useful in constructing coordinate systems (later)</li>
<li>Determine left / right</li>
<li>Determine inside / outside</li>
</ul>
</li>
<li>Orthonormal bases and coordinate frames  正交基底和坐标系<ul>
<li>Critical issue is transforming between these systems/ bases</li>
</ul>
</li>
</ul>
<h3 id="matrices"><a href="#matrices" class="headerlink" title="matrices"></a>matrices</h3><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210913231941.png" alt="image-20210913223344462" style="zoom:50%;" /></p>
<ul>
<li>$(AB)^{T} = B^{T}A^{T}$</li>
<li>$AA^{-1} = A^{-1}A = I$</li>
<li>$(AB)^{-1} = B^{-1}A^{-1}$</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210913231942.png" alt="image-20210913223854494" style="zoom: 50%;" /></p>
<p>In Graphics, pervasively used to represent transformations</p>
<ul>
<li>translation, rotation,shear剪切,scale缩放</li>
</ul>
<h2 id="Lecture-3-Transformation"><a href="#Lecture-3-Transformation" class="headerlink" title="Lecture 3: Transformation"></a>Lecture 3: Transformation</h2><h3 id="why-study-transformation"><a href="#why-study-transformation" class="headerlink" title="why study transformation"></a>why study transformation</h3><ul>
<li>modeling<ul>
<li>translation</li>
<li>rotation</li>
<li>scaling</li>
</ul>
</li>
<li>viewing<ul>
<li>3D (projection)</li>
<li>2D (projection)</li>
</ul>
</li>
</ul>
<h3 id="2D-transformations"><a href="#2D-transformations" class="headerlink" title="2D transformations:"></a>2D transformations:</h3><ul>
<li><p>representing transformations using matrices</p>
</li>
<li><p>rotation</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914135044.png" alt="image-20210914135044036" style="zoom:50%;" /></p>
<ul>
<li>$R_{\theta} = \begin{bmatrix} cos\theta &amp; -sin\theta \\ sin\theta &amp; cos\theta \end{bmatrix} $</li>
<li>$R_{-\theta} = \begin{bmatrix} cos\theta &amp; sin\theta \\ -sin\theta &amp; cos\theta \end{bmatrix} = R_{\theta}^{T} = R_{\theta}^{-1}(by \quad definition) $</li>
<li>正交矩阵： A·A^T^ = E</li>
<li>默认绕原点旋转</li>
<li>默认逆时针旋转</li>
</ul>
</li>
<li><p>scale matrix</p>
<p>$\begin{bmatrix} x^{‘} \\ y^{‘} \end{bmatrix} = \begin{bmatrix} s_{x} &amp; 0 \\ 0 &amp; s_{y} \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}$</p>
</li>
<li><p>reflection matrix 反射（镜像）矩阵</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914124142.png" alt="image-20210914124142211" style="zoom: 50%;" /></p>
<p>$\begin{bmatrix} x^{‘} \\ y^{‘} \end{bmatrix} = \begin{bmatrix} -1 &amp; 0 \\ 0 &amp; 1\end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}$</p>
</li>
<li><p>shear matrix</p>
<p>​    <img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914134639.png" alt="image-20210914134632720" style="zoom: 50%;" /></p>
<p>$\begin{bmatrix} x^{‘} \\ y^{‘} \end{bmatrix} = \begin{bmatrix} -1 &amp; a \\ 0 &amp; 1\end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}$</p>
<ul>
<li>Hints:<ul>
<li>horizontal shift is 0 at y = 0</li>
<li>horizontal shift is a at y = 1</li>
<li>vertical shift is always</li>
</ul>
</li>
</ul>
</li>
<li><p>Linear transforms </p>
<ul>
<li>线性变换：可以用一个矩阵表示的变换</li>
<li>x’ = ax + by</li>
<li>y’ = cx + dy</li>
<li>$\begin{bmatrix} x^{‘} \\ y^{‘} \end{bmatrix} = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}$</li>
</ul>
</li>
</ul>
<h3 id="Homogeneous-coordinates-齐次坐标"><a href="#Homogeneous-coordinates-齐次坐标" class="headerlink" title="Homogeneous coordinates 齐次坐标"></a>Homogeneous coordinates 齐次坐标</h3><ul>
<li><p>Why homogeneous coordinates  </p>
<p>for example:  translation</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914135553.png" alt="image-20210914135548707" style="zoom:50%;" /></p>
<p>$\begin{bmatrix} x^{‘} \\ y^{‘} \end{bmatrix} = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix} + \begin{bmatrix} t_{x} \\ t_{y} \end{bmatrix} $</p>
<ul>
<li><p>so, translation is not linear transform!</p>
</li>
<li><p>因为平移变换不能直接用一个矩阵表示，必须加一个向量；  </p>
</li>
<li><p>add a third coordinates</p>
<p>引入齐次坐标可以解决问题，让平移也能只用一个矩阵表示</p>
</li>
<li><p>2D point = (x,y,1)^T^</p>
</li>
<li><p>2D vector = (x,y,0)^T^</p>
</li>
<li><p>向量 + 向量，结果齐次项是0，还是向量</p>
</li>
<li><p>点 - 点，得到的是一个向量，齐次项也变成0 </p>
</li>
<li><p>点 + 向量，表示一个点的移动，结果 还是点 ！</p>
</li>
<li><p>点 + 点是什么呢？齐次项变成2。将所有项除以2，齐次项又变为1 。所以点 + 点结果实际上是两个点的中点。</p>
</li>
</ul>
</li>
<li><p>Affine  transformation 仿射变换</p>
<ul>
<li><p>仿射变换：先线性变换再加上一次平移</p>
</li>
<li><p>$\begin{bmatrix} x^{‘} \\ y^{‘} \\ 1 \end{bmatrix} = \begin{bmatrix} a &amp; b &amp; t_{x}\\ c &amp; d  &amp; t_{y}  \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \begin{bmatrix} x \\  y \\ 1 \end{bmatrix} $</p>
</li>
<li><p>scale </p>
<p>$ S(s_{x}, s_{y}) = \begin{bmatrix} s_{x} &amp; 0 &amp; 0 \\ 0 &amp; s_{y}  &amp; 0  \\ 0 &amp; 0 &amp; 1 \end{bmatrix}$</p>
</li>
<li><p>rotation </p>
<p>$ R(\alpha) = \begin{bmatrix} cos\alpha &amp; -sin\alpha &amp; 0 \\ sin\alpha &amp; cos\alpha  &amp; 0  \\ 0 &amp; 0 &amp; 1 \end{bmatrix}$</p>
</li>
<li><p>translation</p>
<p>$ T(t_{x}, t_{y}) = \begin{bmatrix} 1 &amp; 0 &amp; t_{x} \\ 0 &amp; 1  &amp; t_{y} \\ 0 &amp; 0 &amp; 1 \end{bmatrix}$</p>
</li>
</ul>
</li>
<li><p>transform ordering matters</p>
<ul>
<li>matrix multiplication is not commutative 可交换的</li>
</ul>
</li>
</ul>
<h3 id="composing-transforms"><a href="#composing-transforms" class="headerlink" title="composing transforms"></a>composing transforms</h3><ul>
<li><p>decomposing complex transforms</p>
<ul>
<li>translate center to origin</li>
<li>rotate</li>
<li>translate back</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914142248.png" alt="image-20210914142248083"></p>
<p>which means $T(c) · R(\alpha) · T(-c)$</p>
<ul>
<li><p>分解：变换可以分解，注意先后顺序是从右到左</p>
</li>
<li><p>2D变换矩阵（缩放，旋转，平移变换）</p>
</li>
</ul>
</li>
<li></li>
</ul>
<h2 id="Lecture-4：-Transformation-Cont"><a href="#Lecture-4：-Transformation-Cont" class="headerlink" title="Lecture 4： Transformation Cont"></a>Lecture 4： Transformation Cont</h2><h3 id="3D-transformations"><a href="#3D-transformations" class="headerlink" title="3D transformations"></a>3D transformations</h3><ul>
<li><p>3D point = (x,y,z,1)^T^</p>
</li>
<li><p>3D vector = (x,y,z,0)^T^</p>
</li>
<li><p>$\begin{bmatrix} x^{‘} \\ y^{‘} \\ z_{‘} \\ 1 \end{bmatrix} = \begin{bmatrix} a &amp; b &amp; c &amp; t_{x}\\ d &amp; e &amp; f &amp; t_{y}\\g &amp; h &amp; i &amp; t_{z}  \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix} \begin{bmatrix} x \\ y \\z \\ 1 \end{bmatrix} $</p>
</li>
<li><p>三维空间中的齐次变换，最后一行和二维变换类似，是0 0 0 1，平移还是在矩阵最后一列</p>
</li>
<li><p>对于仿射变换，是先应用线性变换，再加上平移</p>
</li>
<li><p>what is order?</p>
<ul>
<li><p>linear transform first or translation first?</p>
</li>
<li><p>scale </p>
<p> $S(s_{x}, s_{y},s_{z}) = \begin{bmatrix} s_{x} &amp; 0 &amp; 0 &amp; 0\\ 0 &amp;  s_{y} &amp; 0 &amp; 0\\0 &amp; 0 &amp;  s_{z} &amp; 0  \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}  $</p>
</li>
<li><p>translation</p>
<p> $T(t_{x}, t_{y}, t_{z}) = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; t_{x}\\ 0 &amp;  1 &amp; 0 &amp; t_{y} \\0 &amp; 0 &amp;  1 &amp; t_{z}  \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}  $</p>
</li>
<li><p>rolation</p>
<ul>
<li>rolation around x-, y-,or  z-axis</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914203113.png" alt="image-20210914203043532" style="zoom:50%;" /></p>
<ul>
<li>绕哪个轴旋转，哪个坐标就不变</li>
<li><p>不过𝑅𝑦矩阵稍微不同，其他两个都是右上角𝑠𝑖𝑛是负的，只有他是左下角𝑠𝑖𝑛是负的 因为𝑥叉乘𝑦得到𝑧，𝑧叉乘𝑦得到𝑥，但是𝑧叉乘𝑥得到𝑦（而不是𝑥叉乘𝑧），所以是反的</p>
</li>
<li><p>$R_{xyz}(\alpha, \beta, \gamma) = R_{x}(\alpha)R_{y}(\beta)R_{z}(\gamma)$</p>
</li>
</ul>
</li>
<li><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914220209.png" alt="image-20210914220208915"></p>
</li>
</ul>
</li>
</ul>
<h3 id="Viewing-观测-transformation"><a href="#Viewing-观测-transformation" class="headerlink" title="Viewing (观测) transformation"></a>Viewing (观测) transformation</h3><ul>
<li><p>View (视图) / Camera transformation  </p>
<ul>
<li>Think about how to take a photo <ul>
<li>Find a good place and arrange people (model transformation) </li>
<li>Find a good “angle” to put the camera (view transformation)</li>
<li>Cheese! (projection transformation)<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914224154.png" alt="image-20210914224154439"></li>
<li>定义相机</li>
<li>位置</li>
<li>往哪看</li>
<li>向上方向</li>
<li>现实中是移动相机，变换景物</li>
<li>图形学中，相机不动，永远在原点</li>
<li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914223830.png" alt="image-20210914223829896" style="zoom:50%;" /></li>
<li>经过变换，把相机的位置移动到原点，同时保持看到的景物不变</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914225726.png" alt="image-20210914225726865"></p>
</li>
<li><p>这个从“歪”的坐标轴旋转回正的坐标轴，不太好写。 但是这个变换的逆过程，即：从正的坐标轴旋转到“歪”的坐标轴，是好写的， 于是我们先写从“正”坐标轴变换到“歪”坐标轴的变换矩阵，再求其逆矩阵，就可以得到待求的变换矩阵。 又因为旋转矩阵是正交矩阵，所以他的逆矩阵就只需要转置一下就可以得到了！ 注意，不但相机要做这个变换，其他物体也要做这个变换，因为我们想让相机看到的景物相对不变。 （以上部分个人认为非常巧妙和关键！）</p>
</li>
<li><p>Projection (投影) transformation</p>
<ul>
<li><p>3D to 2D</p>
</li>
<li><p>Orthographic (正交) projection  </p>
<p> <img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914230822.png" alt="image-20210914230821994"></p>
<ul>
<li><em>没有近大远小</em></li>
<li>平行投影<ul>
<li>首先定义空间中一个立方体，将其translate，使其中心在原点，再scale成标准立方体（边长为2</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915120758.png" alt="image-20210915120758604"></li>
<li>再次提醒，注意𝑧轴是近大远小 OpenGL等API是反过来的</li>
</ul>
</li>
<li><p>Perspective (透视) projection</p>
<p> <img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210914230840.png" alt="image-20210914230840011"></p>
<ul>
<li><p>更像人眼看到的场景</p>
</li>
<li><p>Most common in Computer Graphics, art, visual system </p>
</li>
<li><p>Further objects are smaller </p>
</li>
<li><p>Parallel lines not parallel; converge to single point</p>
</li>
<li>Recall: property of homogeneous coordinates<ul>
<li>(x, y, z, 1), (kx, ky, kz, k != 0), (xz, yz, z2, z != 0) all represent  the same point (x, y, z) in 3D </li>
<li>e.g. (1, 0, 0, 1) and (2, 0, 0, 2) both represent (1, 0, 0)</li>
</ul>
</li>
<li>how to do perspective projection<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915122816.png" alt="image-20210915122816233"></li>
<li>先将frustum远平面，挤压成和近平面一样大（从左图变成右图）</li>
<li>再做正交投影，投影到近平面</li>
<li>上述操作过程中几点假设：<ul>
<li>1）近平面保持不变 </li>
<li>2）z值保持不变，只是向内收缩</li>
</ul>
</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915122907.png" alt="image-20210915122907760"></li>
<li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915122956.png" alt="image-20210915122956833"></li>
<li>挤压这一步怎么做？ 上图是从侧面观察frustum 用相似三角形知识可以得到新坐标的表达式，但是第三个分量目前还不知道（这里利用之前讲的那个性质： 齐次坐标，如果我们对点的坐标所有分量同时乘以k，他表示的还是原来那个点！  </li>
<li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915123123.png" alt="image-20210915123123207"></li>
<li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915123140.png" alt="image-20210915123140120"></li>
<li>如何求解第三行<ul>
<li>任何近平面上的点不会改变（也就是对于任意的(𝑥, 𝑦, 𝑛, 1)，经过这个矩阵变换后，点的位置仍然不变）</li>
<li>任何远平面上的点，𝑧值不会改变</li>
<li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915123415.png" alt="image-20210915123415483"></li>
<li>点(𝑥, 𝑦, 𝑧, 1)是可以通过矩阵变换得到(𝑛𝑥, 𝑛𝑦, 𝑢𝑛𝑘𝑛𝑤𝑜𝑛, 𝑧)向量的。 根据上文提到的性质（1），经过这个变换，点实际没有改变 而同时，(𝑥, 𝑦, 𝑧, 1)本身可以写成(𝑥, 𝑦, 𝑛, 1)（为什么把𝑧替换成𝑛？因为近平面的𝑧坐标就是都是𝑛，所以可以做这个替换。）然后同时乘以𝑛， 变成(𝑛𝑥, 𝑛𝑦, 𝑛 ଶ , 𝑛) 经过上面两个推导，可以看出，第三行前两个数一定是0 因为𝑛 ଶ这个分量和𝑥和𝑦都毫无关系，因此前两个数必定是0 这样，我们就解出了第三行前两个数，都是0 接下来求A和B</li>
<li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915123500.png" alt="image-20210915123459854"></li>
<li>远平面上有一个特殊点，(0, 0, 𝑓)经过变换挤压仍然不变 所以(0, 0, 𝑓)经过变换仍然是(0, 0, 𝑓) 根据近平面我们得到$An + B = n^{2}$,根据远平面的中心点我们得到$Af + B = f^{2}</li>
<li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915123625.png" alt="image-20210915123625508"></li>
<li>这样我们就能解出A和B了， 这样终于把从透视投影挤压成正交投影的矩阵，解出来了</li>
<li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915123651.png" alt="image-20210915123651566"></li>
</ul>
</li>
</ul>
</li>
<li><p>思考题</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/image/20210915123710.png" alt="image-20210915123709902"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Lecture05-Rasterization-1-Triangles"><a href="#Lecture05-Rasterization-1-Triangles" class="headerlink" title="Lecture05: Rasterization 1(Triangles)"></a>Lecture05: Rasterization 1(Triangles)</h2><h3 id="Finishing-up-Viewing"><a href="#Finishing-up-Viewing" class="headerlink" title="Finishing up Viewing"></a>Finishing up Viewing</h3><ul>
<li>Viewport(视口) transformation</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005190732.png" alt="image-20211005190731847" style="zoom: 50%;" /></p>
<p>上节课把透视投影转化成正交投影 这里引入另外一个概念 Field of View，表示你能看到的角度的范围 注意看上图中红色线的夹角，就是垂直可视角度，他越大，可视角度越大 同理还有水平可视角度</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005190927.png" alt="image-20211005190926969"></p>
<p>MVP这三个变换之后，所有东西都会停留在一个1，1，1的位于原点的标准立方体中 下一步就要把这立方体画在屏幕上</p>
<h3 id="Rasterization（光栅化，即把东西花在屏幕上"><a href="#Rasterization（光栅化，即把东西花在屏幕上" class="headerlink" title="Rasterization（光栅化，即把东西花在屏幕上"></a>Rasterization（光栅化，即把东西花在屏幕上</h3><h4 id="屏幕"><a href="#屏幕" class="headerlink" title="屏幕"></a>屏幕</h4><ul>
<li><p>像素是最小的屏幕单位</p>
</li>
<li><p>每个像素有不同的颜色</p>
</li>
<li><p>屏幕空间：就是给屏幕定义一个坐标系 比如，可以定义左下角是原点。 </p>
</li>
<li><p>实际上像素的中心是(𝑥 + 0.5, 𝑦 + 0.5)</p>
</li>
<li><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005191523.png" alt="image-20211005191523440"></p>
</li>
<li><p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005191557.png" alt="image-20211005191556944"></p>
<p>我们要做的就是把标准立方体空间映射到屏幕这个二维世界中去 𝑧暂时不管 其他两个坐标是[−1, 1] ଶ转换到 [0, 𝑤𝑖𝑑𝑡ℎ] ∗ [0, ℎ𝑒𝑖𝑔ℎ𝑡] 使用上面这个矩阵做变换</p>
</li>
</ul>
<h4 id="Rasterizing-a-triangle"><a href="#Rasterizing-a-triangle" class="headerlink" title="Rasterizing  a triangle"></a>Rasterizing  a triangle</h4><ul>
<li><p>三角形可以拼接在三维空间中的面，或者二维空间中复杂的图形 </p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005191718.png" alt="image-20211005191718242"></p>
</li>
<li><p>三角形内部一定是平面的 </p>
</li>
<li>给三角形顶点定义不同属性，可以在三角形内部进行插值</li>
<li>通过采样的方式，来画出三角形<ul>
<li>采样就是把函数离散化的过程</li>
<li>可以对时间，面积，方向，体积… 进行采样</li>
</ul>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005191941.png" alt="image-20211005191941468"></p>
<p>定义二值函数：</p>
<script type="math/tex; mode=display">
inside(tri, x,y) = \left\{\begin{array}{rcl}
1  &      & {Point(x,y) \ in  \ triangle \ t}\\
0  &      & {otherwise}\\
\end{array}\right.</script><ul>
<li><p>这里我们要做的就是给定一个三角形，判断像素中心是否在三角形内部。</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005192820.png" alt="image-20211005192820679"></p>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005192920.png" alt="image-20211005192920137"></p>
<p>那么，如何判断一个点是否在三角形内？用叉乘！！ 比如对上图，判断Q是否在三角形内部 首先$𝑃1𝑃2 \  X \ 𝑃1𝑄 $，将会得到一个z为正数的向量，也就是结果向量朝向屏幕外的，利 用右手定则，可以得知𝑄在𝑃1𝑃2的左侧（因为如果在右侧，那么结果将会是向量𝑧为负 数，那么结果向量就朝向屏幕内部） 类似的𝑃2𝑃0 𝑋 𝑃2𝑄，得到𝑄在右侧，不对劲！ 𝑃0𝑃1 𝑋 𝑃0𝑄，得到𝑄在左侧</p>
<p>注意，向量按照一定的顺序去判断，比如我们上面是按照P1, P2, P0去判断的</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005193137.png" alt="image-20211005193137111"></p>
<p>检查屏幕所有的像素太花时间！ 可以只检查蓝色的包围盒（Bounding box）部分</p>
<p><img src="https://cdn.jsdelivr.net/gh/ChangQingAAS/for_picgo/img/20211005193213.png" alt="image-20211005193213667"></p>
<p>也可以每一行设置一个包围盒，进一步减小包围盒 很适用于那种三角形很小，但是包围盒很大的（窄长三角形</p>
<h2 id="some-words"><a href="#some-words" class="headerlink" title="some words"></a>some words</h2><p>syllabus  教学大纲</p>
<p>cube  立方体</p>
<p>canonical 标准的</p>
<p>aspect ratio 横纵比</p>
<p>requirements 要求</p>
<p>submission 提交</p>
<p>slides 幻灯片</p>
<p>Bulletin Board System BBS</p>
<p>Semantic Segmentation 语义切割</p>
<p>code skeletons 代码框架</p>
<p>IDE: Integrated Development Environment</p>
<p>parse 解析</p>
<p>Academic integrity 学术诚信</p>
<p>Valentine’s Day 情人节</p>
<p>brutal 粗暴的</p>
<p>coordinates 坐标</p>
<p> Parallelogram law 平行四边形法则</p>
<p>Triangle law 三角法则</p>
<p>orthogonal 正交的</p>
<p> scalar 标量</p>
<p>decompose 分解</p>
<p>haunt 出没</p>
<p>pervasively 普遍地</p>
<p>multiplication 乘法</p>
<p>trivial 琐碎的</p>
]]></content>
      <categories>
        <category>CG</category>
      </categories>
      <tags>
        <tag>CG</tag>
      </tags>
  </entry>
  <entry>
    <title>墨子平台训练教程</title>
    <url>/MARL/MADRL/%E5%A2%A8%E5%AD%90%E5%B9%B3%E5%8F%B0%E8%AE%AD%E7%BB%83%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<p>下面介绍一下windows版本墨子平台的使用流程：</p>
<ol>
<li><h2 id="进入华戍防务-官网-hs-defense-com-下载软件"><a href="#进入华戍防务-官网-hs-defense-com-下载软件" class="headerlink" title="进入华戍防务 官网(hs-defense.com)下载软件"></a>进入<a href="http://www.hs-defense.com/col.jsp?id=105">华戍防务 官网(hs-defense.com)</a>下载软件</h2></li>
</ol>
<p>我下载的是<code>墨子·联合作战推演系统（个人版）</code> </p>
<p><code>墨子·AI开发包</code>主要是一些开发会用到的代码和文档。代码也可以在gitee.com上找到：<a href="https://gitee.com/hs-defense/moziai">moziai: 墨子AI开发包及“子牙”智能体开源代码  </a></p>
<p><code>竞赛客户端(互联网)</code> 主要是用来军事推演比赛的，与强化学习训练关系不大，可以不下载。</p>
<p><code>墨子·AI版（Linux)</code> 因为一开始没办法下载，后面断断续续也没有弄出什么效果</p>
<p>注：</p>
<ul>
<li><p>文件采用ftp协议下载，下载链接格式为：ftp: ip/port/文件夹s/文件</p>
</li>
<li><p>由于部分浏览器不支持，推荐使用QQ浏览器下载</p>
</li>
<li><p>一般点击下载按钮后，会默认让迅雷接管下载</p>
</li>
</ul>
<ol>
<li><h2 id="安装软件"><a href="#安装软件" class="headerlink" title="安装软件"></a>安装软件</h2><p>​    根据安装手册安装该软件，基本上按照手册进行就可以了</p>
</li>
<li><h2 id="启动该软件"><a href="#启动该软件" class="headerlink" title="启动该软件"></a>启动该软件</h2><ol>
<li>进入安装目录下的\MoziData，右键点击<code>mysql.bat</code>，以管理员身份运行它，随后退出</li>
<li>进入安装目录下的\MoziServer，运行MoziServer.exe，尽量关掉杀毒软件，最好以管理员身份运行<ul>
<li>这里大概会遇到一个问题：<code>临时许可码过期，请联系华戍防务重新授权</code>，具体解决方案略</li>
<li>我在数据库方面也出过问题，不过忘了具体细节了</li>
</ul>
</li>
</ol>
</li>
<li><h2 id="运行代码，进行仿真训练"><a href="#运行代码，进行仿真训练" class="headerlink" title="运行代码，进行仿真训练"></a>运行代码，进行仿真训练</h2><ol>
<li><p>获取代码，在IDE打开代码</p>
</li>
<li><p>加载代码所需的scen想定文件</p>
<ul>
<li>进入墨子平台，可以在想定一栏下找到<code>加载想定</code>的选项，想定文件是从<code>安装路径\MoziServer\bin\Scenarios\</code>获取的，因此需要把代码包里的scen文件放到这里。</li>
<li>加载想定文件，选择推演方</li>
<li>随后可以看到墨子平台上有了具体的想定环境</li>
<li>如果要更改想定文件，改完之后，需要保存，再重新加载<ul>
<li>因为训练的每个回合都会刷新环境，如果不保存更改到新scen文件，那么这个更改只能用于一个回合</li>
</ul>
</li>
</ul>
</li>
<li><p>运行main.py代码即可看到效果</p>
<ul>
<li><p>注：</p>
<ul>
<li><p>这里可能需要指定一下 墨子平台的路径，可以执行代码</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">os.environ[&#39;MOZIPATH&#39;] &#x3D; &#39;D:\\MoZiSystem\\Mozi\\MoziServer\\bin&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>也可以在<code>编辑系统环境变量</code>中添加路径</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
</ol>
<p>注：Linux版本的墨子平台还在折腾，因为虚拟机比较卡，服务器上又没办法弄出显示界面，还折腾了一堆配置，最近还没弄，如果有结果了，会在这里更新的</p>
]]></content>
      <categories>
        <category>MARL</category>
      </categories>
      <tags>
        <tag>MARL</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>一些比较好的文章</title>
    <url>/MARL/MADRL/%E4%B8%80%E4%BA%9B%E6%AF%94%E8%BE%83%E5%A5%BD%E7%9A%84%E6%96%87%E7%AB%A0/</url>
    <content><![CDATA[<p><a href="https://blog.justlovesmile.top/posts/bfa4054.html">深度学习 | 《深度学习入门之PyTorch》阅读笔记 | Justlovesmile’s BLOG</a></p>
<p>注： 这本书的github代码已经4年没更新了，只适合速览一下，做个overview, 并不适合手敲学习</p>
<p><a href="https://blog.justlovesmile.top/posts/43678.html">深度学习 | “花书”，Deep Learning笔记 | Justlovesmile’s BLOG</a></p>
<p> 注：偏数学</p>
]]></content>
      <categories>
        <category>MARL</category>
      </categories>
      <tags>
        <tag>MARL</tag>
        <tag>机器学习，碎碎念</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库基础__笔记</title>
    <url>/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%94%B6%E8%97%8F%E5%A4%B9/</url>
    <content><![CDATA[<p><a href="https://blog.justlovesmile.top/posts/41347.html">大学课程 | 数据库基础 | Justlovesmile’s BLOG</a></p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>算法分析与设计__笔记</title>
    <url>/%E7%AE%97%E6%B3%95/%E7%AE%97%E6%B3%95/%E6%94%B6%E8%97%8F%E5%A4%B9/</url>
    <content><![CDATA[<p><a href="https://blog.justlovesmile.top/posts/16050.html">大学课程 | 《算法分析与设计》笔记 | Justlovesmile’s BLOG</a></p>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>什么是node.js</title>
    <url>/js/js/%E4%BB%80%E4%B9%88%E6%98%AFnode.js/</url>
    <content><![CDATA[<h1 id="node-js简介"><a href="#node-js简介" class="headerlink" title="node.js简介"></a>node.js简介</h1><ul>
<li><p>node.js是运行在服务端的JavaScript,是一个事件驱动I\O服务端JavaScript环境</p>
</li>
<li><p>查看版本</p>
<pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">node -v<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
</li>
<li><p>node版本管理工具nvm</p>
</li>
</ul>
<h1 id="Node-js应用"><a href="#Node-js应用" class="headerlink" title="Node.js应用"></a>Node.js应用</h1><ul>
<li>Node.js应用的构成：<ul>
<li>引入required模块</li>
<li>创建服务器</li>
<li>接收请求与相应请求</li>
</ul>
</li>
</ul>
<h2 id="创建Node-js应用"><a href="#创建Node-js应用" class="headerlink" title="创建Node.js应用"></a>创建Node.js应用</h2><ul>
<li><p>步骤一：引入required模块</p>
<ul>
<li>使用require指令来载入http模块，并将实例化的HTTP复制给变量http，实例如下：  <pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token keyword">var</span> http <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"http"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
</li>
</ul>
</li>
<li><p>步骤二： 创建服务器</p>
<ul>
<li>创建一个server.js的文件：<pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token keyword">var</span> http <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"http"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

http<span class="token punctuation">,</span><span class="token function">createServer</span><span class="token punctuation">(</span><span class="token keyword">function</span><span class="token punctuation">(</span><span class="token parameter">request<span class="token punctuation">,</span>response</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>
    <span class="token comment">// 发送http头部</span>
    <span class="token comment">//HTTP状态值：200：OK</span>
    <span class="token comment">//内容类型：text/plain</span>
    response<span class="token punctuation">.</span><span class="token function">writeHead</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">,</span><span class="token punctuation">&#123;</span><span class="token string">'Content-Type'</span><span class="token operator">:</span>'text<span class="token operator">/</span>plain<span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//发送响应数据 “Hello World"</span>
    response<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token string">'Hello World\n'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span><span class="token punctuation">)</span>

<span class="token comment">//终端打印如下信息</span>
console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span><span class="token string">'Server running at http://127.0.0.1:8888/'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><p>使用node命令执行以上代码</p>
  <pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">node server.js<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
</li>
<li><p>访问<a href="http://127.0.0.1:8888">http://127.0.0.1:8888</a></p>
</li>
</ul>
</li>
</ul>
<h2 id="npm使用介绍"><a href="#npm使用介绍" class="headerlink" title="npm使用介绍"></a>npm使用介绍</h2><ul>
<li><p>查看npm版本</p>
<pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">npm -v <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
</li>
<li><p>升级版本</p>
<pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">npm install npm -g<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
</li>
<li><p>安装包</p>
<pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">npm install &lt;Module Name&gt; # 本地安装
npm install &lt;Module Name&gt; -g # 全局安装<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
</li>
<li><p>查看安装信息</p>
<pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">npm list -g 			# 查看全局安装的模块
npm ist &lt;Module Name&gt;	# 查看某个模块的版本号
npm ls					# 查看当前目录下的包信息<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
</li>
<li><p>卸载模块</p>
<pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">npm un &lt;Module Name&gt; <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
</li>
<li><p>更新模块</p>
<pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">npm update &lt;Module Name&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
</li>
<li><p>搜索模块</p>
<pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">npm search &lt;Module Name&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
</li>
<li><p>创建模块</p>
<pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">$npm init			# 会自动生成package.json

$npm adduser 		# 在npm资源库中注册用户
Username:XXXX
Password:XXXX
Email:XXXX

$npm publish		# 发布模块
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ul>
<h2 id="Node-js-REPL-交互式解释器"><a href="#Node-js-REPL-交互式解释器" class="headerlink" title="Node.js REPL(交互式解释器)"></a>Node.js REPL(交互式解释器)</h2><ul>
<li><p>REPL(Read Eval Print Loop:交互式解释器)，Node自带交互式解释器，可以执行读取,执行,打印,循环等任务</p>
</li>
<li><p>在Node的REPL中可以执行：</p>
<ul>
<li>简单的表达式计算</li>
</ul>
<pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">$node
&gt;1 + 4
5
&gt; 5 &#x2F; 2
2.5
&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>使用变量</li>
</ul>
<pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">$ node
&gt; 1+4
5
&gt; 5&#x2F;2
2.5
&gt; x&#x3D;10
10
&gt; var y&#x3D;10
undefined
&gt; x+y
20
&gt; console.log(&quot;Hello World!&quot;)
Hello World!
undefined
&gt;  
 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>多行表达式</li>
</ul>
<pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">$ node
&gt; do &#123;
... x++;
... console.log(&quot;x:&quot;+x);
... &#125;while(x&lt;5);
x:1
x:2
x:3
x:4
x:5
undefined
&gt;  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><p>下划线变量<strong>[使用下划线(_)获取上一个表达式的运算结果]</strong></p>
<pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">$ node
&gt; var x&#x3D;10;
undefined
&gt; y&#x3D;10;
10
&gt; x+y
20
&gt; var sum&#x3D;_
undefined
&gt; console.log(sum)
20
undefined
&gt; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ul>
</li>
<li><p>两次ctrl+c停止REPL</p>
<h2 id="Node-js的回调函数"><a href="#Node-js的回调函数" class="headerlink" title="Node.js的回调函数"></a>Node.js的回调函数</h2><ul>
<li><p>Node.js异步编程的直接体现就是回调</p>
</li>
<li><p>阻塞代码实例</p>
<ul>
<li><p>创建一个文件 input.txt ，内容如下：</p>
<pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">Hello world!<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
</li>
<li><p>创建 main.js 文件, 代码如下：</p>
<pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token keyword">var</span> fs <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"fs"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token keyword">var</span> data <span class="token operator">=</span> fs<span class="token punctuation">.</span><span class="token function">readFileSync</span><span class="token punctuation">(</span><span class="token string">'input.txt'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span>data<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span><span class="token string">"程序执行结束!"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
<li><p>以上代码执行结果如下：</p>
<pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">$ node main.js
Hello World！

程序执行结束!<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ul>
</li>
<li><p>非阻塞代码实例</p>
<pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token keyword">var</span> fs <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">"fs"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

fs<span class="token punctuation">.</span><span class="token function">readFile</span><span class="token punctuation">(</span><span class="token string">'input.txt'</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token parameter">err<span class="token punctuation">,</span> data</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>err<span class="token punctuation">)</span> <span class="token keyword">return</span> console<span class="token punctuation">.</span><span class="token function">error</span><span class="token punctuation">(</span>err<span class="token punctuation">)</span><span class="token punctuation">;</span>
    console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span>data<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span><span class="token string">"程序执行结束!"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>以上代码执行结果如下：</p>
<pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">$ node main.js
程序执行结束!
Hello World!<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
</li>
<li><p>第一个实例在文件读取完后才执行完程序。 第二个实例我们不需要等待文件读取完，这样就可以在读取文件时同时执行接下来的代码，大大提高了程序的性能。</p>
</li>
<li><p>因此，阻塞是按顺序执行的，而非阻塞是不需要按顺序的，所以如果需要处理回调函数的参数，我们就需要写在回调函数内。</p>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>js</category>
      </categories>
  </entry>
  <entry>
    <title>How-to-be-a-good-programmer</title>
    <url>/%E7%A2%8E%E7%A2%8E%E5%BF%B5/others/To-be-a-good-programmer/</url>
    <content><![CDATA[<h2 id="实例驱动学习"><a href="#实例驱动学习" class="headerlink" title="实例驱动学习"></a>实例驱动学习</h2><p>在知识爆炸的年代, 想成为一个好的程序员, 要具备这样一个精神:</p>
<p><strong>开发者精神</strong></p>
<blockquote>
<p>开发者精神是指, 从学习编程第一天起, 你的目标, 你所做的事, 永远不是以<code>要学会XX</code>为目标, 而是以<code>开发出XX</code>为目标, 深刻意识到你学习的一切, 最终是为了你的开发而服务的。</p>
</blockquote>
<p>而开发者精神的反面则是<code>学生气</code>:</p>
<p><strong>学生气</strong></p>
<blockquote>
<p>从学习编程的第一天起, 就把自己当学生, 上课听讲记笔记为第一位, 记忆知识点为第一位, 买教材读教材为第一位, 追求<code>学会XX</code>, 而不是<code>用它开发出XX</code>。</p>
</blockquote>
<p>我们举个例子， 假如你要学习Java，<code>学生气</code>的学生行为模式:</p>
<pre><code>读教材, 听课, 记笔记, 追求把这个语言的每个知识点都记得很清楚, 追求一种&quot;内功&quot;的修炼, 在这个过程中, 从来不想着用它去&quot;创造&quot;什么. 在学习的第一天起, 他给自己定下了一个目标:

&quot;我这个学期一定要把这个语言的基础打牢, 最后在考试中取得高分, 并且为以后的学习提供更坚实的基础.&quot;
</code></pre><p>那么具有<code>开发者精神</code>的人会这样做:</p>
<pre><code>先大体了解一下Java语言在哪些领域比较强势, 做一个简单的调研, 得出了Java在Web领域很强势这一个信号, 于是在学习Java的第一天起, 他就给自己定下了一个目标: 

&quot;我要在半年内通过学习Java, 运用Java做出一个中小型的, 基于Web的企业管理后台&quot;
</code></pre><p>这两种人, 起点都一样, 都是<code>完全不会Java</code>, 但是最后的结果往往是, 后者无论是知识熟练度还是实用性都会超过前者, 而且整个学习过程会有源源不断地动力.</p>
<p><code>夯实基础</code>这个目标, 其实是空洞的, 什么叫做夯实? 什么叫做基础? 而且就算你真的夯实了基础, 你也极有可能陷入一种<code>虚无感</code>中, 因为你的所有知识, 都会遗忘.</p>
<p>这就是为什么很多计算机专业学生很爱问的一个问题:</p>
<pre><code>&quot;学了这么多知识, 忘了怎么办?&quot;
</code></pre><p>如果你具备<code>开发者精神</code>, 那么你根本不会理会这个问题, 因为, 你开发出的东西, 你做出的产品, 它就是永久存在在这个世界上的, 你的成就感来源于真实的, 具体的, 可持久延续的项目中, 而不是来源于”我学会了什么”.</p>
<p>我们一定要明白:</p>
<pre><code>一切不谈成就感, 不谈反馈的学习劝导, 都是在耍流氓
</code></pre><p>既然我们想获得反馈, 那么一个很现实的建议就是, 在Github上创建你的第一个开源代码仓库, 长期地, 稳定地commit, 当然, 至于这个仓库到底是干啥的, 这个因人而异, 有可能是你自己开发的一个VScode插件, 有可能是你自己写的读书笔记, 在这个不断地commit的过程中, 感受你的代码仓库不断增长不断完善的过程中, 你会获得一个比较持久的反馈和成就感, 一旦有了反馈和成就感, 那么你的学习动力就会一直保持, 也可以从学习中获得快乐.</p>
<p><strong>记住, 你不是学生, 你是开发者.</strong></p>
<h2 id="知识输入与输出"><a href="#知识输入与输出" class="headerlink" title="知识输入与输出"></a>知识输入与输出</h2><p>当你学习一个东西的时候, 如果学完马上用语言讲给别人听, 你会学的更好, 而且会发现新问题.</p>
<p>很多学习理论都指出, 知识的学习, 输入固然重要, 输出更为重要.</p>
<p>输入就是学习别人的知识, 输出就是把自己学会的知识用文字, 语言的形式表达出来, 很多人的学习, 只有输入, 没有输出, 这样的学习肯定是不行的.</p>
<p>几乎所有优秀的程序员, 都有攥写技术文章的习惯, 很多时候, 并不是他们什么都懂, 而是他们刚学会了什么, 然后就围绕着这个刚学会的东西, 用自己的语言讲出来, 久而久之, 就会被别人觉得是大牛, 但是他和你的区别, 有可能仅仅是是否输出的区别.</p>
<p>建议所有计算机学习者, 都要有写技术文章/读书笔记的习惯, 可以发表在自己的个人主页, 其他平台上, 攥写技术文章也可以很好地增加面试官对你的好感.</p>
<p>写技术文章的时候, 最好用<code>MarkDown</code>哦, 程序员是不需要用<code>Word</code>这种东西的, 我们的内容是要方便发表在网站上, <code>Word</code>不能直接在网站里显示, 而<code>MarkDown</code>可以轻易地转换成<code>html</code>格式文件, 在浏览器中显示.</p>
<h2 id="代码风格篇"><a href="#代码风格篇" class="headerlink" title="代码风格篇"></a>代码风格篇</h2><p>现在几乎所有主流语言都有相应的代码风格检查工具, 一般已IDE或Editor的插件或扩展形式给出</p>
<h3 id="设计模式"><a href="#设计模式" class="headerlink" title="设计模式"></a>设计模式</h3><p>设计模式并不是针对任何一种语言, 而是一种用编程语言描述现实世界问题广泛采用的各种模式, 比如, 观察者模式, 工厂模式, 依赖注入模式等.</p>
<p>好的设计模式, 是好代码的保证, 只有先学会了设计模式, 才能在软件设计中游刃有余, 而且更现实的问题是, 现在很多框架, 比如Spring, Angular等, 都充斥了大量的设计模式, 比如<code>Factory Pattern</code>, <code>Dependency Injection Pattern</code>, 你必须理解这些设计模式, 你才能理解这个框架.</p>
<p>个人认为设计模式没有必要买本教材, 结合视频和技术文章, 就足以理解和运用.</p>
<h3 id="小习惯"><a href="#小习惯" class="headerlink" title="小习惯"></a>小习惯</h3><ol>
<li>不要嫌变量名长, 最好可以直接通过变量名推测变量的作用.</li>
<li>重复出现的代码, 封装成独立的类或函数.</li>
<li>提前降低代码的耦合度, 不同作用, 不同类别的代码, 不要混在一起, 最好分成独立的文件</li>
<li>将代码进行业务分层, 比如在Web开发中, 有数据层, 服务层, DTO层, Controller层, 渲染层等, 只有将层次分开了, 才能获得足够的可拓展性, 不然代码多了你就全乱了.</li>
<li>用良好的设计模式去”设计”软件, 在执行一些算法的时候, 可以想一想它的时空复杂度, 想一想怎么可以让它执行地更快.</li>
</ol>
<h2 id="如何管理自己的电脑"><a href="#如何管理自己的电脑" class="headerlink" title="如何管理自己的电脑"></a>如何管理自己的电脑</h2><h3 id="包管理工具"><a href="#包管理工具" class="headerlink" title="包管理工具"></a>包管理工具</h3><p>为了更好的管理我们的各种应用包，最好使用包管理工具来进行各种包，软件的安装和使用, 方便进行卸载, 更新, 安装, 无需打开浏览器即可完成一系列操作.</p>
<blockquote>
<p>linux : apt-get (ubuntu distro)<br>    osx : brew<br>    windows: Chocolatey</p>
</blockquote>
<p>另外, windows平台上的包管理工具, 在国内的网络环境下经常慢的感人, 所以<code>Chocolatey</code>可能使用体验并不好.  推荐使用今年微软新发布的 <code>WSL2</code>, 这个东西不是那种传统的虚拟机, 有了它你可以像操作linux系统那样操作windows系统, 而且支持 <code>docker</code>, 甚至我推荐以后大家所有命令都可以在<code>WSL2</code>里执行.</p>
<h3 id="容器-Docker"><a href="#容器-Docker" class="headerlink" title="容器 - Docker"></a>容器 - Docker</h3><p>为了更方便的进行开发环境配置，我推荐所有计算机学生尽早了解和使用Docker。</p>
<p>Docker 就是为了解决复杂的环境配置问题而生的。</p>
<p>它将你的软件和软件所依赖的所有环境打包成一个镜像(Image), 该镜像可以在任何一台装有docker 的电脑上运行, 和操作系统无关, 也就是docker 把运行环境和你的操作系统隔离开来了，中间隔了一层docker engine 。</p>
<p>写过不少代码的你一定见过这种现象：你的代码在自己电脑上能运行，但是在别人电脑上就会报错，无法运行，原因很简单，任何软件的运行都需要环境。</p>
<p>比如，jar包的运行需要jre ，python脚本的运行需要python 解释器安装在电脑上，以后你可能还需要运行一些服务，比如数据库mysql server , redis , rabbitmq , 随着软件运行环境复杂度的增加, 你的软件运行条件也变得苛刻，如果你想把本地的应用部署到服务器上，那事更多，需要的环境得一个个地装到你的linux服务器上，如何彻底解决这个问题？</p>
<p>只需在你的电脑中安装docker , 你就可以毫无后顾之忧。</p>
<p>所有的环境，服务，软件都是以Image 的形式打包的，Image 中包含了运行你软件的所有东西，比如你的软件是个python 脚本, 并且使用了第三方库flask，那这个Image 中就包含了python ,也就是它的base image , 也同时包含了flask, 这样的话任何一个装有docker的电脑都可以运行你的image。</p>
]]></content>
      <categories>
        <category>碎碎念</category>
      </categories>
      <tags>
        <tag>Advice</tag>
      </tags>
  </entry>
  <entry>
    <title>Python爬虫之Xpath解析</title>
    <url>/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%ABxpath%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<h2 id="xpath解析"><a href="#xpath解析" class="headerlink" title="xpath解析"></a>xpath解析</h2><ul>
<li>// :从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。</li>
<li>/ : 匹配当前目录下的直接子节点。</li>
<li>.. : 匹配当前节点的父节点。</li>
<li>@：选取属性。</li>
<li>//* : 选取文档中所有元素</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">text &#x3D; &quot;&quot;&quot;
            &lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;
            &lt;bookstore&gt;
            &lt;book&gt;
              &lt;title lang&#x3D;&quot;eng&quot;&gt;Harry Potter&lt;&#x2F;title&gt;
              &lt;price&gt;29.99&lt;&#x2F;price&gt;
            &lt;&#x2F;book&gt;
            &lt;book&gt;
              &lt;title lang&#x3D;&quot;cn&quot;&gt;Learning XML&lt;&#x2F;title&gt;
              &lt;price&gt;39.95&lt;&#x2F;price&gt;
              &lt;aa lang&#x3D;&quot;cn eng aa bb&quot; name&#x3D;&quot;cc&quot;&gt;Learning XML&lt;&#x2F;aa&gt;
            &lt;&#x2F;book&gt;
            &lt;&#x2F;bookstore&gt;
&quot;&quot;&quot;
from lxml import etree
html &#x3D; etree.HTML(text)
# print(etree.tostring(html).decode(&#39;utf-8&#39;))

# 选取所有指定的节点
res &#x3D; html.xpath(&#39;&#x2F;&#x2F;book&#39;)

# 获取指定节点的所有直接子节点
res &#x3D; html.xpath(&#39;&#x2F;&#x2F;book&#x2F;aa&#39;)

# 获取指定节点的父节点
res &#x3D; html.xpath(&quot;&#x2F;&#x2F;aa&#x2F;..&quot;)

# 通过属性匹配选择节点
res &#x3D; html.xpath(&#39;&#x2F;&#x2F;title[@lang&#x3D;&quot;cn&quot;]&#39;)

# 获取文本值
res &#x3D; html.xpath(&#39;&#x2F;&#x2F;title[@lang&#x3D;&quot;cn&quot;]&#x2F;text()&#39;)
res &#x3D; html.xpath(&#39;&#x2F;&#x2F;price&#x2F;text()&#39;)

# 获取属性值 [&#39;eng&#39;, &#39;cn&#39;]
res &#x3D; html.xpath(&#39;&#x2F;&#x2F;title&#x2F;@lang&#39;)

# 属性多值匹配
res &#x3D; html.xpath(&#39;&#x2F;&#x2F;aa[contains(@lang,&quot;aa&quot;)]&#39;)
# 对于属性值有多个的节点，不用contains函数的话，匹配到的是空[]
res &#x3D; html.xpath(&#39;&#x2F;&#x2F;aa[@lang&#x3D;&quot;aa&quot;]&#39;)

# 文本匹配
res &#x3D; html.xpath(&#39;&#x2F;&#x2F;title[contains(text(), &quot;XML&quot;)]&#39;)

# 运算符
res &#x3D; html.xpath(&#39;&#x2F;&#x2F;aa[contains(@lang,&quot;aa&quot;) and @name&#x3D;&quot;cc&quot;]&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫之BeautifulSoup</title>
    <url>/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB%E4%B9%8BBeautifulSoup/</url>
    <content><![CDATA[<h4 id="格式化输出"><a href="#格式化输出" class="headerlink" title="格式化输出"></a>格式化输出</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">html_doc &#x3D; &quot;&quot;&quot;
&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;
&lt;body&gt;
&lt;p class&#x3D;&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;&#x2F;b&gt;&lt;&#x2F;p&gt;

&lt;p class&#x3D;&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were
&lt;a href&#x3D;&quot;http:&#x2F;&#x2F;example.com&#x2F;elsie&quot; class&#x3D;&quot;sister&quot; id&#x3D;&quot;link1&quot;&gt;Elsie&lt;&#x2F;a&gt;,
&lt;a href&#x3D;&quot;http:&#x2F;&#x2F;example.com&#x2F;lacie&quot; class&#x3D;&quot;sister&quot; id&#x3D;&quot;link2&quot;&gt;Lacie&lt;&#x2F;a&gt; and
&lt;a href&#x3D;&quot;http:&#x2F;&#x2F;example.com&#x2F;tillie&quot; class&#x3D;&quot;sister bro&quot; id&#x3D;&quot;link3&quot;&gt;Tillie&lt;&#x2F;a&gt;;
and they lived at the bottom of a well.&lt;&#x2F;p&gt;

&lt;p class&#x3D;&quot;story&quot;&gt;...&lt;&#x2F;p&gt;
&quot;&quot;&quot;
soup &#x3D; BeautifulSoup(html_doc, &#39;html.parser&#39;)
print(soup.prettify())<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="find-all-name-attrs-recursive-string-kwargs"><a href="#find-all-name-attrs-recursive-string-kwargs" class="headerlink" title="find_all(name , attrs , recursive , string , **kwargs)"></a>find_all(name , attrs , recursive , string , **kwargs)</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 查找所有的a标签
res &#x3D; soup.find_all(&#39;a&#39;)
# # 查找所有的a标签和p标签
res &#x3D; soup.find_all([&#39;a&#39;, &#39;p&#39;])

# 查找class&#x3D;title的p标签
res &#x3D; soup.find_all(&#39;p&#39;, &#39;title&#39;)

# 指定属性查找  可支持字符串，正则表达式，或者函数
# 指定id查找元素
res &#x3D; soup.find_all(id&#x3D;&quot;link1&quot;)
# 指定href查找 [&lt;a class&#x3D;&quot;sister&quot; href&#x3D;&quot;http:&#x2F;&#x2F;example.com&#x2F;elsie&quot; id&#x3D;&quot;link1&quot;&gt;Elsie&lt;&#x2F;a&gt;]
res &#x3D; soup.find_all(href&#x3D;re.compile(&#39;elsie&#39;))
# 指定多个属性查找
res &#x3D; soup.find_all(id&#x3D;&#39;link1&#39;, href&#x3D;re.compile(&#39;elsie&#39;))
# 指定多个属性查找 attrs参数
res &#x3D; soup.find_all(attrs&#x3D;&#123;&#39;id&#39;: &#39;link1&#39;, &#39;href&#39;: re.compile(&#39;elsie&#39;)&#125;)

# 通过css搜索
res &#x3D; soup.find_all(class_&#x3D;&quot;sister bro&quot;)
# 通过函数过滤,查找类名长度大于6的元素
res &#x3D; soup.find_all(class_&#x3D;lambda x: x is not None and len(x) &gt; 6)

# recursive参数，如果只想搜索直接子节点  recursive&#x3D;False
res &#x3D; soup.find_all(&#39;title&#39;, recursive&#x3D;False)

# find_all() 方法的返回结果是值包含一个元素的列表
# 而find()方法直接返回第一个结果，没有则返回None.
res &#x3D; soup.find(&#39;a&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="CSS选择器"><a href="#CSS选择器" class="headerlink" title="CSS选择器"></a>CSS选择器</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 类查找
res &#x3D; soup.select(&#39;.sister&#39;)
# ID查找
res &#x3D; soup.select(&#39;#link1&#39;)
res &#x3D; soup.select(&#39;a#link1&#39;)
# 通过是否存在某个属性查找
res &#x3D; soup.select(&#39;a[href]&#39;)
# 指定属性值查找
res &#x3D; soup.select(&#39;a[href&#x3D;&quot;http:&#x2F;&#x2F;example.com&#x2F;tillie&quot;]&#39;)

# 查找返回第一个元素
res &#x3D; soup.select_one(&#39;a[href]&#39;)

# 获取元素的属性值
res &#x3D; soup.select_one(&#39;a[href]&#39;).get(&#39;href&#39;)
# 获取元素的文本
res &#x3D; soup.select_one(&#39;a[href]&#39;).text<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title>python爬虫之requests库</title>
    <url>/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB%E4%B9%8Brequests%E5%BA%93/</url>
    <content><![CDATA[<h2 id="一-发送请求"><a href="#一-发送请求" class="headerlink" title="一.发送请求"></a>一.发送请求</h2><p>requests提供了http的所有基本请求方式：<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">import requests
r &#x3D; requests.post(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;post&quot;)
r &#x3D; requests.put(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;put&quot;)
r &#x3D; requests.delete(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;delete&quot;)
r &#x3D; requests.head(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;get&quot;)
r &#x3D; requests.options(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;get&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p>
<p>基本get请求中参数的传递：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># requests允许使用params关键字参数，以字典的形式来提供get请求url中的参数。
payload &#x3D; &#123;&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;&#125;
r &#x3D; requests.get(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;get&quot;, params&#x3D;payload)
print(r.url)  # http:&#x2F;&#x2F;httpbin.org&#x2F;get?key2&#x3D;value2&amp;key1&#x3D;value1

# 字典中的value还可以以列表的形式传入
payload &#x3D; &#123;&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: [&#39;value2&#39;, &#39;value3&#39;]&#125;

r &#x3D; requests.get(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;get&#39;, params&#x3D;payload)
print(r.url)
http:&#x2F;&#x2F;httpbin.org&#x2F;get?key1&#x3D;value1&amp;key2&#x3D;value2&amp;key2&#x3D;value3<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>添加请求头headers<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">url &#x3D; &#39;https:&#x2F;&#x2F;api.github.com&#x2F;some&#x2F;endpoint&#39;
headers &#x3D; &#123;&#39;user-agent&#39;: &#39;my-app&#x2F;0.0.1&#39;&#125;
r &#x3D; requests.get(url, headers&#x3D;headers)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></p>
<p>Post请求<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">payload &#x3D; &#123;&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;&#125;
r &#x3D; requests.post(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;post&quot;, data&#x3D;payload)
print(r.text)
# 可以为 data 参数传入一个元组列表
# 在表单中多个元素使用同一 key 的时候，这种方式尤其有效：
payload &#x3D; ((&#39;key1&#39;, &#39;value1&#39;), (&#39;key1&#39;, &#39;value2&#39;))
r &#x3D; requests.post(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;post&#39;, data&#x3D;payload)
print(r.text)
&#123;
  ...
  &quot;form&quot;: &#123;
    &quot;key1&quot;: [
      &quot;value1&quot;,
      &quot;value2&quot;
    ]
  &#125;,
  ...
&#125;
# post的为json对象
url &#x3D; &#39;https:&#x2F;&#x2F;api.github.com&#x2F;some&#x2F;endpoint&#39;
payload &#x3D; &#123;&#39;some&#39;: &#39;data&#39;&#125;
r &#x3D; requests.post(url, json&#x3D;payload)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p>
<p>超时设置：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">requests.get(&#39;http:&#x2F;&#x2F;github.com&#39;, timeout&#x3D;0.001)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h2 id="二-响应内容"><a href="#二-响应内容" class="headerlink" title="二.响应内容"></a>二.响应内容</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">import requests
r &#x3D; requests.get(&#39;https:&#x2F;&#x2F;github.com&#x2F;timeline.json&#39;)
r.encoding&#x3D;&#39;utf-8&#39;
r.text
# [&#123;&quot;repository&quot;:&#123;&quot;open_issues&quot;:0,&quot;url&quot;:&quot;https:&#x2F;&#x2F;github.com&#x2F;...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>网页乱码问题:<br><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 查看网页编码
print(res.apparent_encoding)
# 设置编码
res.encoding &#x3D; &#39;GB2312&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></p>
<p>二进制响应内容(r.content)<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">from PIL import Image
from io import BytesIO
#BytesIO用于操作内存中的二进制数据
img&#x3D;Image.open(BytesIO(r.content))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></p>
<p>JSON响应内容（r.json()）<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">import requests
r &#x3D; requests.get(&#39;https:&#x2F;&#x2F;github.com&#x2F;timeline.json&#39;)
r.json()
# [&#123;u&#39;repository&#39;: &#123;u&#39;open_issues&#39;: 0, u&#39;url&#39;: &#39;https:&#x2F;&#x2F;github.com&#x2F;...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></p>
<p>响应状态码（r.status_code）<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">r &#x3D; requests.get(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;get&#39;)
r.status_code
200<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></p>
<p>响应头(r.headers)<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">r.headers
&#123;
    &#39;content-encoding&#39;: &#39;gzip&#39;,
    &#39;transfer-encoding&#39;: &#39;chunked&#39;,
    &#39;connection&#39;: &#39;close&#39;,
    &#39;server&#39;: &#39;nginx&#x2F;1.0.4&#39;,
    &#39;x-runtime&#39;: &#39;148ms&#39;,
    &#39;etag&#39;: &#39;&quot;e1ca502697e5c9317743dc078f67693f&quot;&#39;,
    &#39;content-type&#39;: &#39;application&#x2F;json&#39;
&#125;
r.headers[&#39;Content-Type&#39;]
&#39;application&#x2F;json&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p>
<h2 id="三-Cookies"><a href="#三-Cookies" class="headerlink" title="三.Cookies"></a>三.Cookies</h2><p>如果某个响应中包含一些 cookie，你可以快速访问它们：<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">url &#x3D; &#39;http:&#x2F;&#x2F;example.com&#x2F;some&#x2F;cookie&#x2F;setting&#x2F;url&#39;
r &#x3D; requests.get(url)

r.cookies[&#39;example_cookie_name&#39;]
# &#39;example_cookie_value&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></p>
<p>要想发送你的cookies到服务器，可以使用 cookies 参数：<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">url &#x3D; &#39;http:&#x2F;&#x2F;httpbin.org&#x2F;cookies&#39;
cookies &#x3D; dict(cookies_are&#x3D;&#39;working&#39;)

r &#x3D; requests.get(url, cookies&#x3D;cookies)
r.text
# &#39;&#123;&quot;cookies&quot;: &#123;&quot;cookies_are&quot;: &quot;working&quot;&#125;&#125;&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p>
<h2 id="四-会话"><a href="#四-会话" class="headerlink" title="四.会话"></a>四.会话</h2><p>requests.Session()这样可以在会话中保留状态，保持cookie等<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">import requests
s &#x3D; requests.Session()
s.headers.update(&#123;&#39;x-test&#39;: &#39;true&#39;&#125;)
r &#x3D; s.get(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;headers&#39;, headers&#x3D;&#123;&#39;x-test2&#39;: &#39;true&#39;&#125;)
print(r.text)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></p>
<h2 id="五-代理"><a href="#五-代理" class="headerlink" title="五.代理"></a>五.代理</h2><p>如果需要使用代理，你可以通过为任意请求方法提供 proxies 参数来配置单个请求<br><pre class="line-numbers language-python" data-language="python"><code class="language-python"># http代理
import requests
proxies &#x3D; &#123;
  &quot;https&quot;: &quot;http:&#x2F;&#x2F;41.118.132.69:4433&quot;
&#125;
r &#x3D; requests.post(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;post&quot;, proxies&#x3D;proxies)
# socks代理
proxies &#x3D; &#123;
    &#39;http&#39;: &#39;socks5:&#x2F;&#x2F;user:pass@host:port&#39;,
    &#39;https&#39;: &#39;socks5:&#x2F;&#x2F;user:pass@host:port&#39;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p>
<h2 id="六-Prepared-Request"><a href="#六-Prepared-Request" class="headerlink" title="六.Prepared Request"></a>六.Prepared Request</h2><p>构造requests.Request对象，将Request对象作为参数传入requests.Session()对象的prepare_request()方法中，最后通过Session对象的send()方法发送请求。<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">import requests
from requests import Request
url &#x3D; &#39;http:&#x2F;&#x2F;httpbin.org&#x2F;get&#39;
# 创建Session对象
s &#x3D; requests.Session()
# 构造Request对象
req &#x3D; Request(&#39;GET&#39;,url)
# 将Request对象转换成 PreparedRequest对象
prepped &#x3D; s.prepare_request(req)
# 利用Session对象的send()方法，发送PreparedRequest对象
res &#x3D; s.send(prepped)
print(res.text)
print(type(prepped))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></p>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫之selenium自动化</title>
    <url>/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB%E4%B9%8Bselenium%E8%87%AA%E5%8A%A8%E5%8C%96/</url>
    <content><![CDATA[<h2 id="1-基础操作"><a href="#1-基础操作" class="headerlink" title="1.基础操作"></a>1.基础操作</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">from selenium import webdriver
import time
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

if __name__ &#x3D;&#x3D; &#39;__main__&#39;:
    # 谷歌浏览器驱动
    chromedriver_path &#x3D; &#39;chromedriver72.exe&#39;
    options &#x3D; webdriver.ChromeOptions()
    # 躲避部分网站selenium检测
    options.add_experimental_option(&#39;excludeSwitches&#39;, [&#39;enable-automation&#39;])
    options.add_experimental_option(&quot;useAutomationExtension&quot;, False)

    driver &#x3D; webdriver.Chrome(executable_path&#x3D;chromedriver_path, options&#x3D;options)

    # 躲避部分网站selenium检测
    script &#x3D; &quot;Object.defineProperty(navigator, &#39;webdriver&#39;, &#123;get: () &#x3D;&gt; undefined&#125;);&quot;
    driver.execute_cdp_cmd(&quot;Page.addScriptToEvaluateOnNewDocument&quot;, &#123;&quot;source&quot;: script&#125;)

    # 浏览器最大化
    driver.maximize_window()

    url &#x3D; &#39;https:&#x2F;&#x2F;www.python.org&#x2F;&#39;
    driver.get(url)
    # 显式等待
    wait &#x3D; WebDriverWait(driver, 20, 1)

    # 在主页输入框搜索requests，并点击搜索
    input_ &#x3D; wait.until(EC.presence_of_element_located((By.ID, &#39;id-search-field&#39;)))
    input_.send_keys(&#39;requests&#39;)
    time.sleep(1)
    btn &#x3D; driver.find_element_by_xpath(&#39;&#x2F;&#x2F;button[@title&#x3D;&quot;Submit this Search&quot;]&#39;)
    btn.click()
    time.sleep(10)
    driver.close()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="2-元素定位"><a href="#2-元素定位" class="headerlink" title="2.元素定位"></a>2.元素定位</h2><h4 id="查找单个元素"><a href="#查找单个元素" class="headerlink" title="查找单个元素"></a>查找单个元素</h4><p>最常用的定位元素的两个方法是通过Xpath和id来定位。</p>
<ul>
<li>find_element_by_id</li>
<li>find_element_by_xpath</li>
</ul>
<h4 id="查找多个元素"><a href="#查找多个元素" class="headerlink" title="查找多个元素"></a>查找多个元素</h4><ul>
<li>find_elements_by_xpath</li>
<li>find_elements_by_name</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 通过xpath查找元素
driver.find_element_by_xpath(&#39;&#x2F;&#x2F;button[@title&#x3D;&quot;Submit this Search&quot;]&#39;)
# 通过id查找元素
driver.find_element_by_id(&#39;id-search-field&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="获取元素内部内容"><a href="#获取元素内部内容" class="headerlink" title="获取元素内部内容"></a>获取元素内部内容</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">text &#x3D; driver.find_element_by_xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;ISDCaptcha&quot;]&#x2F;div[2]&#x2F;div&#39;).get_attribute(&#39;innerHTML&#39;)
if &#39;请绘制图中手势&#39; in text:
    print(&#39;出现行为认证&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h4 id="获取元素指定属性的属性值"><a href="#获取元素指定属性的属性值" class="headerlink" title="获取元素指定属性的属性值"></a>获取元素指定属性的属性值</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">driver.find_element_by_xpath(&#39;&#x2F;&#x2F;div[@id&#x3D;&quot;find-step3-phone&quot;]&#39;).get_attribute(&#39;style&#39;)
driver.find_element_by_xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;imgVerifyCodeP&quot;]&#39;).get_attribute(&#39;src&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h4 id="切换到指定iframe"><a href="#切换到指定iframe" class="headerlink" title="切换到指定iframe"></a>切换到指定iframe</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 通过id或者名称
driver.switch_to.frame(&quot;iframeLoginIfm&quot;)

driver.switch_to.frame(0)

frame &#x3D; driver.find_element_by_xpath(&#39;&#x2F;&#x2F;div[@id&#x3D;&quot;loginDiv&quot;]&#x2F;iframe&#39;)
driver.switch_to.frame(frame)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="切换到指定窗口"><a href="#切换到指定窗口" class="headerlink" title="切换到指定窗口"></a>切换到指定窗口</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">driver.switch_to.window(browser.window_handles[1])<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h4 id="切换到alert弹窗"><a href="#切换到alert弹窗" class="headerlink" title="切换到alert弹窗"></a>切换到alert弹窗</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">text &#x3D; driver.switch_to.alert.text
if &#39;图片验证码输入错误&#39; in text:
    print(&#39;图片验证码识别错误&#39;)
    driver.switch_to.alert.accept()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="3-元素交互"><a href="#3-元素交互" class="headerlink" title="3.元素交互"></a>3.元素交互</h2><h4 id="按钮点击"><a href="#按钮点击" class="headerlink" title="按钮点击"></a>按钮点击</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">btn &#x3D; driver.find_element_by_xpath(&#39;&#x2F;&#x2F;div[@role&#x3D;&quot;button&quot;]&#x2F;div&#x2F;span&#x2F;span&#39;)
btn.click()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h4 id="执行js代码"><a href="#执行js代码" class="headerlink" title="执行js代码"></a>执行js代码</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">style_ &#x3D; driver.find_element_by_xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;passport-login-pop&quot;]&#39;).get_attribute(&#39;style&#39;)
style_ &#x3D; style_.replace(&#39;display: none;&#39;, &#39;&#39;)
if not style_:
    style_ &#x3D; &#39;left: 259px; top: 212px; z-index: 60001;&#39;
js &#x3D; &#39;document.getElementById(&quot;passport-login-pop&quot;).setAttribute(&quot;style&quot;,&quot;&#123;&#125;&quot;);&#39;.format(style_)
driver.execute_script(js)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="表单输入"><a href="#表单输入" class="headerlink" title="表单输入"></a>表单输入</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">input_ &#x3D; driver.find_element_by_xpath(&#39;&#x2F;&#x2F;input[@name&#x3D;&quot;session[password]&quot; and @dir&#x3D;&quot;auto&quot;]&#39;)
input_.send_keys(&#39;123qwe&#39;)

from selenium.webdriver.common.keys import Keys
input_.send_keys(Keys.BACK_SPACE)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="页面滚动"><a href="#页面滚动" class="headerlink" title="页面滚动"></a>页面滚动</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">driver.execute_script(&quot;&quot;&quot;
                (function () &#123;
                    var y &#x3D; document.body.scrollTop;
                    var step &#x3D; 100;
                    window.scroll(0, y);
                    function f() &#123;
                        if (y &lt; document.body.scrollHeight) &#123;
                            y +&#x3D; step;
                            window.scroll(0, y);
                            setTimeout(f, 50);
                        &#125;
                        else &#123;
                            window.scroll(0, y);
                            document.title +&#x3D; &quot;scroll-done&quot;;
                        &#125;
                    &#125;
                    setTimeout(f, 1000);
                &#125;)();
                &quot;&quot;&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="模拟拖动"><a href="#模拟拖动" class="headerlink" title="模拟拖动"></a>模拟拖动</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">from selenium.webdriver.common.action_chains import ActionChains

def get_track(self, distance):
    track &#x3D; []
    current &#x3D; 0
    mid &#x3D; distance * 3 &#x2F; 4
    t &#x3D; 0.2
    v &#x3D; 0
    while current &lt; distance:
        if current &lt; mid:
            a &#x3D; 2
        else:
            a &#x3D; -3
        v0 &#x3D; v
        v &#x3D; v0 + a * t
        move &#x3D; v0 * t + 1 &#x2F; 2 * a * t * t
        current +&#x3D; move
        track.append(round(move))
    return track

# 模拟拖动
btn &#x3D; wait.until(EC.presence_of_element_located((By.XPATH, xpath_)))
track &#x3D; get_track(500)
action &#x3D; ActionChains(browser)
action.click_and_hold(btn).perform()
action.reset_actions()
for i in track:
    action.move_by_offset(xoffset&#x3D;i, yoffset&#x3D;0).perform()
    action.reset_actions()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="4-等待"><a href="#4-等待" class="headerlink" title="4.等待"></a>4.等待</h2><h4 id="显式等待"><a href="#显式等待" class="headerlink" title="显式等待"></a>显式等待</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 显式等待
wait &#x3D; WebDriverWait(driver, 20, 1)

input_ &#x3D; wait.until(EC.presence_of_element_located((By.ID, &#39;id-search-field&#39;)))
input_.send_keys(&#39;requests&#39;)
time.sleep(1)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="隐式等待"><a href="#隐式等待" class="headerlink" title="隐式等待"></a>隐式等待</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">from selenium import webdriver

driver &#x3D; webdriver.Chrome()
# 隐式等待
driver.implicitly_wait(10)
driver.get(&#39;https:&#x2F;&#x2F;www.zhihu.com&#x2F;explore&#39;)
logo &#x3D; driver.find_element_by_id(&#39;zh-top-link-logo&#39;)
print(logo)
driver.close()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="5-其他操作"><a href="#5-其他操作" class="headerlink" title="5.其他操作"></a>5.其他操作</h2><h4 id="解决页面加载时间过长问题"><a href="#解决页面加载时间过长问题" class="headerlink" title="解决页面加载时间过长问题"></a>解决页面加载时间过长问题</h4><p>有时候页面有些静态文件加载比较耗时，selenium可以不需要等待页面全部加载完全在去查找元素<br><pre class="line-numbers language-python" data-language="python"><code class="language-python">options &#x3D; webdriver.ChromeOptions()
# 解决页面加载阻塞问题
options.set_capability(&#39;pageLoadStrategy&#39;, &#39;none&#39;)
driver &#x3D; webdriver.Chrome(executable_path&#x3D;self.chromedriver_path, options&#x3D;options)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></p>
<h4 id="添加请求头"><a href="#添加请求头" class="headerlink" title="添加请求头"></a>添加请求头</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">options.add_argument(&quot;user-agent&#x3D;&#123;&#125;&quot;.format(&#39;Mozilla&#x2F;5.0 (Windows NT 10.0; WOW64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.100 Safari&#x2F;537.36&#39;))<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h4 id="添加代理"><a href="#添加代理" class="headerlink" title="添加代理"></a>添加代理</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">socks5 &#x3D; &quot;socks5:&#x2F;&#x2F;&#123;&#125;:&#123;&#125;&quot;.format(socks5_proxy_ip, socks5_proxy_port)
options.add_argument(&quot;--proxy-server&#x3D;&#123;&#125;&quot;.format(socks5))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h4 id="捕捉F12控制台中所有请求记录"><a href="#捕捉F12控制台中所有请求记录" class="headerlink" title="捕捉F12控制台中所有请求记录"></a>捕捉F12控制台中所有请求记录</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

d &#x3D; DesiredCapabilities.CHROME
d[&#39;loggingPrefs&#39;] &#x3D; &#123;&#39;performance&#39;: &#39;ALL&#39;&#125;
d[&#39;goog:chromeOptions&#39;] &#x3D; &#123;
    &#39;perfLoggingPrefs&#39;: &#123;
        &#39;enableNetwork&#39;: True,
    &#125;,
    &#39;w3c&#39;: False,
&#125;
options.add_experimental_option(&#39;perfLoggingPrefs&#39;, &#123;&#39;enableNetwork&#39;: True&#125;)
options.add_experimental_option(&#39;w3c&#39;, False)
driver &#x3D; webdriver.Chrome(executable_path&#x3D;self.chromedriver_path, options&#x3D;options, desired_capabilities&#x3D;d)

# 保存log
log_list &#x3D; []
for entry in driver.get_log(&#39;performance&#39;):
    log_list.append(entry)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="屏幕截图-可以截取图片验证码加以识别"><a href="#屏幕截图-可以截取图片验证码加以识别" class="headerlink" title="屏幕截图,可以截取图片验证码加以识别"></a>屏幕截图,可以截取图片验证码加以识别</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">import win32con
import win32gui
import win32print
from win32api import GetSystemMetrics
from PIL import Image

def get_real_resolution():
    &quot;&quot;&quot;获取真实的分辨率&quot;&quot;&quot;
    hDC &#x3D; win32gui.GetDC(0)
    # 横向分辨率
    w &#x3D; win32print.GetDeviceCaps(hDC, win32con.DESKTOPHORZRES)
    # 纵向分辨率
    h &#x3D; win32print.GetDeviceCaps(hDC, win32con.DESKTOPVERTRES)
    return w, h

def get_screen_size():
    &quot;&quot;&quot;获取缩放后的分辨率&quot;&quot;&quot;
    w &#x3D; GetSystemMetrics(0)
    h &#x3D; GetSystemMetrics(1)
    return w, h

real_resolution &#x3D; get_real_resolution()
screen_size &#x3D; get_screen_size()
screen_scale_rate &#x3D; round(real_resolution[0] &#x2F; screen_size[0], 2)

pic_name &#x3D; &#39;***.png&#39;
driver.save_screenshot(pic_name)
# 找到图片验证码元素
element &#x3D; driver.find_element_by_xpath(xpath_)
left &#x3D; element.location[&#39;x&#39;] * screen_scale_rate
top &#x3D; element.location[&#39;y&#39;] * screen_scale_rate
right &#x3D; (element.location[&#39;x&#39;] + element.size[&#39;width&#39;]) * screen_scale_rate
bottom &#x3D; (element.location[&#39;y&#39;] + element.size[&#39;height&#39;]) * screen_scale_rate
im &#x3D; Image.open(pic_name)
# 裁剪图片
im &#x3D; im.crop((left, top, right, bottom))
im.save(pic_name)
# 把图片转成base64,利用打码平台接口识别
with open(pic_name, &#39;rb&#39;) as f:
    code_img_base64 &#x3D; base64.b64encode(f.read()).decode()
os.remove(pic_name)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title>Python爬虫之构建自己的代理IP池</title>
    <url>/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB%E4%B9%8B%E6%9E%84%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E4%BB%A3%E7%90%86IP%E6%B1%A0/</url>
    <content><![CDATA[<h1 id="Python爬虫—代理池维护"><a href="#Python爬虫—代理池维护" class="headerlink" title="Python爬虫—代理池维护"></a>Python爬虫—代理池维护</h1><h2 id="大致思路"><a href="#大致思路" class="headerlink" title="大致思路"></a>大致思路</h2><ol>
<li>去代理网站上爬取大量代理IP，并将其存储在redis数据库。</li>
<li>定时获取redis中的所有代理IP，检测每一个代理IP是否可用。</li>
<li>通过flask，对外提供获取代理IP的接口，如果想要使用代理池中的代理IP，只需要访问我们提供的接口即可。<br><strong>现在网上免费代理IP网站越来越少，而且免费的代理质量非常不稳定，本文只是提供构建代理IP池的一种思路，实战的话还是要想办法获取优质的代理。</strong></li>
</ol>
<h2 id="代理池系统具体实现思路"><a href="#代理池系统具体实现思路" class="headerlink" title="代理池系统具体实现思路"></a>代理池系统具体实现思路</h2><p><img src="../pic/spiders/proxypool.png" alt=""></p>
<h2 id="代理池完整代码"><a href="#代理池完整代码" class="headerlink" title="代理池完整代码"></a><a href="agent_pool">代理池完整代码</a></h2><h4 id="agent-pool-py-整体流程"><a href="#agent-pool-py-整体流程" class="headerlink" title="agent_pool.py 整体流程"></a>agent_pool.py 整体流程</h4><p>存储模块：主要实现的功能是，去一些免费代理网站爬取大量的代理IP，并存储至redis数据库中。redis的Sorted Set结构是一个有序集合，我们会对每一个爬取到的代理IP<br>设置一个初始化的优先级10，Sorted Set也是通过这个优先级来进行排序的。&lt;/br&gt;</p>
<ul>
<li>Getter:爬取代理网站的免费代理IP，存入redis</li>
<li>Tester:从redis中取出代理，测试代理是否可用，并调整代理IP的优先级</li>
<li>Controller:启动Getter()与Tester()</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from Crawler import Crawler
from RedisClient import RedisClient
import traceback
import time
import requests
import multiprocessing
from concurrent import futures

FULL_COUNT &#x3D; 2000

class Getter(object):
    # 爬取代理网站的免费代理IP，存入redis
    def __init__(self):
        self.redis_client &#x3D; RedisClient()
        self.crawler &#x3D; Crawler()

    def is_full(self):
        # 判断代理池是否满了
        return self.redis_client.get_proxy_count() &gt;&#x3D; FULL_COUNT

    def run(self):
        # 将爬取到的代理存入redis
        if not self.is_full():
            proxys &#x3D; self.crawler.get_crawler_proxy()
            for proxy in proxys:
                self.redis_client.add(proxy)

class Tester(object):
    # 从redis中取出代理，测试代理是否可用，并调整代理IP的优先级
    def __init__(self, test_url):
        self.redisdb &#x3D; RedisClient()
        # 用来测试代理是否可用的地址
        self.test_url &#x3D; test_url

    def test_proxy(self, proxy):
        try:
            if isinstance(proxy, bytes):
                proxy &#x3D; proxy.decode(&#39;utf-8&#39;)
            proxies &#x3D; &#123;
                &#39;http&#39;: &#39;http:&#x2F;&#x2F;&#39; + proxy,
                &#39;https&#39;: &#39;https:&#x2F;&#x2F;&#39; + proxy
            &#125;
            print(&#39;正在检测:&#123;&#125;&#39;.format(proxy))
            res &#x3D; requests.get(self.test_url, proxies&#x3D;proxies, timeout&#x3D;10)
            if res.status_code &#x3D;&#x3D; 200:
                return True, proxy
            else:
                return False, proxy
                # 代理不可用，就降低其优先级
        except Exception as e:
            return False, proxy
            # print(&#39;代理检测异常:&#123;&#125;  &#123;&#125;&#39;.format(proxy, e))
            self.redisdb.decrease(proxy)
            print(&#39;代理不可用:&#123;&#125;&#39;.format(proxy))


    def run(self):
        print(&#39;启动检测模块......&#39;)
        try:
            # 获取redis中所有爬取到的代理
            proxies &#x3D; self.redisdb.get_all_proxy()
            for i in range(0, len(proxies), 50):
                test_proxies &#x3D; proxies[i:i+50]
                workers &#x3D; len(test_proxies)
                with futures.ThreadPoolExecutor(workers) as executor:
                    tasks_res &#x3D; executor.map(self.test_proxy, test_proxies)
                    for res, proxy in tasks_res:
                        if not res:
                            # 代理不可用，就降低其优先级
                            self.redisdb.decrease(proxy)
                            print(&#39;代理不可用:&#123;&#125;&#39;.format(proxy))
                        else:
                            # 代理可用,将其优先级置为最大
                            self.redisdb.max(proxy)
                            print(&#39;代理可用:&#123;&#125;&#39;.format(proxy))

        except Exception as e:
            print(traceback.format_exc())
            print(&#39;检测模块出错！！！&#39;)

class Controller(object):
    def control_get(self):
        # 获取功能：爬取代理网站，将代理存储到redis
        getter &#x3D; Getter()
        while True:
            try:
                getter.run()
            except:
                print(traceback.format_exc())
            time.sleep(30)

    def control_test(self):
        # 检测功能，检测redis中的代理是否可用
        tester &#x3D; Tester(test_url&#x3D;&#39;http:&#x2F;&#x2F;www.baidu.com&#39;)
        while True:
            try:
                tester.run()
            except:
                print(traceback.format_exc())
            time.sleep(30)

    def run(self):
        print(&#39;代理池开始运行了......&#39;)
        # 两个进程
        get &#x3D; multiprocessing.Process(target&#x3D;self.control_get)
        get.start()
        test &#x3D; multiprocessing.Process(target&#x3D;self.control_test)
        test.start()

if __name__ &#x3D;&#x3D; &#39;__main__&#39;:
    control &#x3D; Controller()
    control.run()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="WebAPI-to-get-proxy-py-通过flask向外提供获取代理IP的接口"><a href="#WebAPI-to-get-proxy-py-通过flask向外提供获取代理IP的接口" class="headerlink" title="WebAPI_to_get_proxy.py 通过flask向外提供获取代理IP的接口"></a>WebAPI_to_get_proxy.py 通过flask向外提供获取代理IP的接口</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">from flask import Flask, g
import RedisClient

&quot;&quot;&quot;
    对外提供web接口，通过提供的web接口，来获取redis中的代理
    g是上下文对象，处理请求时，用于临时存储的对象，每次请求都会重设这个变量。比如：我们可以获取一些临时请求的用户信息。
&quot;&quot;&quot;


app &#x3D; Flask(__name__)


@app.route(&#39;&#x2F;&#39;)
def index():
    return &#39;&lt;h2&gt;欢迎来到daacheng代理池系统&lt;&#x2F;h2&gt;&#39;


def get():
    if not hasattr(g, &#39;redis&#39;):
        g.redis &#x3D; RedisClient.RedisClient()
    return g.redis


@app.route(&#39;&#x2F;random&#39;)
def get_random_proxy():
    # 从代理池中返回一个代理
    redisdb &#x3D; get()
    return redisdb.get_proxy()


@app.route(&#39;&#x2F;count&#39;)
def count():
    # 查询代理池中代理的个数
    redisdb &#x3D; get()
    return str(redisdb.get_proxy_count())


@app.route(&#39;&#x2F;all&#39;)
def get_all():
    # 查询代理池中代理的个数
    redisdb &#x3D; get()
    return str(redisdb.get_all_proxy())


if __name__ &#x3D;&#x3D; &#39;__main__&#39;:
    app.run(host&#x3D;&#39;0.0.0.0&#39;, port&#x3D;5000)
    app.debug &#x3D; True
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title>爬虫概念</title>
    <url>/%E7%88%AC%E8%99%AB/Python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<h2 id="1-robots协议"><a href="#1-robots协议" class="headerlink" title="1.robots协议"></a>1.robots协议</h2><p>也叫robots.txt，是存放在网站根目录下的文本文件，用来告诉搜索引擎该网站哪些内容是不应该被抓取的，哪些是可以抓取的。</p>
<p>如<a href="https://www.csdn.net/robots.txt">https://www.csdn.net/robots.txt</a></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">User-agent: *
Disallow: &#x2F;scripts
Disallow: &#x2F;public
Disallow: &#x2F;css&#x2F;
Disallow: &#x2F;images&#x2F;
Disallow: &#x2F;content&#x2F;
Disallow: &#x2F;ui&#x2F;
Disallow: &#x2F;js&#x2F;
Disallow: &#x2F;scripts&#x2F;
Disallow: &#x2F;article_preview.html*
Disallow: &#x2F;tag&#x2F;
Disallow: &#x2F;*?*
Disallow: &#x2F;link&#x2F;

Sitemap: https:&#x2F;&#x2F;www.csdn.net&#x2F;sitemap-aggpage-index.xml
Sitemap: https:&#x2F;&#x2F;www.csdn.net&#x2F;article&#x2F;sitemap.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="2-常见的反爬虫措施"><a href="#2-常见的反爬虫措施" class="headerlink" title="2.常见的反爬虫措施"></a>2.常见的反爬虫措施</h2><h4 id="1-请求头校验"><a href="#1-请求头校验" class="headerlink" title="1.请求头校验"></a>1.请求头校验</h4><p>一般网站会对请求头进行校验，比如Host，UA，Content-Type字段等，模拟请求的时候，这些常见的请求头最好是带上。</p>
<h4 id="2-IP访问次数控制"><a href="#2-IP访问次数控制" class="headerlink" title="2.IP访问次数控制"></a>2.IP访问次数控制</h4><p>同一个IP地址短时间内大量发起请求，会引起IP限制，解决方法是用代理IP，或者构建自己的代理IP池。</p>
<h4 id="3-接口请求频率限制"><a href="#3-接口请求频率限制" class="headerlink" title="3.接口请求频率限制"></a>3.接口请求频率限制</h4><p>有的网站会控制接口访问的频率，比如有些查询接口，控制两三秒访问一次。</p>
<h4 id="4-接口访问次数限制"><a href="#4-接口访问次数限制" class="headerlink" title="4.接口访问次数限制"></a>4.接口访问次数限制</h4><p>每天限制某个IP或账号访问接口的次数，达到上限后出现二次验证或者直接封账号/IP.比如登录接口</p>
<h4 id="5-行为认证"><a href="#5-行为认证" class="headerlink" title="5.行为认证"></a>5.行为认证</h4><p>请求次数过多会出现人工认证，如图片验证码，滑动认证，点击认证等，可以对接打码平台。</p>
<h4 id="6，自动化环境检测"><a href="#6，自动化环境检测" class="headerlink" title="6，自动化环境检测"></a>6，自动化环境检测</h4><p>selenium自动化工具有的网站会检测出来，大部分可以通过下面两种方式跳过检测,下面两种方式无法处理的话，还可以尝试把页面改为移动端页面(手机模式)，最后还有一种方法就是代理服务器拦截修改js代码，把检测selenium的js修改掉。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">options &#x3D; webdriver.ChromeOptions()
# 躲避部分网站selenium检测
options.add_experimental_option(&#39;excludeSwitches&#39;, [&#39;enable-automation&#39;])
options.add_experimental_option(&quot;useAutomationExtension&quot;, False)

driver &#x3D; webdriver.Chrome(executable_path&#x3D;chromedriver_path, options&#x3D;options)

# 躲避部分网站selenium检测
script &#x3D; &quot;Object.defineProperty(navigator, &#39;webdriver&#39;, &#123;get: () &#x3D;&gt; undefined&#125;);&quot;
driver.execute_cdp_cmd(&quot;Page.addScriptToEvaluateOnNewDocument&quot;, &#123;&quot;source&quot;: script&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>对于移动端appium的检测，可以尝试替换为uiautomator2实现自动化</p>
<h4 id="7-数据动态加载"><a href="#7-数据动态加载" class="headerlink" title="7.数据动态加载"></a>7.数据动态加载</h4><p>有的数据不是通过html页面的接口请求返回的，抓包分析请求，找到正确的数据接口。</p>
<h4 id="8-请求参数加密"><a href="#8-请求参数加密" class="headerlink" title="8.请求参数加密"></a>8.请求参数加密</h4><p>网易云音乐的post请求的请求体就是前端经过js加密后计算得到的，需要逆向js代码</p>
<h4 id="9-返回数据加密"><a href="#9-返回数据加密" class="headerlink" title="9.返回数据加密"></a>9.返回数据加密</h4><p>需要逆向js代码，分析如何解密。还有一种像大众点评的评论，需要通过定位去找到文本。</p>
<h4 id="10-动态更新cookies"><a href="#10-动态更新cookies" class="headerlink" title="10.动态更新cookies"></a>10.动态更新cookies</h4><p>华为手机云服务，每次请求接口都会重新设置cookies，并且请求头参数也需要跟着cookies一起变化</p>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title>jupyter_notebook常用插件介绍</title>
    <url>/others/jupyter-notebook/jupyter-notebook%E5%B8%B8%E7%94%A8%E6%8F%92%E4%BB%B6%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<h2 id="Jupyter-NbExtensions-Configurator"><a href="#Jupyter-NbExtensions-Configurator" class="headerlink" title="Jupyter NbExtensions Configurator"></a>Jupyter NbExtensions Configurator</h2><p>Jupyter NbExtensions Configurator 是Jupyter Notebook的一个扩展工具，它提供了一系列标签，只需勾选相应插件就能自动载入。里面的插件能帮助减少工作量，书写更优雅的代码和更好的展示结构。</p>
<h3 id="安装Jupyter-NbExtensions-Configurator"><a href="#安装Jupyter-NbExtensions-Configurator" class="headerlink" title="安装Jupyter NbExtensions Configurator"></a>安装Jupyter NbExtensions Configurator</h3><p>用pip安装（conda应该同理）</p>
<p>Note:最好在关了jupyter的时候安装，不过问题不大</p>
<pre class="line-numbers language-none"><code class="language-none">pip install jupyter_nbextensions_configurator --user
pip install jupyter_contrib_nbextensions --user

jupyter contrib nbextension install --user

jupyter nbextensions_configurator enable --user<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>安装完毕，进入Jupyter Notebook，在主界面会多出一个NbExtensions的标签，里面有很多插件可供选择，示意图如下：</p>
<p><img src="https://img2018.cnblogs.com/blog/1814900/201909/1814900-20190925203501494-1338913967.png" alt="img"></p>
<p> 下面会重点介绍几个插件的用法，其余的不详细介绍，有兴趣的可以到官网（<a href="https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/index.html）自行了解。">https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/index.html）自行了解。</a></p>
<h3 id="Hinterland"><a href="#Hinterland" class="headerlink" title="Hinterland"></a>Hinterland</h3><p>勾选此插件为代码单元格中的每次按键启用“代码自动补全”菜单，而不是仅用Tab键时启用。</p>
<h3 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h3><p>这个插件会根据Markdown的标题层次形成一个目录，可以通过点击目录，直接定位到对应代码位置，在长代码文件中能起到导航的作用。</p>
<p><img src="https://img2018.cnblogs.com/blog/1814900/201909/1814900-20190925205908910-259935432.png" alt="img"></p>
<p> 开启插件后，会在工具栏多出一个按钮，可通过点击按钮选择是否开启（如上图），官方示例如下</p>
<p><img src="https://img2018.cnblogs.com/blog/1814900/201909/1814900-20190925205346881-634502374.gif" alt="img"></p>
<h3 id="Snippets-Menu"><a href="#Snippets-Menu" class="headerlink" title="Snippets Menu"></a>Snippets Menu</h3><p>向Jupyter笔记本添加可定制的菜单项，以插入代码片段、样板文件和示例。</p>
<p>勾选此插件后，会多出一个Snippets的菜单项，菜单里包含多个模块的示例，通过简单的点击就能生成示例代码，可根据自己的需求稍作修改即可运行，减少代码工作量。</p>
<p><img src="https://img2018.cnblogs.com/blog/1814900/201909/1814900-20190925210916560-990218179.png" alt="img"></p>
<h3 id="Codefolding"><a href="#Codefolding" class="headerlink" title="Codefolding"></a>Codefolding</h3><p>这个扩展将代码折叠功能从CodeMirror添加到codecell。</p>
<p>在编辑模式下，单击边距中的三角形(codecell的左边缘)或键入代码折叠热键(默认为Alt+F)，折叠代码。在命令模式下，折叠热键与编解码器的第一行有关。</p>
<p>折叠前：</p>
<p><img src="https://img2018.cnblogs.com/blog/1814900/201909/1814900-20190925211705294-581239589.png" alt="img"></p>
<p>折叠后是这样：</p>
<p> <img src="https://img2018.cnblogs.com/blog/1814900/201909/1814900-20190925211802722-968013526.png" alt="img"></p>
<p>或者是这样：</p>
<p> <img src="https://img2018.cnblogs.com/blog/1814900/201909/1814900-20190925211812161-1047071957.png" alt="img"></p>
<h3 id="Scratchpad-没添加"><a href="#Scratchpad-没添加" class="headerlink" title="Scratchpad(没添加)"></a>Scratchpad(没添加)</h3><p>为Jupyter Notebook提供一个草稿cell，方便随时测试输出。</p>
<p><img src="https://img2018.cnblogs.com/blog/1814900/201909/1814900-20190925213326635-1759656941.gif" alt="img"></p>
<h3 id="Notify（没添加"><a href="#Notify（没添加" class="headerlink" title="Notify（没添加"></a>Notify（没添加</h3><p>在内核繁忙一段时间后再次空闲时显示一个浏览器通知——繁忙0、5、10或30秒后可配置。</p>
<p>这个插件功能在你需要长时间跑一个代码时可启用，无需在页面等待，程序运行完成后，会弹出通知。</p>
<p><img src="https://img2018.cnblogs.com/blog/1814900/201909/1814900-20190925214041517-1148926022.png" alt="img"></p>
<h3 id="Collapsible-Headings"><a href="#Collapsible-Headings" class="headerlink" title="Collapsible Headings"></a>Collapsible Headings</h3><p>允许notebook有可折叠的部分，以标题分开。</p>
<p>任何标记标题单元格(即以1-6 #字符开头的单元格)在呈现后都是可折叠的。</p>
<p>标题的折叠/扩展状态存储在单元格元数据中，并在笔记本加载时重新加载。</p>
<p><img src="https://img2018.cnblogs.com/blog/1814900/201909/1814900-20190925214559004-2102669271.png" alt="img"></p>
<h3 id="Variableinspector（没添加"><a href="#Variableinspector（没添加" class="headerlink" title="Variableinspector（没添加"></a>Variableinspector（没添加</h3><p>Variableinspector（变量检查器）显示我们在Notebook中创建的所有变量的名称，以及它们的类型、大小、形状和值。如下图所示：</p>
<p><img src="https://pic3.zhimg.com/v2-afebf774b1d9e9869b78c218c57e319e_r.jpg" alt="img"></p>
<p>这个工具对于从RStudio迁移来的项目来说是非常宝贵的。或是当我们不想继续打印df.shape、无法回忆x的类型时，Variableinspector将变得非常有用。</p>
<p>如果有其他用到的插件会继续添加</p>
]]></content>
  </entry>
  <entry>
    <title>关于marp-theme-for-tju的探索</title>
    <url>/%E7%A2%8E%E7%A2%8E%E5%BF%B5/marp/%E5%85%B3%E4%BA%8Emarp-theme-for-tju%E7%9A%84%E6%8E%A2%E7%B4%A2/</url>
    <content><![CDATA[<p>事情的起因是，我无意间了解到marp，然后在github看到这样一个主题：<a href="https://github.com/chenyang1999/Marp_theme_for_THUslides">Marp_theme_for_THUslides</a>，于是我就想给本校做一个类似的主题，方便开组会时使用</p>
<p>但是经过很多尝试，我发现我的一些想法并不能实现，于是我暂时搁置，在解决这个需求的过程中，总结了一些marp的主题，主要是自用。</p>
<p>为了不让自己的时间白白浪费，我把自己有价值的一部分work，放到网络上，希望为后面探索的人节省时间。</p>
<p>放弃做marp-theme-tjuslides的主要原因就是：图片不能透明插入，会自动补白，这是我向rgb背景图插入透明校徽图的情况：</p>
<p><img src="https://raw.githubusercontent.com/ChangQingAAS/for_picgo/main/img/20210810203724.png" alt="image-20210810202422699" style="zoom: 67%;" /></p>
<p>这意味着，我使用的theme的颜色是受到限制的，一旦我使用别人的主题时，如果要加入相关元素就要自己改颜色。</p>
<p>或者我可以直接来几张背景图（不过这种情况下，由于一些遮拦，直接引用别人的样式文件可能会出问题，就需要花费时间调整一些样式语句了，）。虽然我觉得这样不够meaningful，但是我还是做了，毕竟也是用的上的，我用一些天大ppt,在<a href="https://github.com/ChangQingAAS/marp-themes/tree/main/tju-images">这里</a>整理了一些天大PPT背景图。</p>
<h2 id="我常用的样式："><a href="#我常用的样式：" class="headerlink" title="我常用的样式："></a>我常用的样式：</h2><hr>
<pre class="line-numbers language-none"><code class="language-none">---
marp: true

# 主题

theme: default

# 标题

title: TCP协议设计

# 页码，出现在右下角

paginate: True 

# 版本

version: 1.0.0 

# 页脚

# footer: 

# 页眉

header: 

# 大小，也可以写16:9

size: 4K

---

&lt;style&gt;

    section&#123; 

      background-image:url(&#39;.&#x2F;tju-images&#x2F;tju0.png&#39;);  

      background-size:cover;  

      position: absolute;  

    &#125;

    section h1 &#123;font-size:40px;color:black;margin-top:px;&#125;

    section h2 &#123;font-size:30px;color:black;margin-top:px;&#125;

    section p &#123;font-size: 25px;color:black;&#125;

    section table &#123;text-align: center;font-size: 32px;color:black;&#125;

    section a &#123;font-size: 25px;color:black;&#125;

    li &#123;font-size: 30px;text-align: left;&#125;


    img &#123;

        margin-left: auto; 

        margin-right:auto; 

        display:block;

        margin:0 auto;

        width:25cm;

      &#125;

&lt;&#x2F;style&gt;
---<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>正式使用了一次，ppt做出来是图片，啥都不能动，真的难顶。</p>
<p>不适合多人协作</p>
]]></content>
      <categories>
        <category>碎碎念</category>
      </categories>
      <tags>
        <tag>marp</tag>
        <tag>折腾</tag>
      </tags>
  </entry>
  <entry>
    <title>用Markdown制作幻灯片:Marp</title>
    <url>/%E6%95%99%E7%A8%8B/marp/%E7%94%A8Markdown%E5%88%B6%E4%BD%9C%E5%B9%BB%E7%81%AF%E7%89%87-Marp/</url>
    <content><![CDATA[<h2 id="特别鸣谢"><a href="#特别鸣谢" class="headerlink" title="特别鸣谢"></a>特别鸣谢</h2><p>本文对以下链接进行了整理：</p>
<ul>
<li><a href="https://www.lianxh.cn/news/97fccdca2d7a5.html">用Markdown制作幻灯片-五分钟学会Marp（上篇）-M110a| 连享会主页 (lianxh.cn)</a></li>
<li><a href="https://www.lianxh.cn/news/521900220dd33.html">用Markdown制作幻灯片-五分钟学会Marp（下篇）-M110b| 连享会主页 (lianxh.cn)</a></li>
<li><a href="https://www.mina.moe/archives/11046">【教程】你在用 Marp 时可能会用的语法 ——litble – MiNa!</a></li>
</ul>
<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1.前言"></a>1.前言</h2><p>​    在 Microsoft PowerPoint 里面，我们需要调整幻灯片的格式，操作起来些许麻烦。幻灯片通常是辅助演讲者的一个工具，这也是说，幻灯片不需要制作太复杂，简洁美观即可 </p>
<p>​    Markdown 文字排版高效，且风格简洁美观，是许多写作者的不二之选。而多数场景中， Markdown 的这种特点对幻灯片的制作也很合适。</p>
<p>​    那么，我们是否可以使用 Markdown 高效地制作一个漂亮的幻灯片？</p>
<p>​    答案就是：makedown + marp（based on vscode)</p>
<p>注：</p>
<ul>
<li><p>基于Markdown语法,复制黏贴笔记—-&gt;ppt/pdf/html/png</p>
</li>
<li><p>PPT 里面需要的各种文字效果可以用html,css,js等解决</p>
</li>
<li><p>可以用 latex直接写数学公式</p>
</li>
<li><p>可以画表格等(markdown 语法)  </p>
</li>
<li><p>可以直接插入 Emoji</p>
</li>
<li><p>在弹出保存页面时，可以自己选保存的格式</p>
</li>
</ul>
<h2 id="2-下载与安装"><a href="#2-下载与安装" class="headerlink" title="2.下载与安装"></a>2.下载与安装</h2><h3 id="vscode的安装方式"><a href="#vscode的安装方式" class="headerlink" title="vscode的安装方式"></a>vscode的安装方式</h3><p>在VS Code下载插件：marp for VS code 和 markdown  all in one</p>
<p><img src="https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig6_%20Marp%E6%8F%92%E4%BB%B6%E4%B8%8B%E8%BD%BD_%E5%AE%8B%E6%A3%AE%E5%AE%89.png" alt="img"></p>
<h3 id="其他安装方式"><a href="#其他安装方式" class="headerlink" title="其他安装方式"></a>其他安装方式</h3><p>还可以通过npm 安装 marp,读者可以自行探索</p>
<p>（不过在生成相关文件时，需要做一些命令行，vscode版的基本通过图形界面就可以解决）</p>
<h2 id="3-操作流程"><a href="#3-操作流程" class="headerlink" title="3.操作流程"></a>3.操作流程</h2><h3 id="3-1-新建文件"><a href="#3-1-新建文件" class="headerlink" title="3.1 新建文件"></a>3.1 新建文件</h3><p>打开 VS Code ，点击<code>文件-&gt;新建文件</code>，将其存为 <code>.md</code> 文件。此时，我们就创建好一个 Markdown 文档。</p>
<h3 id="3-2-幻灯片分页"><a href="#3-2-幻灯片分页" class="headerlink" title="3.2 幻灯片分页"></a>3.2 幻灯片分页</h3><p>首先，我们需要在 Markdown 文档开头标记 <code>marp: true</code>，以启用 Marp 功能。也就是告诉编辑器，你要制作的是幻灯片（Slide）。</p>
<p><strong>需要说明的有两点：</strong></p>
<ul>
<li>其一，<code>marp: true</code> 语句中的冒号为半角模式下录入的；上下的 <code>---</code> 不可省略；</li>
<li>其二，新建的 Markdown 文档也可以不做上述标记，通过点击菜单条中的 Marp 图标即可自动添加。具体步骤为：依次点击 <code>Marp 图标</code> → <code>Toggle Marp feature for current Markdown ( markdown.marp.toggleMarpFeature )</code>，软件会自动在文档开头添加 <code>marp: true</code>。如下图所示：</li>
</ul>
<p><img src="https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/20210602233115.png" alt="img"></p>
<p>文字信息如下：</p>
<pre class="line-numbers language-none"><code class="language-none">---
marp: true
---# Your slide deck

Start writing!<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><strong>幻灯片通常有多个页面，那么该如何分页？</strong></p>
<p>Markdown 通常用<code>---</code>作为水平分割线，而 Marp 则用 <code>---</code> 表示「分页符」，即用以「分割幻灯片」（Slide）。</p>
<p><em>演示效果如下：</em></p>
<p><img src="https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig9_%20%E5%88%86%E9%9A%94%E7%AC%A6_%E5%AE%8B%E6%A3%AE%E5%AE%89.gif" alt="img"></p>
<h3 id="3-3-幻灯片预览"><a href="#3-3-幻灯片预览" class="headerlink" title="3.3 幻灯片预览"></a>3.3 幻灯片预览</h3><p>点击 Marp 图标的右侧，我们就可以在 VS Code 编辑器的右侧实时预览幻灯片。</p>
<p><img src="https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig10_%20%E9%A2%84%E8%A7%88_%E5%AE%8B%E6%A3%AE%E5%AE%89.gif" alt="img"></p>
<h3 id="3-4-导出幻灯片"><a href="#3-4-导出幻灯片" class="headerlink" title="3.4 导出幻灯片"></a>3.4 导出幻灯片</h3><p>点击 <code>Marp 图标-&gt; Export slide deck ....( markdown.marp.export )</code>即可导出文件。</p>
<p><img src="https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig10_%20Marp%E9%80%89%E6%8B%A9%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F_%E5%AE%8B%E6%A3%AE%E5%AE%89.png" alt="img"></p>
<p>目前，<strong>Marp</strong> 可导出四种格式的文件：</p>
<ul>
<li><strong>HTML</strong></li>
<li><strong>PDF</strong></li>
<li><strong>PPTX</strong> (幻灯片)</li>
<li><strong>PNG</strong> (仅限于幻灯片的第一页)</li>
<li><strong>JPEG</strong> (仅限于幻灯片的第一页)</li>
</ul>
<p>需要注意的是：</p>
<ul>
<li><p>导出 <strong>HTML</strong> 格式的文档不需要安装任何插件，也可以很方便地转换为 PDF 格式。</p>
<ul>
<li>HTML 格式的幻灯片双击即可用默认浏览器打开，亦可右击文件，选择特定的浏览器打开 (建议用 Google 浏览器打开)。</li>
<li>展示时建议采用全屏播放模式：按快捷键 <strong>F11</strong> 即可；再次按下 <strong>F11</strong> 即可退出全屏。</li>
<li>用 Google 浏览器打开后，按快捷键 <strong>Ctrl+P</strong>，并选择「另存为 PDF」，即可把 HTML 格式的幻灯片转换为 PDF 格式。</li>
</ul>
</li>
<li><p>若需导出 PDF 和 PPTX 格式，可能需要安装 <code>pandoc</code></p>
</li>
</ul>
<h2 id="4-指令"><a href="#4-指令" class="headerlink" title="4.指令"></a>4.指令</h2><p>仅靠 Markdown 制作出的幻灯片格式可能会比较单调。为了制作出更加漂亮的幻灯片，我们还需要学习一种被称之为 <strong>指令（Directives）</strong> 的扩展语法。例如，指令 <code>theme</code> 可改变幻灯片的主题，<code>paginate</code> 可显示出幻灯片的页码，<code>footer</code> 用于设置幻灯片的页脚内容， <code>size</code> 可调整幻灯片的大小， <code>backgroundColor</code> 用于变换幻灯片的背景颜色等。</p>
<h3 id="4-1-指令的使用方法"><a href="#4-1-指令的使用方法" class="headerlink" title="4.1 指令的使用方法"></a>4.1 指令的使用方法</h3><p><strong>那么，如何使用这些指令呢？</strong></p>
<p>Marp 提供两种使用方法：</p>
<ul>
<li><p><strong>HTML comment</strong></p>
<p>这种需要在 <code>theme</code> 等指令前后添加<code>&lt;!-- --&gt;</code>。</p>
<pre class="line-numbers language-none"><code class="language-none">&lt;!--
theme: default
paginate: true
--&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
</li>
<li><p><strong>Front-matter</strong></p>
<p>第二种则是直接写在 Markdown 文档的开头（就是将指令和 <code>marp: ture</code> 写一块），此时无须再添加<code>&lt;!-- --&gt;</code>。</p>
<pre class="line-numbers language-none"><code class="language-none">---
marp: ture
theme: default
paginate: true
---<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ul>
<p> </p>
<h3 id="4-2-指令类型"><a href="#4-2-指令类型" class="headerlink" title="4.2 指令类型"></a>4.2 指令类型</h3><p>指令类型（ Type of directives ）可分为全局指令（ Global directives ）和局部指令（ Local directives ），有点类似于 Stata 中的宏。</p>
<p>其中，全局指令是整个幻灯片设定值，例如 <code>theme</code>、<code>headingDivider</code>、<code>style</code>。在全局指令前面添加前缀 <code>$</code>，就可以实现对整个幻灯片的设定。</p>
<p>而<strong>局部指令用以设置当前幻灯片页面以及后续页面</strong>。例如，我们用<code>&lt;!-- backgroundColor: aqua --&gt;</code> 设置幻灯片的背景颜色。</p>
<pre class="line-numbers language-none"><code class="language-none">&lt;!-- backgroundColor: aqua --&gt;

This page has aqua background.

---

The second page also has same color.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>幻灯片演示效果如下：</p>
<p><img src="https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig17_%20%E5%85%A8%E5%B1%80%E6%8C%87%E4%BB%A4_%E5%AE%8B%E6%A3%AE%E5%AE%89.png" alt="img"></p>
<p>当然，如果您只想将局部指令应用于当前页面，则需要指令前添加前缀 <code>_</code> 。</p>
<pre class="line-numbers language-none"><code class="language-none">&lt;!-- _backgroundColor: aqua --&gt;

Add underscore prefix &#96;_&#96; to the name of local directives.

---

The second page would not apply setting of directives.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>对比上下两图，可以明显地看出两者之间的差异。</p>
<p><img src="https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig18_%20%E5%B1%80%E9%83%A8%E6%8C%87%E4%BB%A4_%E5%AE%8B%E6%A3%AE%E5%AE%89.png" alt="img"></p>
<h3 id="4-3-标题分隔符"><a href="#4-3-标题分隔符" class="headerlink" title="4.3 标题分隔符"></a>4.3 标题分隔符</h3><p>除了页面分割符<code>---</code>，如果文章结构比较清晰，我们还可以使用全局指令 <code>headingDivider</code> 分隔幻灯片页面。换句话说，就是 <code>headingDivider</code> 通过识别 Markdown 文档的标题来实现幻灯片分页。</p>
<p>例如，下面两个 Markdown 文档具有相同的输出。</p>
<p>一般语法如下：</p>
<pre class="line-numbers language-none"><code class="language-none"># 1st page

The content of 1st page

---

## 2nd page### The content of 2nd page

Hello, world!

---

# 3rd page

Thanks <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>Heading Divider 如下：</p>
<pre class="line-numbers language-none"><code class="language-none">&lt;!-- headingDivider: 2 --&gt;# 1st page

The content of 1st page

## 2nd page### The content of 2nd page

Hello, world!

# 3rd page

Thanks <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>个人认为用处不大</p>
<h3 id="4-4-页码"><a href="#4-4-页码" class="headerlink" title="4.4 页码"></a>4.4 页码</h3><p>局部命令 <code>paginate</code> 用于显示幻灯片的页码。</p>
<pre class="line-numbers language-none"><code class="language-none">&lt;!-- paginate: true --&gt;

You would be able to see a page number of slide in the lower right.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>当然，如果我们不想在标题页面出现页码，只需将指令 <code>paginate</code> 移到第二页即可。</p>
<pre class="line-numbers language-none"><code class="language-none"># Title slide

This page will not paginate by lack of &#96;paginate&#96; local directive.

---

&lt;!-- paginate: true --&gt;

It will paginate slide from a this page.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>添加页码：<code>&lt;!-- page_number: true --&gt;</code>，取消页码：<code>&lt;!-- page_number:false --&gt;</code>，这个也是，加<code>*</code>表示只对某一页操作</p>
<h3 id="4-5-页眉和页脚"><a href="#4-5-页眉和页脚" class="headerlink" title="4.5 页眉和页脚"></a>4.5 页眉和页脚</h3><p>当需要在多张幻灯片中显示相同的页眉或页脚时，可将局部指令 <code>header</code>或 <code>footer</code> 写在 Markdown 文档的最前面。</p>
<pre class="line-numbers language-none"><code class="language-none">---

marp: true
header: &#39;Stata连享会&#39;
footer: 连享会 · [直播间](http:&#x2F;&#x2F;lianxh.duanshu.com) &amp;emsp;  | &amp;emsp;  lianxh.cn &amp;emsp; | &amp;emsp;  [课程主页](https:&#x2F;&#x2F;lianxh.duanshu.com&#x2F;#&#x2F;brief&#x2F;course&#x2F;c3f79a0395a84d2f868d3502c348eafc) &amp;emsp; | &amp;emsp;  [课程展示](https:&#x2F;&#x2F;gitee.com&#x2F;arlionn&#x2F;p101)

---

# VS Code + Marp : 用 Markdown 制作幻灯片##### 作者:宋森安 (西南财经大学)；张家豪(西北大学)##### 邮箱: songsean@88.com

--- 

### 文章目录
- #### 一、前言
- #### 二、下载与安装
- #### 三、操作教程
- #### 四、讨论
- #### 五、参考资料
- #### 六、相关推文<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>上述 Markdown 文档改动自 <strong><a href="https://gitee.com/arlionn/md/wikis/Marp - 用 Markdown 写幻灯片.md?sort_id=1987882">连玉君老师的幻灯片</a></strong>，其演示效果如下图：</p>
<p><img src="https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig27_%20%E9%A1%B5%E8%84%9A_%E5%AE%8B%E6%A3%AE%E5%AE%89.gif" alt="img"></p>
<p>添加页脚：<code>&lt;!-- footer: 页脚内容 --&gt;</code>，如果写为<code>&lt;!-- *footer: 页脚内容 --&gt;</code>，就是仅本页添加页脚。<code>&lt;!-- footer: --&gt;</code> 就相当于取消页脚</p>
<h3 id="4-6页面大小"><a href="#4-6页面大小" class="headerlink" title="4.6页面大小"></a>4.6页面大小</h3><p>写上类似这种：<code>&lt;!-- $size: 16:9 --&gt;</code>，可以调节 PPT 尺寸。</p>
<h3 id="4-7-summary"><a href="#4-7-summary" class="headerlink" title="4.7 summary"></a>4.7 summary</h3><p>下面是我的日常使用</p>
<pre class="line-numbers language-none"><code class="language-none">---
marp: true
# 主题
theme: nord
# 标题
title: 我是标题
# 页码，出现在右下角
paginate: True 
# 版本
version: 1.0.0 
# 页脚
footer: 我是页脚
# 页眉
header: 我是页眉
# 大小，也可以写16:9
size: 4K
# 类别,原理应该就是样式的叠加，这部分我不太清楚，读者可以自行探索
class:<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="5-主题风格"><a href="#5-主题风格" class="headerlink" title="5. 主题风格"></a>5. 主题风格</h2><h3 id="简单演示"><a href="#简单演示" class="headerlink" title="简单演示"></a>简单演示</h3><p>目前，Marp 提供 <strong>Default (书页黄)、Gaia(海蓝) 和 uncover</strong> 三种主题风格。</p>
<p>下面笔者简单演示这三种主题风格：</p>
<pre class="line-numbers language-none"><code class="language-none">---
marp: true
---
&lt;!-- theme: Default--&gt;
## &lt;!-- fit --&gt; 
VS Code + Marp: 用 Markdown 制作幻灯片
### 来源：Stata 连享会
### 作者：宋森安(西南财经大学); 张家豪(西北大学)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<blockquote>
<p>注释：<code>&lt;!-- fit --&gt;</code> 用于自动调整标题（一级标题）大小，以适应幻灯片大小。</p>
</blockquote>
<p>Default 主题风格的演示效果如下：</p>
<p><img src="https://www.mina.moe/wp-content/uploads/2019/01/TIM%E5%9B%BE%E7%89%8720190118110508.png" alt="default"></p>
<p>将<code>&lt;!-- theme: Default--&gt;</code>更换为<code>&lt;!-- theme: Gaia--&gt;</code>，幻灯片演示风格就变成下图：</p>
<p><img src="https://www.mina.moe/wp-content/uploads/2019/01/TIM%E5%9B%BE%E7%89%8720190118110629.png" alt="gaia"></p>
<p>可以发现，在 Gaia 主题背景下，幻灯片的内容会对齐到左上方。但是我们可以使用 <code>class: lead</code> 来改变。</p>
<pre class="line-numbers language-none"><code class="language-none">&lt;!--
theme: gaia
class: lead
--&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>添加 <code>class: lead</code> 后，幻灯片的演示效果如下：</p>
<p><img src="https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig13_%20%E9%A3%8E%E6%A0%BC3_%E5%AE%8B%E6%A3%AE%E5%AE%89.png" alt="img"></p>
<p>Uncover 主题风格演示效果如下：</p>
<p><img src="https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig16_%20%E9%A3%8E%E6%A0%BC4_%E5%AE%8B%E6%A3%AE%E5%AE%89.png" alt="img"></p>
<h3 id="添加新的主题"><a href="#添加新的主题" class="headerlink" title="添加新的主题"></a>添加新的主题</h3><p>在./vscode/settings.json中，把css主题样式文件加入路径：</p>
<pre class="line-numbers language-none"><code class="language-none">&#123;
  &quot;markdown.marp.themes&quot;: [
    &quot;https:&#x2F;&#x2F;tapioca24.github.io&#x2F;marp-themes&#x2F;tapioca24.css&quot;
  ]
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>路径可以是网络地址，也可以是本地地址（最好是你的marp项目所在地址</p>
<h3 id="使用主题"><a href="#使用主题" class="headerlink" title="使用主题"></a>使用主题</h3><ul>
<li><p>引用一种主题：<code>&lt;!-- $theme: gaia --&gt;</code></p>
</li>
<li><p>在使用了 gaia 主题后，写上：<code>&lt;!-- template: invert --&gt;</code> 后，所有 PPT 默认使用 invert 背景色。</p>
</li>
<li><p>在某一页写上<code>&lt;!-- *template: invert --&gt;</code> 后，仅这一页 PPT 用 invert（<code>*</code>就是仅这一页使用的意思）。</p>
</li>
<li><p>或者直接在header，使用指令直接指定全局主题。（可通过指定某一页的主题来屏蔽全局主题）</p>
<pre class="line-numbers language-none"><code class="language-none">---
marp: true
theme: tapioca24
---<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ul>
<h3 id=""><a href="#" class="headerlink" title=" "></a> </h3><h2 id="6-图片语法"><a href="#6-图片语法" class="headerlink" title="6. 图片语法"></a>6. 图片语法</h2><p>图片语法句式为 <code>![](image.jpg)</code>。其中，<code>[keywords]</code> 用于设置幻灯片的尺寸、颜色等参数，<code>(image.jpg)</code>是图片地址。</p>
<p>插入图片的语法：<code>![](图片路径)</code> 路径可以是本机路径也可以是http路径</p>
<h3 id="6-1-调整图片大小"><a href="#6-1-调整图片大小" class="headerlink" title="6.1 调整图片大小"></a>6.1 调整图片大小</h3><p>在<code>[keywords]</code>中添加 <code>width</code> 、 <code>height</code> 等关键词调整图片的大小。</p>
<pre class="line-numbers language-none"><code class="language-none">![width:200px](image.jpg) &lt;!-- Setting width to 200px --&gt;
![height:30cm](image.jpg) &lt;!-- Setting height to 300px --&gt;
![width:200px height:30cm](image.jpg) &lt;!-- Setting both lengths --&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>这里也可以使用关键词的缩写 <code>w</code> 和 <code>h</code> ：</p>
<pre class="line-numbers language-none"><code class="language-none">![w:32 h:32](image.jpg) &lt;!-- Setting size to 32x32 px --&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p> <code>![缩放比例（如 200%)](图片路径)</code>：图片缩放后插入 PPT 中</p>
<h3 id="6-2-图片滤镜"><a href="#6-2-图片滤镜" class="headerlink" title="6.2 图片滤镜"></a>6.2 图片滤镜</h3><p>Marp 还支持将 <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/filter">CSS filters</a> 应用于图片语法，替换<code>[keywords]</code>中的内容，可对图片进行渲染。例如，<code>![blur:10px]()</code>、<code>![brightness:1.5]()</code> 、 <code>![contrast:200%]()</code>、<code>![saturate:2.0]()</code>、<code>![sepia:1.0]()</code>等指令。当省略括号内的参数时，软件会直接使用默认参数。</p>
<p>当然，我们可以将多个滤镜应用于图片。</p>
<pre class="line-numbers language-none"><code class="language-none">![brightness:.8 sepia:50%](https:&#x2F;&#x2F;fig-lianxh.oss-cn-shenzhen.aliyuncs.com&#x2F;Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig20_%20%E8%A5%BF%E5%8D%97%E8%B4%A2%E7%BB%8F%E5%A4%A7%E5%AD%A6_%E5%AE%8B%E6%A3%AE%E5%AE%89.jpg)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>例如，笔者使用 <code>![brightness:.8 sepia:50%](https://example.com/image.jpg)</code>，原图就被渲染成深褐色。</p>
<p><img src="https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig21_%20%E8%A5%BF%E5%8D%97%E8%B4%A2%E7%BB%8F%E5%A4%A7%E5%AD%A6_%E5%AE%8B%E6%A3%AE%E5%AE%89.png" alt="img"></p>
<p> </p>
<h3 id="6-3-幻灯片背景"><a href="#6-3-幻灯片背景" class="headerlink" title="6.3 幻灯片背景"></a>6.3 幻灯片背景</h3><p>关键词 <code>bg</code> 可设置幻灯片的背景，具体句式为：<code>![bg](https://example.com/background.jpg)</code>。</p>
<p>此外，我们可在 <code>bg</code> 后添加关键词选项，用于调整背景图片的尺寸。</p>
<p>例如，<code>![bg cover](image.jpg)</code>（缩放图像以填充幻灯片，这也是默认图片设置） 、 <code>![bg contain](image.jpg)</code> （缩放图像以适应幻灯片） 、 <code>![bg auto](image.jpg)</code>（不缩放图像，并使用原始大小） 、 <code>![bg 150%](image.jpg)</code>（按照指定百分比缩放）。</p>
<p> 若一页 PPT 内插入多张背景图片，它们会并列分布</p>
<p>也可使用这样的语法定义幻灯片背景</p>
<pre class="line-numbers language-none"><code class="language-none">&lt;style scoped&gt;
section&#123;
  background-image:url(&#39;.&#x2F;tju-images&#x2F;tju1.png&#39;);
  background-size:cover;
  position: absolute;
  &#125;
&lt;&#x2F;style&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="6-4-高级背景"><a href="#6-4-高级背景" class="headerlink" title="6.4 高级背景"></a>6.4 高级背景</h3><p>我们还可以通过高级背景来实现多重背景、背景拆分，甚至通过图片滤镜来设置幻灯片背景。</p>
<pre class="line-numbers language-none"><code class="language-none">![bg](https:&#x2F;&#x2F;fakeimg.pl&#x2F;800x600&#x2F;0288d1&#x2F;fff&#x2F;?text&#x3D;A)
![bg](https:&#x2F;&#x2F;fakeimg.pl&#x2F;800x600&#x2F;02669d&#x2F;fff&#x2F;?text&#x3D;B)
![bg](https:&#x2F;&#x2F;fakeimg.pl&#x2F;800x600&#x2F;67b8e3&#x2F;fff&#x2F;?text&#x3D;C)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<blockquote>
<p>注释：背景图片由网站 <a href="https://fakeimg.pl/">fakeimg.pl</a> 自动生成的<strong>假图片</strong>，改变 <code>text=</code> 后的内容，图片文字随之改变。</p>
</blockquote>
<p>多重背景图的演示效果如下：</p>
<p><img src="https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig14_%20%E8%83%8C%E6%99%AF1_%E5%AE%8B%E6%A3%AE%E5%AE%89.png" alt="img"></p>
<pre class="line-numbers language-none"><code class="language-none">![bg vertical](https:&#x2F;&#x2F;fakeimg.pl&#x2F;800x600&#x2F;0288d1&#x2F;fff&#x2F;?text&#x3D;A)
![bg](https:&#x2F;&#x2F;fakeimg.pl&#x2F;800x600&#x2F;02669d&#x2F;fff&#x2F;?text&#x3D;B)
![bg](https:&#x2F;&#x2F;fakeimg.pl&#x2F;800x600&#x2F;67b8e3&#x2F;fff&#x2F;?text&#x3D;C)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>在 <code>bg</code> 后加入 <code>vertical</code> 后，背景图片就会垂直排列，幻灯片演示效果如下图：</p>
<p><img src="https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig15_%20%E8%83%8C%E6%99%AF2_%E5%AE%8B%E6%A3%AE%E5%AE%89.png" alt="img"></p>
<p>在 <code>bg</code> 右侧添加 <code>left</code> 或 <code>right</code> 可设置背景图的位置。添加 <code>left</code> 或 <code>right</code> 后，幻灯片内容所占的空间也会减小一半。</p>
<pre class="line-numbers language-none"><code class="language-none">---
marp: true
---
![bg left](https:&#x2F;&#x2F;fig-lianxh.oss-cn-shenzhen.aliyuncs.com&#x2F;Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig20_%20%E8%A5%BF%E5%8D%97%E8%B4%A2%E7%BB%8F%E5%A4%A7%E5%AD%A6_%E5%AE%8B%E6%A3%AE%E5%AE%89.jpg)

&lt;!-- color: black--&gt;# 欢迎报考 西北大学<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>拆分背景的演示效果如下图：</p>
<p><img src="https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig22_%20%E5%88%86%E6%8B%86%E8%83%8C%E6%99%AF1_%E5%AE%8B%E6%A3%AE%E5%AE%89.png" alt="img"></p>
<p>我们也可以将多张图片固定在一侧。</p>
<p>文本信息如下：</p>
<pre class="line-numbers language-none"><code class="language-none">---
marp: true
---
![bg right](https:&#x2F;&#x2F;fig-lianxh.oss-cn-shenzhen.aliyuncs.com&#x2F;Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig20_%20%E8%A5%BF%E5%8D%97%E8%B4%A2%E7%BB%8F%E5%A4%A7%E5%AD%A6_%E5%AE%8B%E6%A3%AE%E5%AE%89.jpg)

![bg](https:&#x2F;&#x2F;fig-lianxh.oss-cn-shenzhen.aliyuncs.com&#x2F;Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig20_%20%E8%A5%BF%E5%8D%97%E8%B4%A2%E7%BB%8F%E5%A4%A7%E5%AD%A6_%E5%AE%8B%E6%A3%AE%E5%AE%89.jpg)

&lt;!-- color: black--&gt;# 欢迎报考 ##  西北大学、西南财经大学<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>幻灯片演示效果如下图：</p>
<p><img src="https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig23_%20%E5%88%86%E6%8B%86%E8%83%8C%E6%99%AF2_%E5%AE%8B%E6%A3%AE%E5%AE%89.png" alt="img"></p>
<p>除了通过图片设置背景外，Marp 也支持设定颜色参数来改变幻灯片背景。背景颜色的基本句式为 <code>![bg](颜色参数)</code> ，文字颜色的基本句式为 <code>![](颜色参数)</code>。如下图所示：</p>
<p><img src="https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/Marp%E5%88%9B%E5%BB%BA%E5%B9%BB%E7%81%AF%E7%89%87_Fig24_%20%E9%A2%9C%E8%89%B2%E8%AE%BE%E7%BD%AE_%E5%AE%8B%E6%A3%AE%E5%AE%89.png" alt="img"></p>
<h2 id="7-一些技巧"><a href="#7-一些技巧" class="headerlink" title="7.一些技巧"></a>7.一些技巧</h2><h3 id="from-tapioca24"><a href="#from-tapioca24" class="headerlink" title="from tapioca24"></a>from <a href="https://github.com/tapioca24">tapioca24</a></h3><p>There are several utility classes that can be used to enrich the presentation of slides.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Class</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>text-center</code></td>
<td>Align all content to the center.</td>
</tr>
<tr>
<td><code>text-&lt;size&gt;</code></td>
<td>Increase the font size. The <code>size</code> can be specified in intervals of 25 from 125 to 1000.</td>
</tr>
</tbody>
</table>
</div>
<p>Give the class to only the desired page by writing the following in the slide.</p>
<pre class="line-numbers language-none"><code class="language-none">&lt;!-- _class: text-center text-125 --&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://sspai.com/post/55718">Marp：用 Markdown「写」PPT 的新选择 - 少数派 (sspai.com)</a></li>
<li><a href="https://marpit.marp.app/"><strong>Marpit</strong>: Markdown slide deck framework</a></li>
<li><a href="https://marp.app/"><strong>Marp</strong>: Markdown Presentation Ecosystem</a></li>
<li><a href="https://marketplace.visualstudio.com/items?itemName=marp-team.marp-vscode"><strong>Marp for VS Code</strong> - Visual Studio Marketplace</a></li>
<li><a href="https://github.com/marp-team/marp-vscode"><strong>GitHub - Marp for VS Code</strong>: Create slide deck written in Marp Markdown on VS Code</a></li>
<li><a href="https://fakeimg.pl/"><strong>fakeimg.pl</strong></a></li>
<li><strong><a href="https://gitee.com/arlionn/md">连享会 Markdown 仓库</a></strong></li>
</ul>
<p> </p>
<h2 id="相关推文"><a href="#相关推文" class="headerlink" title="相关推文"></a>相关推文</h2><ul>
<li>专题：<a href="https://www.lianxh.cn/blogs/30.html">Markdown</a><ul>
<li><a href="https://www.lianxh.cn/news/03ff18d8a9957.html">Markdown：五分钟Markdown教程</a></li>
<li><a href="https://www.lianxh.cn/news/845d7f5a2d977.html">Markdown教程之LaTeX数学公式简介</a></li>
<li><a href="https://www.lianxh.cn/news/6dfbbaefbb2d5.html">Markdown：一键将-Word-转换为-Markdown</a></li>
<li><a href="https://www.lianxh.cn/news/e3116b7e22ff8.html">Markdown-图床</a></li>
<li><a href="https://www.lianxh.cn/news/554f3e9c9f08d.html">Markdown常用LaTex数学公式</a></li>
<li><a href="https://www.lianxh.cn/news/dbbd90d8b96ac.html">连玉君Markdown笔记</a></li>
<li><a href="https://www.lianxh.cn/news/c061d6b77c6aa.html">Markdown中书写LaTeX数学公式简介</a></li>
<li><a href="https://www.lianxh.cn/news/61836b4326f32.html">在 Markdown 中使用表情符号</a></li>
<li><a href="https://www.lianxh.cn/news/1be9d1ba1a023.html">在 Markdown 中使用表情符号</a></li>
<li><a href="https://www.lianxh.cn/news/054fd41922063.html">mdnice.cn——Markdown、知乎、公众号排版神器</a></li>
<li><a href="https://www.lianxh.cn/news/b37f5fac84457.html">两种网页转Markdown的简便方法</a></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>折腾</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>css语法学习</title>
    <url>/css/marp/css%E8%AF%AD%E6%B3%95%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h2 id="图片设置"><a href="#图片设置" class="headerlink" title="图片设置"></a>图片设置</h2><h3 id="更改长宽"><a href="#更改长宽" class="headerlink" title="更改长宽"></a>更改长宽</h3><pre class="line-numbers language-none"><code class="language-none">![width:200px](image.jpg)
![height:300px](image.jpg)
![w:200px h:30cm](image.jpg)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h3 id="图片滤镜（Image-Filter）"><a href="#图片滤镜（Image-Filter）" class="headerlink" title="图片滤镜（Image Filter）"></a>图片滤镜（Image Filter）</h3><p>基于 CSS 的 filter 属性，Marp 可以对图片进行一些基于模糊、亮度、对比度等的操作，如：</p>
<pre class="line-numbers language-none"><code class="language-none">![blur:15px](image.png)
![brightness:0.5](image.png)
![contract:150%](image.png)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h3 id="背景图片"><a href="#背景图片" class="headerlink" title="背景图片"></a>背景图片</h3><p><img src="background.png" alt="bg"></p>
<p>同时通过在 <code>bg</code> 后追加图片的格式属性，如 <code>[bg fit]</code> ，可以具体设置背景图片的缩放方式。其中 <code>cover</code> 表示充满页面， <code>fit</code> 表示拉伸以适应页面， <code>auto</code> 为不做缩放使用原图片比例。</p>
<h3 id="更改布局"><a href="#更改布局" class="headerlink" title="更改布局"></a>更改布局</h3><h4 id="背景图片布局"><a href="#背景图片布局" class="headerlink" title="背景图片布局"></a><strong>背景图片布局</strong></h4><pre class="line-numbers language-text" data-language="text"><code class="language-text">![bg](images/9BBDF9.png)
![bg](images/2EC0F9.png)
![bg](images/B95F89.png)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>在其中一张图片后加入属性 <code>vertical</code> 将使图片纵向组合。</p>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">![bg vertical](images/9BBDF9.png)
![bg](images/2EC0F9.png)
![bg](images/B95F89.png)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h3 id="更新图片与文字位置"><a href="#更新图片与文字位置" class="headerlink" title="更新图片与文字位置"></a><strong>更新图片与文字位置</strong></h3><p>有时候想左文右图，或者左图右文的布局，可以设置背景图片的位置</p>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">![bg right w:15cm](images/prometheuslogo.png)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h5 id="图片加阴影drop-shadow"><a href="#图片加阴影drop-shadow" class="headerlink" title="图片加阴影drop-shadow"></a>图片加阴影<code>drop-shadow</code></h5><pre class="line-numbers language-plantuml" data-language="plantuml"><code class="language-plantuml">start 
if (condition A ) then (yes)
	:Text 1;
	endif
stop<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
]]></content>
      <categories>
        <category>css</category>
      </categories>
      <tags>
        <tag>marp</tag>
        <tag>css</tag>
        <tag>折腾</tag>
      </tags>
  </entry>
  <entry>
    <title>服务器上配置jupyter_notebook_个人简易教程</title>
    <url>/%E6%9C%8D%E5%8A%A1%E5%99%A8/jupyter-notebook/%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E9%85%8D%E7%BD%AEjupyter-notebook-%E4%B8%AA%E4%BA%BA%E7%AE%80%E6%98%93%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="后续有时间再细写"><a href="#后续有时间再细写" class="headerlink" title="后续有时间再细写"></a>后续有时间再细写</h2><h2 id="准备工作："><a href="#准备工作：" class="headerlink" title="准备工作："></a>准备工作：</h2><p>现在服务器上下载anaconda和jupyter notebook</p>
<p>然后配置文件，密码之类的</p>
<p>然后在xshell连接端口</p>
<p>然后localhost:8888，输入密码即可</p>
<h2 id="参考链接："><a href="#参考链接：" class="headerlink" title="参考链接："></a>参考链接：</h2><p><a href="https://blog.csdn.net/watermel__/article/details/114988331">https://blog.csdn.net/watermel__/article/details/114988331</a></p>
<p><a href="https://blog.csdn.net/qq_24027563/article/details/80589880">https://blog.csdn.net/qq_24027563/article/details/80589880</a></p>
<p><a href="https://www.baidu.com/link?url=V_stHI6UhDDIIwOfI1dRk0CxGUz7JLQJvAAVplwItVLOykn1ZuSlBmJK_OaKaSui&amp;wd=&amp;eqid=eb38869200037c5e00000003610f9dfc">https://www.baidu.com/link?url=V_stHI6UhDDIIwOfI1dRk0CxGUz7JLQJvAAVplwItVLOykn1ZuSlBmJK_OaKaSui&amp;wd=&amp;eqid=eb38869200037c5e00000003610f9dfc</a></p>
]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>jupyter_notebook</tag>
        <tag>踩坑</tag>
      </tags>
  </entry>
  <entry>
    <title>耶鲁大学博弈论公开课笔记</title>
    <url>/%E5%8D%9A%E5%BC%88%E8%AE%BA/math/%E8%80%B6%E9%B2%81%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%BC%88%E8%AE%BA%E5%85%AC%E5%BC%80%E8%AF%BE%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="耶鲁大学博弈论公开课笔记"><a href="#耶鲁大学博弈论公开课笔记" class="headerlink" title="耶鲁大学博弈论公开课笔记"></a><strong>耶鲁大学博弈论公开课笔记</strong></h1><p><strong><a href="https://wenku.baidu.com/view/d9e4d40fc5da50e2524d7fc1.html">耶鲁大学公开课博弈论笔记(博弈论24讲) - 百度文库 (baidu.com)</a></strong></p>
<h2 id="P1-导论-五个入门结论"><a href="#P1-导论-五个入门结论" class="headerlink" title="P1 导论-五个入门结论"></a>P1 导论-五个入门结论</h2><h4 id="理性人"><a href="#理性人" class="headerlink" title="理性人"></a>理性人</h4><p>​    指代这一类人，他们只关心自己的利益<em>(这个定义并不完备，需要修改）</em></p>
<h4 id="举例："><a href="#举例：" class="headerlink" title="举例："></a>举例：</h4><ul>
<li>囚徒困境（注意模型的不完备性）</li>
<li>宿舍卫生打扫问题（没人愿意投入）</li>
<li>企业打价格战（无限降价会使得自己收到损失）</li>
</ul>
<h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><ul>
<li><p>如果选择a的结果严格优于b,那么就说a相当于b来说是一个严格优势策略。结论：不要选择严格劣势策略（<strong>Do not play a strictly dominated strategy.</strong></p>
</li>
<li><p>选择严格优势策略（无论对方选择什么，自己的收益都会更高）</p>
</li>
<li><p>在不同的参与者，不同的利益关心情况下，做出的选择都会不同</p>
</li>
<li><p>理性人的理性选择导致了次优的结果（<strong>Rational choice in this case,can lead to outcomes that suck.(Rational choice by rational players,can lead to bad outcomes.</strong></p>
</li>
<li><p>协和谬误 Payoffs matter.苟欲求之，必先知之（<strong>You can’t get what you want,till you know what you want.</strong></p>
</li>
<li><p>人总是以自己为出发点思考问题。</p>
</li>
<li><p>策略决策的核心：换位思考，站在别人的立场上看比人会怎样做，在考虑自己受益的同时，要注意别人的选择。（<strong>Put yourself in other’s shoes and try to figure out what they will do.</strong></p>
</li>
<li><p>耶鲁大学的学生都很自私。（<strong>Yale students are evil.</strong></p>
</li>
</ul>
<h2 id="P2-学会换位思考"><a href="#P2-学会换位思考" class="headerlink" title="P2 学会换位思考"></a>P2 学会换位思考</h2><h4 id="博弈的三大要素"><a href="#博弈的三大要素" class="headerlink" title="博弈的三大要素"></a>博弈的三大要素</h4><ul>
<li>players;参与人</li>
<li>strategies；策略集合</li>
<li>payoffs; 收益</li>
</ul>
<h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><ul>
<li>打渔问题</li>
<li>解决全球变暖问题（控制碳排放</li>
<li>防线布置问题（汉尼拔是否翻越阿尔卑斯山</li>
</ul>
<h4 id="严格劣势策略"><a href="#严格劣势策略" class="headerlink" title="严格劣势策略"></a>严格劣势策略</h4><ul>
<li>​    无论对方做出哪个选择，你的这个选择都是 strictly 不利的 弱劣势策略：其中的一个策略严格劣于另外一个策略。 </li>
</ul>
<p>​    </p>
<p>​    当对手有严格优势策略时，而我方两个策略相同时，考虑对手优势策略下的收益。 </p>
<h4 id="互动活动：-全班同学写1-100的数字，写到平均数的三分之二的人即为获胜方。"><a href="#互动活动：-全班同学写1-100的数字，写到平均数的三分之二的人即为获胜方。" class="headerlink" title="互动活动： 全班同学写1-100的数字，写到平均数的三分之二的人即为获胜方。"></a>互动活动： 全班同学写1-100的数字，写到平均数的三分之二的人即为获胜方。</h4><ul>
<li><h5 id="规则"><a href="#规则" class="headerlink" title="规则"></a>规则</h5><ul>
<li>所有人都从 1 到 100 中选个数字，最接近所有人选的数字的均值的2/3 者为获胜方</li>
</ul>
</li>
<li><h5 id="推理过程"><a href="#推理过程" class="headerlink" title="推理过程"></a>推理过程</h5><ul>
<li>作为理性人．每个人都会选择67（ 100 <em> 2/3 ）以下的数，进一步假设你的对手也是理性的，你会选择 45 ( 100</em> 4/9 ）以下的数 … … </li>
<li>依据哲学观点，如果大家都是理性程度相当的，（极端理性）那么最后数字将为 1 ，然而结果却是 9 （在本次实验中</li>
<li>这说明<strong>博弈的复杂性</strong></li>
</ul>
</li>
<li><h5 id="导出"><a href="#导出" class="headerlink" title="导出"></a>导出</h5><ul>
<li>共同知识的概念<ul>
<li>在本次实验中，是这样的：要在其中了解到对方都是理性人，知道对方知道对方都是理性人，知道对方知道对方知道对方都是理性人……</li>
</ul>
</li>
<li>共同知识与共有知识之间的区别<ul>
<li>common knowledge（不同于 mutual knowledge共有知识.） ：站在对手的角度思考对手在这次博弈中有多老练，思考对手知道你在博弈中有多老练，思考对手知道你在思考他有多老练，如此一直循环成立 （<em>这个是网上抄的，我觉得不strict</em> </li>
</ul>
</li>
</ul>
</li>
<li><p>共有知识和共同知识</p>
<ul>
<li>共有知识 + 外部信息 -&gt; 共同知识<ul>
<li>例如：脏脸博弈，皇帝的新衣，沉默的螺旋</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="P3-迭代剔除和中位选民定理"><a href="#P3-迭代剔除和中位选民定理" class="headerlink" title="P3 迭代剔除和中位选民定理"></a>P3 迭代剔除和中位选民定理</h2><p>学会在剔除劣势策略的情况下再重新审视博弈问题，再做决策。站在对方的立场上，考虑他们不会选择什么，再考虑对方会认为我们不会选择什么</p>
<h5 id="利用迭代剔除法领悟中间选民问题"><a href="#利用迭代剔除法领悟中间选民问题" class="headerlink" title="利用迭代剔除法领悟中间选民问题"></a>利用迭代剔除法领悟中间选民问题</h5><ul>
<li>迭代剔除法<ul>
<li>反复消去严格下策，不断把劣势策略剔除出去，最后只剩下相对优势的策略</li>
</ul>
</li>
<li>中间选民问题<ul>
<li>政治选举候选人的politic position. 如果从极左到极右有10个程度，则大部分人会选择5，6。 在政治上，人们倾向于中间站位。</li>
<li>在两党制中，政党表述纲领要吸引中间位置的选民，他们认为在选举中处于中间标度可以吸引左右两边的选民，并以此获得胜利。</li>
<li>理论的不完备性<ul>
<li>太多了，qaq</li>
<li>这个公开课是200几年的，此时并没有发生民粹主义的崛起，可以用川普当选等所谓黑天鹅事件来find这个theory的missing</li>
<li>可以由理论的missing导出理论成立所需的假设条件</li>
</ul>
</li>
<li>理论成立的条件<ul>
<li>有两个参与人</li>
<li><del>政治立场能使选民相信</del></li>
</ul>
</li>
<li>延伸出的问题（在商业上，人们倾向于选址集中。）<ul>
<li>加油站选址</li>
<li>快餐店选址<ul>
<li>比如顾客认为这一片都是快餐店，从而帮助顾客选择这个地方（对于快餐企业，在不确定哪个位置较佳的时候会选在同一处）</li>
<li>对于某快餐企业：根据其他快餐店地址，确定自己的新店地址可以减少决策量</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>在迭代剔除法不能运用时，比如说该博弈方1和2均没有严格下策，可以用二维坐标系画出选择策略之后的收益分布（即<strong>做一个函数辅助决策</strong>）</p>
<p>最佳策略：在对方不同选择的概率下，做出自己的最佳选择。 列方程，线性规划，求不同范围下的最优解。用数字使得自己的解释更为有力。</p>
<h2 id="P4-足球比赛和商业合作之最佳对策"><a href="#P4-足球比赛和商业合作之最佳对策" class="headerlink" title="P4 足球比赛和商业合作之最佳对策"></a>P4 足球比赛和商业合作之最佳对策</h2><h3 id="罚点球"><a href="#罚点球" class="headerlink" title="罚点球"></a>罚点球</h3><ul>
<li>​    一个进过模型简化的点球模型：罚球这可以选择左路，中路，右路3种路线去踢点球，们将可以选择向左扑救或者向右扑救。罚球者的收益很容易计算</li>
<li>结论<ul>
<li>无论什么时候，罚球者向中路踢都不是一个最优的选择</li>
<li>不要选择一个在任何brief下都不是最优策略的策略</li>
<li>这里的brief并不是门将会向左或向右，而是指概率。我的理解是对中庸之道的批判。所以本例中，虽然罚球者的3种策略里没有劣势策略，不过还是可以用以上原则剔除掉一个策略</li>
</ul>
</li>
<li>missing<ul>
<li>罚球者是右撇子</li>
<li>门将可以中出（to be continued)</li>
</ul>
</li>
</ul>
<h3 id="Partnership-game：商业合作"><a href="#Partnership-game：商业合作" class="headerlink" title="Partnership game：商业合作"></a>Partnership game：商业合作</h3><ul>
<li>列出合作对象的效用函数，对此求导，令其一介导数为0，即可得出其最佳投入精力关于对方投入精力的函数（使自己的效益最大化，总效益-个人投入）。 </li>
<li>假设合作只有两方，令两函数相等，所得交点即为纳什均衡。 即双方都不愿意偏离这一点。</li>
</ul>
<h2 id="P5-纳什均衡"><a href="#P5-纳什均衡" class="headerlink" title="P5 纳什均衡"></a>P5 纳什均衡</h2><h3 id="1-纳什均衡的两大定义"><a href="#1-纳什均衡的两大定义" class="headerlink" title="1.纳什均衡的两大定义"></a>1.纳什均衡的两大定义</h3><ul>
<li><p>No individual can do better by divieting. </p>
</li>
<li><p>NE can be thought of self-fullfiling. </p>
</li>
</ul>
<p>2.任何参与人都严格不会改变策略，改变策略严格不会使参与人获得增益</p>
<p>3.其他参与人不改变行为的前提下，自己改变行为并没有任何好处</p>
<p>4.严格劣势永远不是最佳策略，最佳策略才可以出现NE</p>
<p>5.博弈会朝着趋向于一个均衡方向自然发展，结果不断趋向于一个NE</p>
<p>6.协和谬误不同于囚徒困境</p>
<ul>
<li>它有两个(nash)均衡，即all in和all out</li>
<li>前者可以通过沟通解决，本身具有强制力，因为符合自身利益。例如： 在银行里存钱、微软垄断（规模效应） 等。</li>
<li>后者无沟通</li>
</ul>
<p>7.区分协和谬误和沉默的螺旋之间的关系</p>
<ul>
<li>沉默的螺旋（The Spiral Of Silence）是一个政治学和大众传播理论。理论基本描述了这样一个现象：人们在表达自己想法和观点的时候，如果看到自己赞同的观点受到广泛欢迎，就会积极参与进来，这类观点就会越发大胆地发表和扩散；而发觉某一观点无人或很少有人理会（有时会有群起而攻之的遭遇），即使自己赞同它，也会保持沉默。意见一方的沉默造成另一方意见的增势，如此循环往复，便形成一方的声音越来越强大，另一方越来越沉默下去的螺旋发展过程。理论是基于这样一个假设：大多数个人会力图避免由于单独持有某些态度和信念而产生的孤立。</li>
</ul>
<h2 id="P6-第六讲：约会策略与古诺模型"><a href="#P6-第六讲：约会策略与古诺模型" class="headerlink" title="P6 第六讲：约会策略与古诺模型"></a>P6 第六讲：约会策略与古诺模型</h2><p><del>策略互补博弈 》协调博弈：性别大战（不同人有不同偏好） 策略代替博弈》</del></p>
<h5 id="古诺模型"><a href="#古诺模型" class="headerlink" title="古诺模型"></a>古诺模型</h5><ul>
<li>假设：2个公司生产可以完全互补产品，</li>
<li>在完全竞争市场上，成本=价格 </li>
<li>在完全垄断市场上，价格=边际成本 </li>
<li>古诺模型中，价格高于完全竞争市场，低于完全垄断市场</li>
</ul>
]]></content>
      <categories>
        <category>博弈论</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>经济学</tag>
      </tags>
  </entry>
  <entry>
    <title>《深度学习中的数学》读书记录</title>
    <url>/%E4%B9%A6%E6%91%98/math/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B/</url>
    <content><![CDATA[<h2 id="《深度学习中的数学》"><a href="#《深度学习中的数学》" class="headerlink" title="《深度学习中的数学》"></a>《深度学习中的数学》</h2><p><img src="https://raw.githubusercontent.com/ChangQingAAS/for_picgo/main/img/20210903124351.jpeg" alt="深度学习的数学"></p>
<p>挺水的</p>
<p>知道了一些基本概念</p>
<ul>
<li>神经网络：相较于原来更为抽象的原理学习，这次学到了一些具体的表述方式，以及核函数的意义</li>
<li>BP：没有仔细看 但是对原理的理解进一步加深了</li>
<li>CNN： 一些实例加强了我对CNN的理解</li>
</ul>
]]></content>
      <categories>
        <category>书摘</category>
      </categories>
      <tags>
        <tag>books</tag>
      </tags>
  </entry>
  <entry>
    <title>tf.graph()</title>
    <url>/tensorflow/MADRL/tf.graph()/</url>
    <content><![CDATA[<h2 id="tf-graph"><a href="#tf-graph" class="headerlink" title="tf.graph()"></a>tf.graph()</h2><p> tf.Graph()表示实例化一个用于tensorflow计算和表示用的数据流图，不负责运行计算。在代码中添加的操作和数据都是画在纸上的画，而图就是呈现这些画的纸。我们可以利用很多线程生成很多张图，但是默认图就只有一张。</p>
<p>  tf中可以定义多个计算图，不同计算图上的张量和运算是相互独立的，不会共享。计算图可以用来隔离张量和计算，同时提供了管理张量和计算的机制。</p>
<p>  1、使用g = tf.Graph()函数创建新的计算图</p>
<p>  2、在with g.as_default():语句下定义属于计算图g的张量和操作</p>
<p>  3、在with tf.Session()中通过参数graph=xxx指定当前会话所运行的计算图</p>
<p>  4、如果没有显示指定张量和操作所属的计算图，则这些张量和操作属于默认计算图</p>
<p>  5、一个图可以在多个sess中运行，一个ses也能运行多个图</p>
<p>  操作示例：</p>
<p># 默认计算图上的操作</p>
<p>a = tf.constant([1.0, 2.0])</p>
<p>b = tf.constant([2.0, 3.0])</p>
<p>result = a + b</p>
<p># 定义两个计算图</p>
<p>g1 = tf.Graph()</p>
<p>g2 = tf.Graph()</p>
<p># 在g1中定义张量和操作</p>
<p>with g1.as_default():</p>
<p>  a = tf.constant([1.0, 1.0])</p>
<p>  b = tf.constant([1.0, 1.0])</p>
<p>  result1 = a + b</p>
<p># 在g2中定义张量和操作</p>
<p>with g2.as_default():</p>
<p>  a = tf.constant([2.0, 2.0])</p>
<p>  b = tf.constant([2.0, 2.0])</p>
<p>  result2 = a + b</p>
<p># 创建会话</p>
<p>with tf.Session(graph=g1) as sess:</p>
<p>  out = sess.run(result1)</p>
<p>  print(out)</p>
<p>with tf.Session(graph=g2) as sess:</p>
<p>  out = sess.run(result2)</p>
<p>  print(out)</p>
<p>with tf.Session(graph=tf.get_default_graph()) as sess:</p>
<p>  out = sess.run(result)</p>
<p>  print(out)</p>
<p>返回：</p>
<p>[2.0, 2.0]</p>
<p>[4.0, 4.0]</p>
<p>[3.0, 5.0]</p>
]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>一些关于DRL的概念</title>
    <url>/MARL/MADRL/%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<p>注意力机制（方法论）：为了让计算机更加适应人类交流场景，我们必须教会计算机选择遗忘和关联上下文，这种机制就是所谓的「注意力机制」 </p>
<p>1、将<strong>卷积神经网络</strong> 的感知能力与强化学习的决策能力结合在一起,可以较好地解决传统算法存在的容易陷入局部最优、在相近的障碍物群中震荡且不能识别路径、在狭窄通道中摆动以及障碍物附近目标不可达等问题, 并且大大提高了机器人轨迹跟踪和动态避障的实时性和适应性。</p>
<p>2、制约传统强化学习算法的瓶颈在于Q-learning是一种表格方法，根据过去出现过的状态动作空间，更新和迭代Q值，使得其适用的状态和动作空间非常小，如果一个状态从未出现过，强化学习是无法处理的，模型几乎就没有泛化能力，为增强模型预测能力，可以使用<strong>回归函数拟合Q值</strong> 。</p>
<p>3、算法可以平衡运用探索和利用策略，<strong>搜索策略定义为小概率随机搜索策略和及时回报小于设定阈值进行随机动作搜索的引导式(δ)策略</strong> 。可以使强化学习智能体与环境的交互中学习一个好的策略，同时又不至于在试错的过程中丢失太多的奖励。</p>
<p>4、深度卷积神经网络进行训练时存在的假设是训练数据是独立同分布，而从环境中采集到的数据之间存在着关联性，利用这些数据进行顺序训练，算法模型将存在不稳定问题. 通过<strong>经验回放的方式</strong> 可以令训练出的模型收敛且稳定。</p>
<p>5、奖惩函数可以定义为与距离有关（如避障时智能体与障碍物之间的距离等）。</p>
<p>6、目标函数可以使用基于梯度下降的<strong>Adam 优化算法。</strong></p>
<p><img src="https://raw.githubusercontent.com/ChangQingAAS/for_picgo/main/img/20210903103622.png" alt="image-20210710170957091"></p>
<p><img src="https://raw.githubusercontent.com/ChangQingAAS/for_picgo/main/img/20210903103632.png" alt="image-20210710171922815"></p>
<p><img src="https://raw.githubusercontent.com/ChangQingAAS/for_picgo/main/img/20210903103638.png" alt="image-20210710184402541" style="zoom:50%;" /></p>
<p>DDPG 结合了 DQN、确定性策略梯度算法 DPG ( deterministic policy gradient) 和 演 员-评 论 家 算 法( actor-critic methods) ，可解决强化学习中连续动作 空间问题。DQN 利用神经网络来逼近值函数，其 参数是每层网络的权重，对值函数进行更新其实就 是更新权重参数。DPG 算法采用异策略学习方法， 行动策略采用随机策略，以保证足够的探索，评估策 略采用 确 定 策 略，以减少动作空间的采样数量。 DPG 采用演员-评论家算法框架，它通过分离策略函 数和价值函数来降低学习难度，策略函数被称为演 员，价值函数被称为评论家，演员根据当前的环境状态产生一个动作，而评论家则对演员采取的动作进 行评价。</p>
<p>评论家网络模型选择 （On policy 类还是 off policy ) 算法，演员网络模型选择策略梯度算法。 在常规 DDPG 算法中，网络从回放缓冲区中随机采样进行离线训练。回放缓冲区是一个有限大小 的缓冲区 Ｒ，元祖( s<sub>t</sub>，a<sub>t</sub>，rt，s<sub>t + 1</sub> ) 储存在缓冲区中并 且根据探索策略随机采样。</p>
<p><img src="https://raw.githubusercontent.com/ChangQingAAS/for_picgo/main/img/20210903103704.png" alt="image-20210711112606803"></p>
<p>　传统的强化学习方法主要针对的是离散状态和行为空间的马尔科 夫决策过程，也就是状态的值函数或行为的值函数采用了表格的形式 来进行存储和迭代计算。但是实际工程应用中的许多优化决策问题是 具有大规模或连续的状态或行为空间的情况，所以表格型强化学习算 法也同动态规划法一样存在维数灾难。为了克服维数灾难，以实现对 连续性状态或行为空间的马尔科夫决策过程的最优值函数和最优策略 的逼近，我们就必须研究强化学习的泛化问题或推广问题，也就是利 用有限的学习经验和记忆以实现对一个大范围空间的有效知识获取和 表示的方法。</p>
]]></content>
      <categories>
        <category>MARL</category>
      </categories>
      <tags>
        <tag>MARL</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>人工蜂群算法</title>
    <url>/MARL/MADRL/%E4%BA%BA%E5%B7%A5%E8%9C%82%E7%BE%A4%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h3 id="人工蜂群算法"><a href="#人工蜂群算法" class="headerlink" title="**人工蜂群算法"></a>**人工蜂群算法</h3><ul>
<li><p><strong>模仿蜜蜂行为提出的一种优化方法 群智能算法</strong></p>
</li>
<li><p><strong>特点</strong></p>
</li>
</ul>
<ul>
<li><p><strong>其使用了较少的控制参数，并且鲁棒性强，在每次迭代过程中都会进行全局和局部的最优解搜索，因此能够找到最优解的概率大大增加。</strong></p>
</li>
<li><p><strong>不需要了解问题的特殊信息，只需要对问题进行优劣的比较，通过各人工蜂个体的局部寻优行为，最终在群体中使全局最优值突现出来，有着较快的收敛速度。</strong></p>
</li>
</ul>
<ul>
<li>人工蜂群算法详解**](<a href="https://zhuanlan.zhihu.com/p/253220840">https://zhuanlan.zhihu.com/p/253220840</a>)</li>
</ul>
]]></content>
      <categories>
        <category>MARL</category>
      </categories>
      <tags>
        <tag>MARL</tag>
        <tag>群体智能</tag>
      </tags>
  </entry>
  <entry>
    <title>多智能体强化学习</title>
    <url>/MARL/MADRL/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h3 id="多智能体强化学习"><a href="#多智能体强化学习" class="headerlink" title="多智能体强化学习"></a>多智能体强化学习</h3><ul>
<li>强化学习的核心思想是“试错”（trial-and-error）：智能体通过与环境的交互，根据获得的反馈信息迭代地优化** </li>
<li><strong><strong>多智能体问题的建模——博弈论基础</strong></strong></li>
</ul>
<ul>
<li><p><strong>马尔科夫博弈</strong></p>
</li>
<li><p><strong>纳什均衡</strong></p>
</li>
<li><p><strong><strong>均衡求解方法**</strong></strong>是多智能体强化学习的基本方法，它对于多智能体学习的问题，结合了强化学习的经典方法（如 Q-learning）和博弈论中的均衡概念，通过 RL 的方法来求解该均衡目标，从而完成多智能体的相关任务。** </p>
<ul>
<li><strong>**多智能体问题的求解——多智能体强化学习算法</strong></li>
</ul>
</li>
</ul>
<ul>
<li><strong>independent Q-learning</strong></li>
</ul>
<pre><code>* **每个智能体把其他智能体都当做环境中的因素，仍然按照单智能体学习的方式、通过与环境的交互来更新策略**
* **简单易实现**
* **忽略了其他智能体也具备决策的能力、所有个体的动作共同影响环境的状态** 
* **很难稳定地学习并达到良好的效果。**
</code></pre><ul>
<li><p><strong>智能体之间是完全竞争关系（零和随机博弈</strong></p>
<ul>
<li><strong>minimax Q-learning 算法</strong></li>
</ul>
</li>
</ul>
<pre><code>  *  **对于智能体 i，它需要考虑在其他智能体（i-）采取的动作（a-）令自己（i）回报最差（min）的情况下，能够获得的最大（max）期望回报**
</code></pre><ul>
<li><p><strong>智能体之间是半合作半竞争（混合）关系</strong></p>
<ul>
<li><p><strong>Nash Q-learning 方法</strong></p>
<ul>
<li><strong>当每个智能体采用普通的 Q 学习方法，并且都采取贪心的方式、即最大化各自的 Q 值时，这样的方法容易收敛到纳什均衡策略</strong></li>
<li><strong>Nash Q-learning 方法可用于处理以纳什均衡为解的多智能体学习问题。</strong></li>
<li><strong>它的目标是通过寻找每一个状态的纳什均衡点，从而在学习过程中基于纳什均衡策略来更新 Q 值。</strong></li>
<li><strong>对于单个智能体 i，在使用 Nash Q 值进行更新时，它除了需要知道全局状态 s 和其他智能体的动作 a 以外，还需要知道其他所有智能体在下一状态对应的纳什均衡策略π。进一步地，当前智能体就需要知道其他智能体的 Q(s’)值，这通常是根据观察到的其他智能体的奖励和动作来猜想和计算。所以，Nash Q-learning 方法对智能体能够获取的其他智能体的信息（包括动作、奖励等）具有较强的假设，在复杂的真实问题中一般不满足这样严格的条件，方法的适用范围受限。</strong></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>智能体之间是完全合作关系</strong></p>
<ul>
<li><p><strong><strong>智能体通过协作获得最优回报时，不需要协调机制</strong></strong></p>
<ul>
<li><p><strong>假设对于环境中的所有智能体 {A,B} 存在不只一个最优联合动作，即有 {πA,πB} 和{hA,hB}，那么 A 和 B 之间就需要协商机制，决定是同时取π，还是同时取 h；因为如果其中一个取π、另一个取 h，得到的联合动作就不一定是最优的了。</strong></p>
</li>
<li><p><strong>Team Q-learning 是一种适用于不需要协作机制的问题的学习方法，它提出对于单个智能体 i，可以通过下面这个式子来求出它的最优动作 hi：</strong><br><img src="https://pic3.zhimg.com/80/v2-8904fe3a6f84d280ed2ba3cef5ce1b26_720w.jpg" alt="img"></p>
</li>
<li><strong>Distributed Q-learning 也是一种适用于不需要协作机制的问题的学习方法，不同于 Team Q-learning 在选取个体最优动作的时候需要知道其他智能体的动作，在该方法中智能体维护的是只依据自身动作所对应的 Q 值，从而得到个体最优动作。</strong></li>
</ul>
</li>
<li><p><strong><strong>隐式的协调机制</strong></strong><br><strong>在智能体之间需要相互协商、从而达成最优的联合动作的问题中，<em>**</em></strong>个体之间的相互建模<strong>**</strong>，能够为智能体的决策提供潜在的协调机制**</p>
<ul>
<li><p><strong>联合动作学习（joint action learner，JAL）方法</strong></p>
</li>
<li><p><strong>智能体 i 会基于观察到的其他智能体 j 的历史动作、对其他智能体 j 的策略进行建模</strong></p>
</li>
<li><p><strong>频率最大 Q 值（frequency maximum Q-value, FMQ）方法</strong></p>
</li>
<li><p><strong>在个体 Q 值的定义中引入了个体动作所在的联合动作取得最优回报的频率，从而在学习过程中引导智能体选择能够取得最优回报的联合动作中的自身动作，那么所有智能体的最优动作组合被选择的概率也会更高。</strong></p>
</li>
<li><p><strong>基于平均场理论的多智能体强化学习（Mean Field MARL, MFMARL）方法</strong></p>
</li>
<li><p><strong>针对大规模群体问题</strong></p>
</li>
<li><p><strong>将传统强化学习方法（Q-learning）和平均场理论（mean field theory）相结合。平均场理论适用于对复杂的大规模系统建模，它使用了一种简化的建模思想：对于其中的某个个体，所有其他个体产生的联合作用可以用一个 “平均量” 来定义和衡量</strong></p>
</li>
</ul>
</li>
<li><p><strong>显式的协作机制</strong></p>
</li>
</ul>
</li>
</ul>
<pre><code>  * **主要是人机之间的交互，考虑现存的一些约束条件 / 先验规则等**
</code></pre><ul>
<li><p><strong>多智能体深度强化学习</strong></p>
<ul>
<li><strong>中心化的控制器（centralized controller）并不一定可行，或者说不一定是比较理想的决策方式。而如果采用完全分布式的方式，每个智能体独自学习自己的值函数网络以及策略网络、不考虑其他智能体对自己的影响，无法很好处理环境的不稳定问题。利用强化学习中 actor-critic 框架的特点，能够在这两种极端方式中找到协调的办法。</strong></li>
<li><strong><strong>policy-based 的方法</strong></strong></li>
</ul>
</li>
</ul>
<pre><code>* ****MADDPG******（Multi-Agent Deep Deterministic Policy Gradient**

  *  **这种方法是在深度确定策略梯度（Deep Deterministic Policy Gradient，DDPG）方法的基础上、对其中涉及到的 actor-critic 框架进行改进，使用集中式训练、分布式执行的机制（centralized training and decentralized execution），为解决多智能体问题提供了一种比较通用的思路。**
  *  **MADDPG 为每个智能体都建立了一个中心化的 critic，它能够获取全局信息（包括全局状态和所有智能体的动作）并给出对应的值函数 Qi(x,a1,...,an)，这在一定程度上能够缓解多智能体系统环境不稳定的问题。另一方面，每个智能体的 actor 则只需要根据局部的观测信息作出决策，这能够实现对多智能体的分布式控制。**
    **在基于 actor-critic 框架的学习过程中，critic 和 actor 的更新方式和 DDPG 类似。对于 critic，它的优化目标为：**
    ![img](https://pic3.zhimg.com/80/v2-1af4b831552fa91e51e36c59c537594a_720w.jpg)
    **对于 actor，考虑确定性策略μi(ai|oi)，策略更新时的梯度计算可以表示为：**
    ![img](https://pic3.zhimg.com/80/v2-1af4b831552fa91e51e36c59c537594a_720w.jpg)
  *  **MADDPG 进一步提出了可以通过维护策略逼近函数 来估计其他智能体的策略，******通过对其他智能体的行为建模******使得维护中心化的 Q 值、考虑联合动作效应对单个个体来说是可行的。**

  *  **MADDPG 在处理环境不稳定问题方面还使用了******策略集成（policies ensemble）******的技巧。由于环境中的每个智能体的策略都在迭代更新，因此很容易出现单个智能体的策略对其他智能体的策略过拟合，即当其他智能体的策略发生改变时，当前得到的最优策略不一定能很好的适应其他智能体的策略。为了缓和过拟合问题，MADDPG 提出了策略集成的思想，即对于单个智能体 i，它的策略μi 是由多个子策略μi^k 构成的集合。在一个 episode 中，只使用一种从集合中采样得到的子策略进行决策和完成交互。在学习过程中最大化的目标是所有子策略的期望回报 **

  *  **总结：MADDPG 的核心是在 DDPG 算法的基础上，对每个智能体使用全局的 Q 值来更新局部的策略，该方法在完全合作、完全竞争和混合关系的问题中都能取得较好效果。 **

*  **COMA（略**
</code></pre><ul>
<li><p><strong><strong>value-based 的方法</strong></strong></p>
<ul>
<li><strong>policy-based 方法中，中心化的值函数是直接使用全局信息进行建模，没有考虑个体的特点。在多智能体系统是由大规模的多个个体构成时，这样的值函数是难以学习或者是训练到收敛的，很难推导出理想的策略。并且仅依靠局部观测值，无法判断当前奖励是由于自身的行为还是环境中其他队友的行为而获得的。</strong></li>
<li><strong>值分解网络（value decomposition networks, VDN）（略</strong></li>
</ul>
</li>
</ul>
<pre><code>  * **将全局的 Q(s,a)值分解为各个局部 Qi(si,ai)的加权和，每个智能体拥有各自的局部值函数。**
</code></pre>]]></content>
      <categories>
        <category>MARL</category>
      </categories>
      <tags>
        <tag>MARL</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>路径规划</title>
    <url>/MARL/MADRL/%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92/</url>
    <content><![CDATA[<p><strong>路径规划</strong></p>
<ul>
<li><p>多智能体强化学习路径规划**](<a href="https://zhuanlan.zhihu.com/p/100813774">https://zhuanlan.zhihu.com/p/100813774</a>)</p>
</li>
<li><p>基于以上分析，移动机器人智能路径规划方法研究虽然取得了重要成果，但仍存在局限性，如遗传算法、蚁群算法容易陷入局部最优，神经网络算法需要大量样本。目前的改进算法以多种算法相结合、分层优化等方式为主，虽弥补了缺点，但存在诸多发展瓶颈，如算法复杂度增加，收敛速度慢。</p>
<p>较于其他算法，强化学习，学习能力强，适应复杂未知环境，但目前强化学习的试错学习、状态泛化，需要耗费大量资源。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>MARL</category>
      </categories>
      <tags>
        <tag>MARL</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>避障</title>
    <url>/MARL/MADRL/%E9%81%BF%E9%9A%9C/</url>
    <content><![CDATA[<h3 id="避障"><a href="#避障" class="headerlink" title="避障"></a><strong>避障</strong></h3><ul>
<li><p>集群协同避障汇总**](<a href="https://blog.csdn.net/qq_16775293/article/details/88556986">https://blog.csdn.net/qq_16775293/article/details/88556986</a>)</p>
</li>
<li><p>奖惩函数 与 避障的关系</p>
</li>
<li><p>人工势场法</p>
<ul>
<li>人工势场路径规划技术的基本思想是将机器人在环境中的运动视为一种机器人，在虚拟的人工受力场中的运动。障碍物对机器人产生斥力，目标点对机器人产生引力，引力和斥力的合力作为机器人的控制力，从而控制机器人避开障碍物而到达目标位置。</li>
<li>对于动态路径规划问题来说，与机器人避障相关的主要是机器人与障碍物之间的<strong>相对位置和相对速度</strong>，而非绝对位置和速度。将机器人与目标物的相对位置与相对速度引入吸引势函数，将机器人与障碍的相对位置与相对速度引入排斥势函数，提出动态环境下的机器人路径规划算法。 </li>
<li>人工势场路径规划技术<em>原理简单，便于底层的实时控制，在机器人的实时避障和平滑轨迹控制等方面得到了广泛研究</em>。但人工势场路径规划方法通常<strong>存在局部极小点</strong>，尽管也有不少针对局部极小的改进方法，但到目前为止，仍未找到完全满意的答案。另外,在引力和斥力场设计时<strong>存在人为不确定因素</strong>，<strong>在障碍物较多时还存在计算量过大等问题</strong>，这些因素的存在限制了人工势场路径规划方法的广泛应用。应用中的<strong>难点是动态环境中引力场与斥力场的设计、局部极小问题的解决</strong>。</li>
</ul>
</li>
<li><p>无人机群 在运动过程中 需要（通过传感器）感知到动态障碍物的运动状态方可避障</p>
</li>
<li><p>无人机基于当前环境信息的局部路径规划算法，将对环境的建模与搜索避障融为一体，能对规划结果进行实时反馈和校正，动态性高，但是由于缺乏全局环境信息，规划结果往往不是全局最优，甚至可能找不到正确路径或完整路径</p>
</li>
<li><h3 id="移动机器人路径规划算法存在的问题"><a href="#移动机器人路径规划算法存在的问题" class="headerlink" title="移动机器人路径规划算法存在的问题"></a>移动机器人路径规划算法存在的问题</h3></li>
</ul>
<ul>
<li><img src="https://raw.githubusercontent.com/ChangQingAAS/for_picgo/main/img/20210903103725.png" alt="image-20210710162533883" style="zoom: 67%;" /><ul>
<li>要解决的问题是 未知环境下的动态障碍物路径规划 基于传感器信息的局部路径规划</li>
<li>环境是啥？</li>
<li><img src="https://raw.githubusercontent.com/ChangQingAAS/for_picgo/main/img/20210903103730.png" alt="image-20210710182339529"></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>MARL</category>
      </categories>
      <tags>
        <tag>MARL</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>英文单词</title>
    <url>/MARL/code_notes/%E4%BB%A3%E7%A0%81%E4%B8%AD%E7%9A%84%E8%8B%B1%E6%96%87/</url>
    <content><![CDATA[<h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><p>reg  寄存器</p>
<p>mean 平均值</p>
<p>clip  裁剪</p>
<p>fluid ?流动的</p>
<p>spawn 产卵</p>
<p>denom 分母</p>
<p>dim 维度</p>
<p>utils 工具</p>
<p>lemma 引理</p>
<p>priority 优先权</p>
<p>Dueling 决斗 竞争</p>
<p>minibatch 小批量</p>
<p>algo 算法（简写</p>
<p>built-in 内置</p>
<p>renderer 渲染器</p>
<p>gridworld 网格</p>
<p>Placeholder  占位符</p>
<p>render 渲染 刷新</p>
<p>enumerate 枚举</p>
<p>Norm  常态 定额 标准</p>
<p>pred  预测</p>
<p>session 一节，一段时间</p>
<p>eval 评估</p>
<p>adversaries 对手</p>
<p>benchmark 基准</p>
<p>scenario 设想 方案，预测/场景</p>
<p>parser 解析器</p>
<p>script 脚本</p>
<p>optimizer 优化器</p>
<p>discount factor 折扣因子通常表示为 <em>γ</em>，是一个乘以未来预期奖励的因子，在[0，1]的范围内变化。它控制着未来奖励相对于眼前奖励的重要性。折扣因子越低，未来的奖励就越不重要，代理将倾向专注于只产生即时奖励的动作。</p>
<p>MLP 多层感知器</p>
<p>preset 预设</p>
<p>discrete 离散的</p>
<p>landmark 特征<del>地标</del></p>
<p>round 回合</p>
<p>properties 属性</p>
<p>diagnostic 诊断的</p>
<p>metrics 指标</p>
<p>deception 欺骗</p>
<p>interval 间隔</p>
<p>deceive 欺骗</p>
<p>Covert 隐蔽</p>
<p>crypto 秘密(支持者)</p>
<p>encrypt 加密</p>
<p>policy  策略</p>
<p>跑完整个训练数据集，叫做一个epoch。一个epoch包含多个episode。一个episode完成一次模型验证，保存最优模型，简单来说就是多少个step进行一次模型验证。</p>
<p>假设整个训练数据集有n=10,000个样本，batch_size=10，那么一个epoch就包含10,000/10=1,000个step（或iteration）。假设episodes=100，即一个episode包含100个step，那么一个epoch就包含1,000/100=10个episode。每一个episode完成后，进行一次模型验证，并保存模型（一般模型性能没有提升，则不保存）</p>
<p>agent根据某个策略执行一系列action到结束就是一个episode。</p>
<p>episode就是每次游戏达到结束后所有action的集合，就是每局完整游戏的策略</p>
<ul>
<li><strong>Episode</strong>：介于初始状态和最终状态之间的所有状态；例如一局象棋。代理的目标是在一个情景（Episode）内获得最大的总奖励。在没有终态的情况下，考虑无限的情景。重要的是，不同的情景是完全相互独立的。</li>
<li><strong>Episodic Tasks</strong>：由不同情景组成的强化学习任务，意味着，每个情景都有最终状态。</li>
<li><strong>Expected Return</strong>：有时称为总体奖励（overall reward），有时称为G，是整个情景的预期奖励。</li>
<li><strong>Experience Replay</strong>：由于强化学习任务没有预先生成的可以学习的训练集，代理必须记录它遇到的所有状态转换，以便以后可以从中学习。用于存储这些内容的内存缓冲区（memory-buffer）通常被称为 Experience Replay。这些内存缓冲区有几种类型和架构，最常见的是 <a href="https://en.wikipedia.org/wiki/Circular_buffer">循环内存缓冲区（cyclic memory buffer）</a>，确保代理保持对其新行为的训练，而不是对可能不再相关的事情的训练；另一种是 <a href="https://en.wikipedia.org/wiki/Reservoir_sampling">基于存储区采样的内存缓冲区（reservoir-sampling-based memory）</a>，确保记录的每个状态转换都有均匀的概率插入缓冲区。</li>
<li><strong>Exploitation &amp; Exploration</strong>：强化学习任务没有预先生成的训练集，他们可以从中学习——他们创建自己的经验并“动态”学习。为了做到这一点，代理需要在许多不同的状态下尝试许多不同的动作，以便尝试和学习所有可用的可能性，并找到使其整体奖励最大化的路径，这被称为探索（Exploitation），因为代理探索环境。另一方面，如果代理所做的只是探索，它将永远不会最大化整体奖励——它还必须使用它学到的信息来这样做，这被称为开发（Exploration），因为代理人利用其知识来最大化其获得的回报。两者之间的权衡是强化学习问题的最大挑战之一，因为两者必须平衡，以便代理既能充分探索环境，又能利用它所学的知识，并重复它发现的最有价值的路径。</li>
<li><strong>Greedy Policy, \</strong>ε ε*ε**<em>-Greedy Policy*</em>：贪婪策略意味着代理不断执行被认为能产生最高预期回报的操作。显然，这样的策略根本不允许代理进行探索。为了仍然允许一些探索，通常使用ε-贪婪策略：选择[0，1]范围内的数字(称为ε)，并且在选择动作之前，选择[0，1]范围内的随机数。如果该数字大于ε，则选择贪婪操作；但如果该数字小于ε，则选择随机操作。注意，如果ε=0，则策略成为贪婪策略，如果ε=1，则始终探索。</li>
<li><strong>k-Armed Bandits: See Bandits</strong>。</li>
<li><strong>Markov Decision Process (MDP)</strong>：马尔可夫属性意味着每个状态只依赖于它的前一个状态、从该状态采取的选定动作以及在该动作执行后立即获得的奖励。数学表示为：s ′ = s ′ ( s , a , r ) s’ = s’(s, a, r)<em>s</em>′=<em>s</em>′(<em>s</em>,<em>a</em>,<em>r</em>)，其中 s ′ s’<em>s</em>′ 是未来状态，s s<em>s</em> 是它的前一个状态，a a<em>a</em> 和 r r<em>r</em> 是动作和奖励。不需要事先知道 s s<em>s</em> 之前发生了什么——马尔可夫属性假设 s s<em>s</em> 包含了所有相关信息。马尔可夫决策过程是基于这些假设的决策过程。</li>
<li><strong>Model-Based &amp; Model-Free</strong>：基于模型和无模型是代理在试图优化其策略时可以选择的两种不同方法。最好用一个例子来解释：假设你正在学习如何玩21点（Blackjack）。你可以通过两种方式做到这一点：第一种选择是提前计算，在游戏开始之前，所有状态的获胜概率，以及给定所有可能动作的所有状态转移概率（state-transition probabilities），然后简单地按照你的计算动作。第二种选择是在没有任何先验知识的情况下简单地执行，利用 试错法（trial-and-error）获取信息。注意，使用第一种方法，是在建模环境，而第二种方法不需要关于环境的信息。这正是基于模型和无模型的区别；第一种方法是基于模型的，而后者是无模型的。</li>
<li><strong>Monte Carlo (MC)</strong>：蒙特卡罗方法是使用重复随机抽样（repeated random sampling）来获得结果的算法。它们在强化学习算法中经常被用来获得期望值（expected values）；例如，通过一次又一次地返回到相同的状态来计算状态值函数（state Value function），并对每次收到的实际累积奖励进行平均。</li>
<li><strong>On-Policy &amp; Off-Policy</strong>：每个强化学习算法都必须遵循一些策略，以便决定在每个状态下执行哪些操作。然而，算法的学习过程在学习时不必考虑该策略。关注产生过去状态动作决策的策略的算法被称为策略（on-policy）算法，而忽略它的算法被称为无策略（off-policy）算法。一个众所周知的无策略算法是 <em>Q</em>−<em>L<strong>e</strong>a<strong>r</strong>n<strong>i</strong>n**g</em>，因为它的更新规则使用将产生最高 Q-Value 的动作，而实际使用的策略可能会限制该动作或选择另一个动作。Q<em>−</em>L<strong>e</strong>a<strong>r</strong>n<strong>i</strong>n<strong>g<em> 的策略变体称为 </em>S</strong>a<strong>r</strong>s<em>*a</em>，其中更新规则使用遵循的策略选择的操作。</li>
<li><strong>Q-Learning</strong>：Q-Learning 是一种无策略强化学习算法，被认为是最基本的算法之一。在其最简化的形式中，它使用一个表来存储所有可能的状态-动作对（ state-action pairs）的所有Q-Values。它使用贝尔曼方程（Bellman equation）更新这个表，而动作选择通常是用 ε-贪婪策略 进行的。最简单的形式（状态转换和预期回报中没有不确定性）的 Q-Learning 的更新规则是：<em>Q</em>(<em>s</em>,<em>a</em>)=<em>r</em>(<em>s</em>,<em>a</em>)+<em>γ</em> <em>m<strong>a</strong>x**a</em> <em>Q</em>(<em>s</em>′,<em>a</em>)</li>
</ul>
<p>它的一个更复杂的版本 Deep Q-Network variant 更受欢迎，有时简称为 Deep Q-Learning 或 Q-Learning。这种变体用神经网络代替了状态-动作表（state-action table），以便处理大规模任务，其中可能的状态-动作对的数量可能是巨大的。可以在<a href="https://medium.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677">这里</a>找到这个算法的教程。</p>
<ul>
<li><strong>Q Value (Q Function)</strong>：通常记为 Q ( s , a ) Q(s, a)<em>Q</em>(<em>s</em>,<em>a</em>)，有时带 π π<em>π</em> 下标，有时在Deep RL中表示为 Q ( s , a ; θ ) Q(s, a; θ)<em>Q</em>(<em>s</em>,<em>a</em>;<em>θ</em>)，Q Value是对总体预期回报的度量，假设代理处于状态 s s<em>s</em> 并执行动作 a a<em>a</em>，然后按照某些策略 π π<em>π</em> 继续执行，直到情景结束。Q 是 Quality 一词的缩写，数学上定义为：<br>Q ( s , a ) = E [ ∑ n = 0 N γ n r n ] Q(s,a) = E[\sum^N_{n=0}\gamma^n r_n]<em>Q</em>(<em>s</em>,<em>a</em>)=<em>E</em>[<em>n</em>=0∑<em>N</em>​<em>γ<strong>n</strong>r**n</em>​]<br>其中 N N<em>N</em> 是从状态 s s<em>s</em> 到结束状态的状态数，γ γ<em>γ</em> 是折扣因子，r 0 r^0<em>r</em>0 是在状态 s s<em>s</em> 中执行动作 a a<em>a</em> 后立即获得的奖励。</li>
<li><strong>REINFORCE Algorithms</strong>：强化算法是一类强化学习算法，它们根据策略相对于<a href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf">策略参数</a>的梯度来更新策略参数。该名称通常只用大写字母书写，因为它最初是原始算法组设计的首字母缩写：REward Increment = <a href="https://link.springer.com/content/pdf/10.1007/BF00992696.pdf">Nonnegative Factor x Offset Reinforcement x Characteristic Eligibility</a></li>
<li><strong>Reinforcement Learning (RL)</strong>：与监督学习和无监督学习一样，强化学习是机器学习和人工智能的主要领域之一。它与一个任意存在的学习过程有关，这个任意存在被称为代理（Agent），它周围的世界被称为环境（Environment）。代理寻求从环境中获得最大的回报，并执行不同的动作，以了解环境如何响应并获得更多的回报。RL任务的最大挑战之一是将动作与延迟奖励相关联，延迟奖励是代理在产生奖励的动作完成后很久才收到的奖励。因此，它被大量用于解决不同类型的游戏，从井字棋、象棋、Atari 2600到围棋和星际争霸。</li>
<li><strong>Reward</strong>：代理从环境中接收的数值，作为对代理操作的直接响应。代理的目标是最大化它在一个情景内获得的整体奖励，因此奖励是代理采取所需行为的动机。所有的动作都会产生奖励，奖励可以大致分为三种：强调期望行为的积极奖励；强调代理应该偏离的行为的消极奖励；以及零奖励，这意味着代理没有做任何特殊或独特的事情。</li>
<li><strong>Sarsa</strong>：Sarsa 算法基本上是Q-Learning算法，为了使它成为一个在线算法，做了一些修改。Q-Learning 更新规则基于最佳Q-Value 的贝尔曼方程（Bellman equation），因此在状态转换和预期回报没有不确定性的情况下，Q-Learning 更新规则是：<br>Q ( s , a ) = r ( s , a ) + γ  m a x a  Q ( s ′ , a ) Q(s,a) = r(s,a) + \gamma\ max_a\ Q(s’, a)<em>Q</em>(<em>s</em>,<em>a</em>)=<em>r</em>(<em>s</em>,<em>a</em>)+<em>γ</em> <em>m<strong>a</strong>x**a</em>​ <em>Q</em>(<em>s</em>′,<em>a</em>)<br>为了将其转换为基于策略的算法，修改了最后一项：<br>Q ( s , a ) = r ( s , a ) + γ  Q ( s ′ , a ) Q(s,a) = r(s,a) + \gamma\ Q(s’, a)<em>Q</em>(<em>s</em>,<em>a</em>)=<em>r</em>(<em>s</em>,<em>a</em>)+<em>γ</em> <em>Q</em>(<em>s</em>′,<em>a</em>)<br>这里，动作 a a<em>a</em> 和 a ’ a’<em>a</em>’ 由相同的策略选择。算法的名称源自其更新规则，该规则基于 ( s , a , r , s ’ , a ’ ) (s,a,r,s’,a’)(<em>s</em>,<em>a</em>,<em>r</em>,<em>s</em>’,<em>a</em>’)，所有这些都来自同一个策略。</li>
<li><strong>State</strong>：代理在环境中遇到的每个场景都被正式称为状态。代理通过执行操作在不同状态之间转换。还值得一提的是标志着一集结束的终端状态。在达到终端状态后，没有可能的状态，新的一集开始了。通常，终端状态被表示为一种特殊状态，在这种状态下，所有动作都转换到相同的终端状态，奖励为0。</li>
<li><strong>State-Value Function</strong>：See Value Function。</li>
<li><strong>Temporal-Difference (TD)</strong>：时间差分是一种结合动态规划和蒙特卡罗原理的学习方法；它像蒙特卡洛一样“在飞行中”学习，但是像动态规划一样更新它的估计。最简单的时域差分算法之一，称为一步时域或时域(0)。它根据以下更新规则更新值函数（Value Function）：<br>V ( s t ) = V ( s t ) + α [ r t + 1 + γ V ( s t + 1 ) − V ( s t ) V(s_t) = V(s_t) + \alpha [r_{t+1} + \gamma V(s_{t+1}) - V(s_t)<em>V</em>(<em>s**t</em>​)=<em>V</em>(<em>s**t</em>​)+<em>α</em>[<em>r**t</em>+1​+<em>γ**V</em>(<em>s**t</em>+1​)−<em>V</em>(<em>s**t</em>​)<br>其中 V V<em>V</em> 是值函数，s s<em>s</em> 是状态，r r<em>r</em> 是奖励，γ γ<em>γ</em> 是折扣因子，α α<em>α</em> 是学习率，t t<em>t</em> 是时间步长，<strong>等号用作更新运算符，而不是等式</strong>。方括号中的术语被称为 <strong>时间差异误差（temporal difference error）</strong>。</li>
<li><strong>Terminal State</strong>：See State。</li>
<li><strong>Upper Confident Bound (UCB)</strong>：UCB是一种探索方法，它试图确保每个动作都得到很好的探索。考虑一个完全随机的探索策略——也就是说，每个可能的动作都有相同的被选择的机会。有些动作很有可能会比其他动作被探索得更多。选择的动作越少，代理对其预期奖励的信心就越低，其开发阶段就可能受到损害。UCB的探索考虑了每一个动作被选择的次数，并给那些不太被探索的动作额外的权重。从数学上对此进行形式化，所选择的动作是通过：<br>a c t i o n = a r g m a x a  [ R ( a ) + c l n t N ( a ) ] action = argmax_a\ [R(a) + c\sqrt{\frac{lnt}{N(a)}}]<em>a<strong>c</strong>t<strong>i</strong>o**n</em>=<em>a<strong>r</strong>g<strong>m</strong>a<strong>x</strong>a</em>​ [<em>R</em>(<em>a</em>)+<em>c**N</em>(<em>a</em>)<em>l<strong>n</strong>t</em>​​]<br>其中，R ( a ) R(a)<em>R</em>(<em>a</em>) 是动作 a a<em>a</em> 的预期总回报，t t<em>t</em> 是采取的步骤数（总共选择了多少个动作），N ( a ) N(a)<em>N</em>(<em>a</em>) 是选择动作 a a<em>a</em> 的次数，c c<em>c</em> 是可配置的超参数。这种方法有时也被称为 乐观探索（exploration through optimism），因为它给不太被探索的行为一个更高的价值，鼓励模型选择它们。</li>
<li><strong>Value Function</strong>：价值函数通常表示为<em>V</em>(<em>s</em>)，有时带有 π 下标，它是对总体预期奖励的一种度量，假设代理处于状态<em>s</em>，然后按照某个策略<em>π</em> 继续执行，直到情景结束。它在数学上定义为：<em>V</em>(<em>s</em>)=<em>E</em>[<em>n</em>=0∑<em>N</em>​<em>γ<strong>n</strong>r**n</em>​]<br>虽然它看起来确实类似于 Q-Value 的定义，但是有一个隐含的，重要的区别：对于n<em>=0，V</em>(<em>s</em>) 的 <em>r</em>0 的奖励是在任何动作被执行之前处于状态<em>s</em> 的预期奖励，而在Q-Value中，<em>r</em>0 是在某个动作被执行之后的预期奖励。这种差异也产生了优势函数（Advantage function）。</li>
</ul>
<p>env  环境</p>
<p>obs  观察</p>
<p>arg 参量</p>
<p>arglist 参量表</p>
]]></content>
      <categories>
        <category>MARL</category>
      </categories>
      <tags>
        <tag>MARL</tag>
      </tags>
  </entry>
  <entry>
    <title>Python函数参数的传递机制</title>
    <url>/%E7%A2%8E%E7%A2%8E%E5%BF%B5/others/Python%E5%87%BD%E6%95%B0%E5%8F%82%E6%95%B0%E7%9A%84%E4%BC%A0%E9%80%92%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h1 id="Python函数参数的传递机制"><a href="#Python函数参数的传递机制" class="headerlink" title="Python函数参数的传递机制"></a>Python函数参数的传递机制</h1><h2 id="不可变对象是值传递"><a href="#不可变对象是值传递" class="headerlink" title="不可变对象是值传递"></a>不可变对象是值传递</h2><p><strong>python中不可变对象，函数实际参数(实参)传递给形式参数(形参)的过程，实际上是把实际参数值的副本(复制品)传入函数，参数本身不会收到任何影响。</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def func(a, b):
    a, b &#x3D; b, a
    # 函数内 a:2 b:1
    print(&#39;函数内 a:&#123;&#125; b:&#123;&#125;&#39;.format(a, b))

if __name__ &#x3D;&#x3D; &#39;__main__&#39;:
    a &#x3D; 1
    b &#x3D; 2
    # 可以理解为是将a, b的值复制一份传入
    func(a, b)
    # 函数外 a:1 b:2
    print(&#39;函数外 a:&#123;&#125; b:&#123;&#125;&#39;.format(a, b))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="可变对象是引用传递-地址传递"><a href="#可变对象是引用传递-地址传递" class="headerlink" title="可变对象是引用传递(地址传递)"></a>可变对象是引用传递(地址传递)</h2><p>对于可变对象如字典，列表等，参数传递的方式是引用传递，也就是将可变对象的引用(内存地址)传递给函数，参数会受到影响。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def func(a):
    a.append(4)
    a.append(5)
    # 函数内 a:[1, 2, 3, 4, 5]
    print(&#39;函数内 a:&#123;&#125;&#39;.format(a))

if __name__ &#x3D;&#x3D; &#39;__main__&#39;:
    a &#x3D; [1, 2, 3]
    func(a)
    # 函数内 a:[1, 2, 3, 4, 5]
    print(&#39;函数外 a:&#123;&#125;&#39;.format(a))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
]]></content>
      <categories>
        <category>碎碎念</category>
      </categories>
  </entry>
  <entry>
    <title>FIRST</title>
    <url>/%E7%A2%8E%E7%A2%8E%E5%BF%B5/others/example/</url>
    <content><![CDATA[<h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><p>大家好，这是第一篇博客</p>
]]></content>
      <categories>
        <category>碎碎念</category>
      </categories>
  </entry>
  <entry>
    <title>保护变量的访问与设置</title>
    <url>/python/others/%E4%BF%9D%E6%8A%A4%E5%8F%98%E9%87%8F%E7%9A%84%E8%AE%BF%E9%97%AE%E4%B8%8E%E8%AE%BE%E7%BD%AE/</url>
    <content><![CDATA[<h1 id="保护变量的访问与设置"><a href="#保护变量的访问与设置" class="headerlink" title="保护变量的访问与设置"></a>保护变量的访问与设置</h1><p>对于私有变量（双下划线）或者保护变量（单下划线），不允许外部直接访问，类似于Java的private，可以通过对外提供get，set接口来访问和修改这类变量，便于控制。</p>
<ul>
<li>@property装饰器会将方法转换为相同名称的只读属性，相当于实现get方法</li>
<li>@xxx.setter装饰器使得可以直接通过 对象.xxx来修改保护变量的值，相当于实现set方法</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class Student(object):
    def __init__(self, name):
        self.name &#x3D; name
        self._score &#x3D; None

    @property
    def score(self):
        return self._score

    @score.setter
    def score(self, value):
        self._score &#x3D; value

if __name__ &#x3D;&#x3D; &#39;__main__&#39;:
    s &#x3D; Student(&#39;王大锤&#39;)
    s.score &#x3D; 100  # 设置保护变量的值
    print(s.score)  # 访问保护变量<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title>猴子补丁</title>
    <url>/%E7%A2%8E%E7%A2%8E%E5%BF%B5/others/%E7%8C%B4%E5%AD%90%E8%A1%A5%E4%B8%81/</url>
    <content><![CDATA[<h1 id="猴子补丁"><a href="#猴子补丁" class="headerlink" title="猴子补丁"></a>猴子补丁</h1><p><strong>monkey patch允许在运行期间动态修改一个类或模块</strong></p>
<ul>
<li>在运行时替换方法、属性等</li>
<li>在不修改第三方代码的情况下增加原来不支持的功能</li>
<li>在运行时为内存中的对象增加patch而不是在磁盘的源代码中增加</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class A:
    def func(self):
        print(&#39;这是A类下的func方法&#39;)

# arg 这个参数是没有用到的，因为func有一个参数，如果这个函数没有参数的话不能这样直接赋值
def monkey_func(arg):
    print(&#39;这是猴子补丁方法&#39;)

if __name__ &#x3D;&#x3D; &#39;__main__&#39;:
    a &#x3D; A()
    # 运行原类下的方法
    a.func()  # 这是A类下的func方法

    # 在不改变原类代码的情况下，动态修改原类的方法，打补丁
    A.func &#x3D; monkey_func

    # 运行替换后的方法
    a.func()  # 这是猴子补丁方法<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>gevent通过打补丁的方式，利用自己的socket替换了python的标准socket模块，利用gevent协程处理高并发的情况</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from gevent import monkey
monkey.patch_all()
import gevent
from socket import *


def talk(conn):
    while 1:  # 循环通讯
        try:
            from_client_msg &#x3D; conn.recv(1024)
            if not from_client_msg:break
            print(&quot;来自客户端的消息:%s&quot; %(from_client_msg))
            conn.send(from_client_msg.upper())
        except:
            break
    conn.close()

if __name__ &#x3D;&#x3D; &#39;__main__&#39;:
    server &#x3D; socket()
    ip_port &#x3D; (&quot;127.0.0.1&quot;, 8001)
    server.bind(ip_port)
    server.listen(5)
    while 1:  # 循环连接
        conn, addr &#x3D; server.accept()
        gevent.spawn(talk, conn)  # 开启一个协程
    server.close()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
]]></content>
      <categories>
        <category>碎碎念</category>
      </categories>
  </entry>
</search>
